<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>klion26</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-11-20T14:04:38.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>klion26</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>git inside</title>
    <link href="http://yoursite.com/2017/11/20/git-inside/"/>
    <id>http://yoursite.com/2017/11/20/git-inside/</id>
    <published>2017-11-20T13:17:59.000Z</published>
    <updated>2017-11-20T14:04:38.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p><a href="https://git-scm.com/" target="_blank" rel="external">Git</a>是一个分布式的版本控制系统，能够完成你能想到的关于版本相关的所有事情，但是 Git 却不是那么好上手，也就是所谓的入门门槛有点高。</p>
</blockquote>
<h1 id="本文会讲什么"><a href="#本文会讲什么" class="headerlink" title="本文会讲什么"></a>本文会讲什么</h1><p>本文会换一个角度讲述 Git 怎么做的，给大家提供一个另外的视角，这个视角主要设计 Git 的存储，这样给大家一个更深的认识，在平时想了解的时候，也能有合适的渠道进行。</p>
<a id="more"></a>
<h1 id="Git-是什么"><a href="#Git-是什么" class="headerlink" title="Git 是什么"></a>Git 是什么</h1><p>这个问题看上去想一句废话，因为文章开始就说了，Git 是一个分布式的版本控制系统。那么底层又是如何实现的呢？以及其他人是怎么看待 Git 的呢？</p>
<blockquote>
<p>In many ways you can just see git as a filesystem. –Linus </p>
</blockquote>
<p>从上面一句话来看，Linus 把 Git 当做一个文件系统来做的，而不是一个传统意义上的版本控制系统，恰好 Git 能够做版本控制的事情。</p>
<h1 id="Git-文件结构"><a href="#Git-文件结构" class="headerlink" title="Git 文件结构"></a>Git 文件结构</h1><p>每一个 Git 仓库中都有一个隐藏的文件夹，名字叫做 <code>.git</code> 这个文件夹包含了所有的文件内容，以及版本控制相关的信息，大致结构如下：</p>
<img src="/2017/11/20/git-inside/file_tree.png" alt="file_tree.png" title="">
<p>说几个大家关注的，有兴趣的可以自己打开文件看看</p>
<ol>
<li>HEAD 指向当前分支的最后一个 commit</li>
<li>config 表示一些本项目的配置（如果没有会使用全局的配置），最明显的是 user.name 和 user.email</li>
<li>hooks 表示一些钩子函数</li>
<li>objects, 这里面保存的是所有的数据（也就是实实在在的内容），会是主要的内容</li>
<li>refs 表示一些引用，包括分支的，远程分支的，tag 的，每一个都具体指向一个 commit</li>
</ol>
<h1 id="几个概念"><a href="#几个概念" class="headerlink" title="几个概念"></a>几个概念</h1><ol>
<li>blob :上面的 objects 中的具体文件内容（保存的是文件内容，如果多个文件的内容一样，则只保存一份）</li>
<li>tree : tree 包含 tree 或者 blob</li>
<li>commit : commit 指向 tree 的根节点</li>
<li>tag : 指向一个具体的 commit</li>
<li>parent : 表示该 commit 从哪个 commit 演变而来</li>
<li>branch : 一条 commit 的链</li>
<li>HEAD : branc 的最新 commit</li>
</ol>
<h1 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子"></a>一个例子</h1><p>假设我们有一个项目，如下图所示：</p>
<img src="/2017/11/20/git-inside/project.png" alt="project.png" title="">
<p>那么我们初始化后的 git 示意图应该是这样的</p>
<img src="/2017/11/20/git-inside/first_version.png" alt="first_version.png" title="">
<p>接着我们修改 c.txt，那么我们会得到下面的示意图</p>
<img src="/2017/11/20/git-inside/second_version.png" alt="second_version.png" title="">
<p>再接着我们修改 a.txt，我们就会得到下面的示意图</p>
<img src="/2017/11/20/git-inside/third_version.png" alt="third_version.png" title="">
<p>上面的图中灰色表示非当前版本，亮色表示当前版本，三角形代表 commit，树形表示 tree，六边形表示 blob。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文介绍了 Git 的一些基本概念，以及一个简单的示例，当然这些还远远不够了解 Git 本身的，但是我认为这些是 Git 中最基本，也是最核心的一些东西了，其他的命令大概都能够从这几个命令中得到。</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://git-scm.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Git&lt;/a&gt;是一个分布式的版本控制系统，能够完成你能想到的关于版本相关的所有事情，但是 Git 却不是那么好上手，也就是所谓的入门门槛有点高。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;本文会讲什么&quot;&gt;&lt;a href=&quot;#本文会讲什么&quot; class=&quot;headerlink&quot; title=&quot;本文会讲什么&quot;&gt;&lt;/a&gt;本文会讲什么&lt;/h1&gt;&lt;p&gt;本文会换一个角度讲述 Git 怎么做的，给大家提供一个另外的视角，这个视角主要设计 Git 的存储，这样给大家一个更深的认识，在平时想了解的时候，也能有合适的渠道进行。&lt;/p&gt;
    
    </summary>
    
    
      <category term="git" scheme="http://yoursite.com/tags/git/"/>
    
      <category term="file-system" scheme="http://yoursite.com/tags/file-system/"/>
    
      <category term="inside" scheme="http://yoursite.com/tags/inside/"/>
    
  </entry>
  
  <entry>
    <title>django-configuration in action</title>
    <link href="http://yoursite.com/2017/11/09/django-configuration-in-action/"/>
    <id>http://yoursite.com/2017/11/09/django-configuration-in-action/</id>
    <published>2017-11-09T14:34:22.000Z</published>
    <updated>2017-11-10T06:36:03.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Django-中统一配置的做法"><a href="#Django-中统一配置的做法" class="headerlink" title="Django 中统一配置的做法"></a>Django 中统一配置的做法</h1><blockquote>
<p>本文主要描述如何在 django 中统一 settings 文件</p>
</blockquote>
<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>django 服务会有多个环境，比如 开发环境、测试环境以及线上环境等。现在大部分使用的方案是针对每一种环境使用一个 settings 文件，然后在不同的环境中使用不同的 settings 文件。这样的设计我认为有至少两个问题:</p>
<ol>
<li>很多公用的配置不太好公用</li>
<li>文件数会很多，项目中管理会比较麻烦</li>
</ol>
<a id="more"></a>
<h1 id="新方案"><a href="#新方案" class="headerlink" title="新方案"></a>新方案</h1><p>针对 settings 文件，在 django 项目中可以使用 django-configuration module 进行管理，从而解决上面的两个问题。</p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><ol>
<li>使用 <code>pip install django-configurations</code> 安装 django-configuration module </li>
<li>修改 settings.py manager.py 以及 wsgi.py 三个文件即可</li>
<li>建立一个配置统一的文件，用于管理不同的配置项</li>
</ol>
<h3 id="具体-Demo"><a href="#具体-Demo" class="headerlink" title="具体 Demo"></a>具体 Demo</h3><p>settings.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div></pre></td><td class="code"><pre><div class="line">from configurations import Configuration</div><div class="line"># Build paths inside the project like this: os.path.join(BASE_DIR, ...)</div><div class="line">import os</div><div class="line"></div><div class="line"></div><div class="line">class Common(Configuration):</div><div class="line">    BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))</div><div class="line"></div><div class="line">   # Quick-start development settings - unsuitable for production</div><div class="line">    # See https://docs.djangoproject.com/en/1.8/howto/deployment/checklist/</div><div class="line"></div><div class="line">    # SECURITY WARNING: keep the secret key used in production secret!</div><div class="line">    SECRET_KEY = &apos;secret_key&apos;</div><div class="line"></div><div class="line">    # SECURITY WARNING: don&apos;t run with debug turned on in production!</div><div class="line">    DEBUG = True</div><div class="line"></div><div class="line">    ALLOWED_HOSTS = []</div><div class="line"></div><div class="line">   # Application definition</div><div class="line"></div><div class="line">    INSTALLED_APPS = (</div><div class="line">		&apos;django.contrib.admin&apos;,</div><div class="line">		&apos;django.contrib.auth&apos;,</div><div class="line">		&apos;django.contrib.contenttypes&apos;,</div><div class="line">		&apos;django.contrib.sessions&apos;,</div><div class="line">		&apos;django.contrib.messages&apos;,</div><div class="line">		&apos;django.contrib.staticfiles&apos;,</div><div class="line">																				    )</div><div class="line"></div><div class="line">		MIDDLEWARE_CLASSES = (</div><div class="line">			&apos;django.contrib.sessions.middleware.SessionMiddleware&apos;,</div><div class="line">			&apos;django.middleware.common.CommonMiddleware&apos;,</div><div class="line">			&apos;django.middleware.csrf.CsrfViewMiddleware&apos;,</div><div class="line">			&apos;django.contrib.auth.middleware.AuthenticationMiddleware&apos;,</div><div class="line">			&apos;django.contrib.auth.middleware.SessionAuthenticationMiddleware&apos;,</div><div class="line">			&apos;django.contrib.messages.middleware.MessageMiddleware&apos;,</div><div class="line">			&apos;django.middleware.clickjacking.XFrameOptionsMiddleware&apos;,</div><div class="line">			&apos;django.middleware.security.SecurityMiddleware&apos;,</div><div class="line">			)</div><div class="line"></div><div class="line">		ROOT_URLCONF = &apos;leopard.urls&apos;</div><div class="line"></div><div class="line">		TEMPLATES = [</div><div class="line">			&#123;</div><div class="line">				&apos;BACKEND&apos;: &apos;django.template.backends.django.DjangoTemplates&apos;,</div><div class="line">				&apos;DIRS&apos;: [],</div><div class="line">				&apos;APP_DIRS&apos;: True,</div><div class="line">				&apos;OPTIONS&apos;: &#123;</div><div class="line">					&apos;context_processors&apos;: [</div><div class="line">					&apos;django.template.context_processors.debug&apos;,</div><div class="line">					&apos;django.template.context_processors.request&apos;,</div><div class="line">					&apos;django.contrib.auth.context_processors.auth&apos;,</div><div class="line">					&apos;django.contrib.messages.context_processors.messages&apos;,</div><div class="line">					],</div><div class="line">				&#125;,</div><div class="line">			&#125;,</div><div class="line">		]</div><div class="line"></div><div class="line">		WSGI_APPLICATION = &apos;leopard.wsgi.application&apos;</div><div class="line"></div><div class="line">		# Internationalization</div><div class="line">		# https://docs.djangoproject.com/en/1.8/topics/i18n/</div><div class="line"></div><div class="line">		LANGUAGE_CODE = &apos;en-us&apos;</div><div class="line"></div><div class="line">		TIME_ZONE = &apos;UTC&apos;</div><div class="line"></div><div class="line">		USE_I18N = True</div><div class="line"></div><div class="line">		USE_L10N = True</div><div class="line"></div><div class="line">		USE_TZ = True</div><div class="line"></div><div class="line">		# Static files (CSS, JavaScript, Images)</div><div class="line">		# https://docs.djangoproject.com/en/1.8/howto/static-files/</div><div class="line"></div><div class="line">		STATIC_URL = &apos;/static/&apos;</div><div class="line"></div><div class="line">		STATE_REDIS_DB = 6</div><div class="line"></div><div class="line">		LOGGING = &#123;</div><div class="line">			&apos;version&apos;: 1,</div><div class="line">			&apos;disable_existing_loggers&apos;: False,</div><div class="line">			&apos;formatters&apos;: &#123;</div><div class="line">				&apos;verbose&apos;: &#123;</div><div class="line">					&apos;format&apos;: &apos;%(levelname)s %(asctime)s %(module)s %(funcName)s %(message)s&apos;</div><div class="line">				&#125;,</div><div class="line">			&#125;,</div><div class="line">			&apos;handlers&apos;: &#123;</div><div class="line">				&apos;mail_admins&apos;: &#123;</div><div class="line">					&apos;level&apos;: &apos;ERROR&apos;,</div><div class="line">					&apos;class&apos;: &apos;django.utils.log.AdminEmailHandler&apos;</div><div class="line">				&#125;,</div><div class="line">				&apos;console&apos;: &#123;</div><div class="line">					&apos;level&apos;: &apos;DEBUG&apos;,</div><div class="line">					&apos;class&apos;: &apos;logging.StreamHandler&apos;</div><div class="line">					&#125;,</div><div class="line">				&apos;realtimejob_file&apos;: &#123;</div><div class="line">					&apos;level&apos;: &apos;INFO&apos;,</div><div class="line">					&apos;class&apos;: &apos;logging.handlers.RotatingFileHandler&apos;,</div><div class="line">					&apos;filename&apos;: &apos;filename&apos;,</div><div class="line">					&apos;formatter&apos;: &apos;verbose&apos;</div><div class="line">					&#125;,</div><div class="line">				&#125;,</div><div class="line">			&apos;loggers&apos;: &#123;</div><div class="line">				&apos;django.request&apos;: &#123;</div><div class="line">					&apos;handlers&apos;: [&apos;mail_admins&apos;],</div><div class="line">					&apos;level&apos;: &apos;ERROR&apos;,</div><div class="line">					&apos;propagate&apos;: True,</div><div class="line">				&#125;,</div><div class="line">				&apos;django_crontab.crontab&apos;: &#123;</div><div class="line">					&apos;handlers&apos;: [&apos;console&apos;],</div><div class="line">						&apos;level&apos;: &apos;DEBUG&apos;,</div><div class="line">						&apos;propagate&apos;: True,</div><div class="line">						&#125;,</div><div class="line">					&apos;realtimejob&apos;: &#123;</div><div class="line">						&apos;handlers&apos;: [&apos;realtimejob_file&apos;],</div><div class="line">						&apos;level&apos;: &apos;INFO&apos;,</div><div class="line">						&apos;propagate&apos;: False</div><div class="line">						&#125;,</div><div class="line">			&#125;,</div><div class="line">		&#125;</div><div class="line"></div><div class="line"></div><div class="line">class Dev(Common):</div><div class="line">	  DEBUG = True</div><div class="line">	  DATABASES = &#123;</div><div class="line">		&apos;default&apos;: &#123;</div><div class="line">			&apos;ENGINE&apos;: &apos;django.db.backends.mysql&apos;,</div><div class="line">			&apos;NAME&apos;: &apos;name&apos;,</div><div class="line">			&apos;USER&apos;: &apos;user&apos;,</div><div class="line">			&apos;PASSWORD&apos;: &apos;password&apos;,</div><div class="line">			&apos;HOST&apos;: &apos;host&apos;,</div><div class="line">			&apos;PORT&apos;: &apos;port&apos;,</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">    STATE_REDIS_HOST = &apos;127.0.0.1&apos;</div><div class="line">	STATE_REDIS_PORT = 6379</div><div class="line">	STATE_REDIS_SOCKET_TIMEOUT = 1000</div><div class="line"></div><div class="line"></div><div class="line">class Prod(Common):</div><div class="line">	# Database</div><div class="line">	# https://docs.djangoproject.com/en/1.8/ref/settings/#databases</div><div class="line">	DATABASES = &#123;</div><div class="line">		&apos;default&apos;: &#123;</div><div class="line">			&apos;ENGINE&apos;: &apos;django.db.backends.mysql&apos;,</div><div class="line">			&apos;NAME&apos;: &apos;name&apos;,</div><div class="line">			&apos;USER&apos;: &apos;user&apos;,</div><div class="line">			&apos;PASSWORD&apos;: &apos;password&apos;,</div><div class="line">			&apos;HOST&apos;: &apos;host&apos;,</div><div class="line">			&apos;PORT&apos;: &apos;port&apos;,</div><div class="line">		&#125;</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<p>manager.py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">if __name__ == &quot;__main__&quot;:</div><div class="line">    os.environ.setdefault(&quot;DJANGO_SETTINGS_MODULE&quot;, &quot;settings&quot;)</div><div class="line">	os.environ.setdefault(&apos;DJANGO_CONFIGURATION&apos;, &apos;Dev&apos;)  #设置默认的环境</div><div class="line"></div><div class="line">	from configurations.management import execute_from_command_line # 引入需要的包</div><div class="line"></div><div class="line">	execute_from_command_line(sys.argv) # 使用 django-configurations 来启动</div></pre></td></tr></table></figure></p>
<p>wsgi.py 修改同上所示<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">import os</div><div class="line">from configurations.wsgi import get_wsgi_application # 引入相关的包</div><div class="line"></div><div class="line">os.environ.setdefault(&quot;DJANGO_SETTINGS_MODULE&quot;, &quot;settings&quot;)</div><div class="line">os.environ.setdefault(&apos;DJANGO_CONFIGURATION&apos;, &apos;Dev&apos;)</div><div class="line"></div><div class="line">application = get_wsgi_application() # 使用 django-configurations 中的如括进行启动</div></pre></td></tr></table></figure></p>
<p>对于希望针对不同的环境引入不同的变量值（变量名字一样），可以新建一个 commonsettings.py 文件。demo 如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">import settings</div><div class="line">import os</div><div class="line"></div><div class="line">DJANGO_CONFIGURATION_KEY = &apos;DJANGO_CONFIGURATION&apos;</div><div class="line">STATE_REDIS_HOST = getattr(settings, os.environ.get(DJANGO_CONFIGURATION_KEY, &apos;Dev&apos;)).STATE_REDIS_HOST</div><div class="line">STATE_REDIS_PORT = getattr(settings, os.environ.get(DJANGO_CONFIGURATION_KEY, &apos;Dev&apos;)).STATE_REDIS_PORT</div><div class="line">STATE_REDIS_DB = getattr(settings, os.environ.get(DJANGO_CONFIGURATION_KEY, &apos;Dev&apos;)).STATE_REDIS_DB</div><div class="line">STATE_REDIS_SOCKET_TIMEOUT = getattr(settings, os.environ.get(DJANGO_CONFIGURATION_KEY, &apos;Dev&apos;)).STATE_REDIS_SOCKET_TIMEOUT</div></pre></td></tr></table></figure>
<p>这样，所有的人从 commonsettings.py 进行引入即可，不需要关心使用的那个环境（commonsettings.py 中没有处理异常，希望对于异常情况提前抛出来）</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Django-中统一配置的做法&quot;&gt;&lt;a href=&quot;#Django-中统一配置的做法&quot; class=&quot;headerlink&quot; title=&quot;Django 中统一配置的做法&quot;&gt;&lt;/a&gt;Django 中统一配置的做法&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;本文主要描述如何在 django 中统一 settings 文件&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h1&gt;&lt;p&gt;django 服务会有多个环境，比如 开发环境、测试环境以及线上环境等。现在大部分使用的方案是针对每一种环境使用一个 settings 文件，然后在不同的环境中使用不同的 settings 文件。这样的设计我认为有至少两个问题:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;很多公用的配置不太好公用&lt;/li&gt;
&lt;li&gt;文件数会很多，项目中管理会比较麻烦&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="django" scheme="http://yoursite.com/tags/django/"/>
    
      <category term="settings" scheme="http://yoursite.com/tags/settings/"/>
    
      <category term="code-style" scheme="http://yoursite.com/tags/code-style/"/>
    
  </entry>
  
  <entry>
    <title>spark_dagscheduler</title>
    <link href="http://yoursite.com/2017/10/16/spark-dagscheduler/"/>
    <id>http://yoursite.com/2017/10/16/spark-dagscheduler/</id>
    <published>2017-10-16T12:01:20.000Z</published>
    <updated>2017-10-18T03:39:10.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>基于 spark 1.6</p>
</blockquote>
<p>面向 Stage 的调度器，负责计算每个 job 的 DAG，并将 DAG 图划分为不同的 stage，对哪些 RDD 以及相应输出进行记录，寻找一个运行相应 job 所需要的最小 stage。然后将 <code>stage</code> 以 <code>TaskSet</code> 的形式提交到下层的 <code>TaskScheduler</code> 进行具体的 task 调度。每个 <code>TaskSet</code> 包含整个可以独立运行的 task，这些 task 能够利用集群上已有的数据立即运行起来，如果集群上已有的数据已经不存在了，那么当前 task 就会失败。</p>
<p>Spark 的 stage 以 RDD 的 shuffle 为界进行划分。窄依赖的 RDD 操作会被穿起来放到一个 task 中，比如 <code>map()</code>, <code>filter()</code> 这样的操作。但是需要使用到 shuffle 依赖的操作，需要多个 stage（至少一个将中间文件写到特定的地方，另外一个从特定的地方进行读取）。每个 Stage，只会对其他 Stage 有 shuffle 依赖，在同一个 stage 中会进行很多计算。实际的将计算串起来的操作在 RDD.compute 中完成。</p>
<p><code>DAGScheduler</code> 同样会基于缓存状态决定 task 希望运行在那（preferred location），如果 shuffle 输出文件丢失造成的 Stage 失败，会重新被提交。在 <strong>Stage 内部</strong> 的不是由 shuffle 文件丢失造成的失败，由 <code>TaskScheduler</code> 来完成，<code>TaskScheduler</code> 会在取消整个 stage 前进行小部分重试。<br><a id="more"></a><br>下面的几个核心概念：</p>
<ul>
<li>Jobs (通过 <code>ActiveJob</code> 来表示）是提交给调度器中最上次工作单元。比如，用户触发一次 action，比如 <code>count()</code>，的时候，就会提交一个 Job。每个 Job 可能会包含多个 stage</li>
<li>Stages 是 Job 中产生中间结果的一系列 Task 的集合，同一个 Stage 的每个 Task 运行着同样的逻辑，只是处理同一个 RDD 的不同分区。Stage 以 Shuffle 为界（后面的 Stage 必须等前面的 Stage 运行完才能继续往下进行）。现在有两种 Stage：<code>ResultStage</code>，Job 的最终执行 action 的 Stage，<code>ShuffleMapState</code> 产生中间文件的 shuffle。如果多 Job 公用同一个 RDD 的话，Stage 可能会在多个 Job 间共享。</li>
<li>Tasks 是组小的独立工作单元，每个 Task 会被独立的分发到具体的机器上运行</li>
<li>Cache tracking: <code>DAGScheduler</code> 会记录哪些 RDD 以及被缓存过，从而避免重复计算，也会记录哪些 Stage 已经生成过输出文件从而避免重复操作</li>
<li>Preferred locations: <code>DAGScheduler</code> 会根据计算所依赖的 RDD 数据、缓存的地址以及 shuffle 输出结果对 Task 进行调度</li>
<li>Cleanup: 如果上游依赖的 Job 处理完成后，下游的数据会被清理掉，防止长驻服务的内存泄漏</li>
</ul>
<p>为了能够从错误中进行恢复，同一个 Stage 可能会被运行多次，每一次就是一个 “attempts”。如果 <code>TaskScheduler</code> 汇报某个 task 失败的原因是因为依赖的前一个 Stage 的输出文件已经不见了，那么 <code>DAGScheduler</code> 会对前一个 Stage 重新进行提交。这通过 <code>CompletionEveent</code> 以及 <code>FetchFailed</code> 或者 <code>ExecutorLost</code> 事件来完成。<code>DAGScheduler</code> 会等待一小段时间来判断是否还有其他 task 需要重试，然后将所有失败的 task 进行重试。在这一过程中，我们需要重新对之前清理过的 Stage 进行计算。由于之前的 “attempt” 可能还在运行，所以需要特别注意</p>
<img src="/2017/10/16/spark-dagscheduler/procesure.jpg" alt="procesure.jpg" title="">
<p>上面的图是 DAGScheduler.scala 的主题脉络，当然还包括其他诸如 <code>taskStarted</code>, <code>taskGettingResult</code>, <code>taskEnded</code>, <code>executorZHeatbeatReceived</code>, <code>executorLost</code>, <code>executorAdded</code>, <code>taskSetFailed</code>, 等函数</p>
<blockquote>
<p>其中带箭头的虚线为消息调用；带箭头的实线为直接调用，无箭头的实线表示方法申明和内部的主要实现（比如 <code>getShuffleMapStage</code> 中包括了 <code>getAncestorShuffleDependencies</code> 和 <code>newOrUsedShuffleStage</code>)</p>
</blockquote>
<h4 id="上面是注释，下面是代码解释"><a href="#上面是注释，下面是代码解释" class="headerlink" title="上面是注释，下面是代码解释"></a>上面是注释，下面是代码解释</h4><p>入口在 <code>runJob</code>，<code>runJob</code> 会调用 <code>submitJob(rdd, func, partitions, callSite, resultHandler, properties)</code> 返回一个 waiter，等待处理完成</p>
<p><code>submitJob</code> 核心代码如下，主要生成一个 waiter 对象，然后发送一个 <code>JobSubmitted</code> 信号</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) =&gt; _]</div><div class="line">val waiter = new JobWaiter(this, jobId, partitions.size, resultHandler)</div><div class="line">eventProcessLoop.post(JobSubmitted(jobId, rdd, func2, partitions.toArray, callSite, waiter, SerializationUtils.clone(properties)))</div></pre></td></tr></table></figure>
<p><code>JobSubmitted</code> 方法会将整个 DAG 图进行 Stage 的划分，然后提交 finalStage（也就是 action 所在的 Stage），其中 <code>newResultStage</code> 会进行具体的 Stage 划分，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">try &#123;</div><div class="line">	// New stage creation may throw an exception if, for example, jobs are run on a</div><div class="line">	// HadoopRDD whose underlying HDFS files have been deleted.</div><div class="line">	finalStage = newResultStage(finalRDD, func, partitions, jobId, callSite)</div><div class="line">&#125; catch &#123;</div><div class="line">	case e: Exception =&gt;</div><div class="line">		logWarning(&quot;Creating new stage failed due to exception - job: &quot; + jobId, e)</div><div class="line">		listener.jobFailed(e)</div><div class="line">		return</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>submitJob</code> 返回的 <code>JobWaiter</code>，JobWaiter 用于控制 Job，以及 Job 结束后进行相应的状态更新</p>
<p><code>newResultStage</code> 会将所有的 Stage 划分出来（通过 <code>getParentStagesAndId</code> 函数，<code>getParentStages</code> 进行具体的 Stage 划分），其中 <code>getParentStages</code> 进行 BFS 进行查找（这个地方 BFS 和 DFS 有什么区别？）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">private def getParentStages(rdd: RDD[_], firstJobId: Int): List[Stage] = &#123;</div><div class="line">	val parents = new HashSet[Stage]</div><div class="line">	val visited = new HashSet[RDD[_]]</div><div class="line">	// We are manually maintaining a stack here to prevent StackOverflowError</div><div class="line">	// caused by recursively visiting</div><div class="line">	val waitingForVisit = new Stack[RDD[_]]</div><div class="line">	def visit(r: RDD[_]) &#123;</div><div class="line">		if (!visited(r)) &#123;</div><div class="line">			visited += r</div><div class="line">			// Kind of ugly: need to register RDDs with the cache here since</div><div class="line">			// we can&apos;t do it in its constructor because # of partitions is unknown</div><div class="line">			for (dep &lt;- r.dependencies) &#123;</div><div class="line">				dep match &#123;</div><div class="line">					case shufDep: ShuffleDependency[_, _, _] =&gt;</div><div class="line">						parents += getShuffleMapStage(shufDep, firstJobId)</div><div class="line">					case _ =&gt;</div><div class="line">						waitingForVisit.push(dep.rdd)</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	waitingForVisit.push(rdd)</div><div class="line">	while (waitingForVisit.nonEmpty) &#123;</div><div class="line">		visit(waitingForVisit.pop())</div><div class="line">	&#125;</div><div class="line">	parents.toList</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>getParentStages 返回某个 RDD 的所有依赖的 stage（直接和间接的），stage 以 getShuffleMapStage() 返回为准</p>
<p><code>getShuffleMapStage</code> 首先从 shuffleToMapStage(shuffleid 到 stage 的 map 结构）中查找，没有找到就以 shuffleDep.rdd 为起始点建立一个依赖关系，并且将整条依赖链上的东西都建立起来</p>
<p><code>getAncestorShuffleDependencies</code> 会以一个 RDD 为起点，找到 RDD 直接&amp;间接 依赖的所有 shuffleDependency</p>
<p><code>getAncestorShuffleDependencies</code> 和  <code>getParentStages</code> 类似但又不一样，前者在搜索的时候，每次都需要把 rdd.dep 入队，而后者只需要将 narrowdependency 的入队。还有为什么两个函数一个结果保存为 Set，一个是 Stack？</p>
<p><code>newShuffleMapStage</code> 中会更新 job 和 stage，以及 stage 和 job 的关系，每个 stage 属于哪些 job，每个 job 包含哪些 stage</p>
<p>整个类中有一个变量 <code>mapOutputTracker :MapOutputTracker</code> 用于记录 shuffle 的结果以及位置</p>
<p><code>MapOutputTracker</code> 记录 shuffleMapStage 的 output location  ，为啥要两个 status map（一个 MapStatus，一个 CachedSerializedStatus，后者是前者的序列化之后的结果）,这些 Map 都是带 ttl 的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">/* org.apache.spark.MapOutputTracker.scala</div><div class="line">* Class that keeps track of the location of the map output of</div><div class="line">* a stage. This is abstract because different versions of MapOutputTracker</div><div class="line">* (driver and executor) use different HashMap to store its metadata.</div></pre></td></tr></table></figure>
<p>Driver 使用 MapOutputTrackerMaster 跟踪 output location，只有所有 partition 的 output location 都就绪了，整个被依赖的 RDD 才是就绪的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">* MapOutputTracker for the driver. This uses TimeStampedHashMap to keep track of map</div><div class="line">* output information, which allows old output information based on a TTL.</div></pre></td></tr></table></figure>
<p>Executor 则使用 <code>MapOutputTrackerWorker</code> 从 Driver 获取 map output 的相应信息</p>
<p><code>newOrUsedShuffleStage</code> 函数中首先查找该 shuffleMapStage 是否注册过 MapOutputTracker，如果注册过就直接获取，如果没有注册过就进行注册。<br>MapOutputTracker 有两个 Map 结构，一个是原始的 partition location，一个是序列化之后的（用于加速？），这两个 map 包含过期清理策略，用于节省空间</p>
<p>定期清理的 Meta 信息包括如下几种：</p>
<ul>
<li>MAP_OUTPUT_TRACKER, </li>
<li>SPARK_CONTEXT, </li>
<li>HTTP_BROADCAST, </li>
<li>BLOCK_MANAGER,</li>
<li>SHUFFLE_BLOCK_MANAGER, </li>
<li>BROADCAST_VARS</li>
</ul>
<h3 id="上面是一条链路上的相关函数，下面包括一些其他的处理"><a href="#上面是一条链路上的相关函数，下面包括一些其他的处理" class="headerlink" title="上面是一条链路上的相关函数，下面包括一些其他的处理"></a>上面是一条链路上的相关函数，下面包括一些其他的处理</h3><h4 id="handleTaskCompletion-处理-Task-comple-的信息（不分成功和失败）"><a href="#handleTaskCompletion-处理-Task-comple-的信息（不分成功和失败）" class="headerlink" title="handleTaskCompletion 处理 Task comple 的信息（不分成功和失败）"></a><code>handleTaskCompletion</code> 处理 Task comple 的信息（不分成功和失败）</h4><p>task complete 会分几种信息：</p>
<ul>
<li><p>Success：<br>首先将 task 从 pendingTask 中去掉<br>  task 分为两种：</p>
<ul>
<li>ResultTask：<br>将 job 对应的当前 task 标记为 true（如果没有标记过的话），如果整个 job 都处理完成，就将 stage 标记为完成</li>
<li><p>ShuffleMapTask：<br>更新 outputLocation （当前 shuffleMapTask 的输出）<br>如果 ShuffleMapTask 的所有 partition 都处理完成，就将当前的 stage 标记为完成<br>这里为了防止是重复进行计算（之前失败过），需要重新进行 outputLocation 的注册（主要是增加 epoch）<br>如果当前当前的 Stage.isAvailable 为 true 就通知所有依赖该 stage 的 stage 可以继续工作了。否则重新提交失败的 task（注意这里available 和上面的 finish 不一样，available 是以 output 是否能够获取到为准）</p>
</li>
<li><p>Resubmitted：<br>将 task 加到 pendingTask 中即可，等待后续的调度</p>
</li>
<li><p>FetchFailed：<br>首先判断失败 task 的 attemp 是否是当前的 attemp，不是就忽略，然后判断当前 stage 是否正在运行，如果不是，忽略。<br>否则将当前的 stage 标记为 finish，然后将进行 mapStage.removeOutputLoc 以及 mapOutputTracker.unregisterMapOutput。<br>如果同一个 executor 上的 fetchFailed 很多（这个有调用方判断），则将所在的 executor 标记为 Failed</p>
</li>
<li><p>其他信息，直接忽略</p>
</li>
</ul>
</li>
</ul>
<h4 id="ExecutorLost"><a href="#ExecutorLost" class="headerlink" title="ExecutorLost"></a>ExecutorLost</h4><p>如果上报的 executor 之前没有上报过（会有一个 Map 记录所有上报过的 executor），或者之前上报的该 executor 对应的 epoch 小于 currentepoch， 则需要进行处理</p>
<p>首先从 blockManagerMaster 中将当前 executor 删除</p>
<p>如果没有启动 externalShuffleService（开启 <code>DynamicAllocation</code> 后需要开启，不然会出问题），或者 fetchFailed（由调用方设置），则进行下面的操作：</p>
<p><code>if (!env.blockManager.externalShuffleServiceEnabled || fetchFailed)</code></p>
<p>所该 executor 上所有的输出都进行标记删除，并且增加 <code>mapOutputTracker</code> 的 epoch</p>
<h4 id="ExecutorAdded"><a href="#ExecutorAdded" class="headerlink" title="ExecutorAdded"></a>ExecutorAdded</h4><p>  如果当前添加的 executor 是马上需要回收的，那么就从即将回收的 map 中删除，防止回收，否则不需要操作</p>
<h4 id="StageCancellation"><a href="#StageCancellation" class="headerlink" title="StageCancellation"></a>StageCancellation</h4><p>  如果有正在运行的 job 依赖当前 stage，则将所有的 job 标记为 cancel</p>
<h4 id="JobCancellation"><a href="#JobCancellation" class="headerlink" title="JobCancellation"></a>JobCancellation</h4><p>  将该 job 以及只由该 job 依赖的 stage 都标记为 failed</p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>  <code>spark.stage.maxConsecutiveAttempts</code> 表示一个 stage 尝试多少次之后会被标记为失败</p>
<p>  SparkContext 中会根据模式生成和注册相应的 backend 以及 taskscheduler</p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ol>
<li>如果一个 Stage 有多个 RDD，那么这些 RDD 是在同一个 TaskSet 中吗</li>
<li>如何模拟 <code>r2 = r0.reduceByKey; r3 = r1.reduceByKey; r4 = r2.map(xx); r5 = r4.union(r3); r6 = r5.map; r7 = r6.reduceByKey</code> 的 Stage 划分和生成（会有多少个 Stage，每个 Stage 分别包含哪些 RDD，以及整个 DAG 怎么整合起来的）</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;基于 spark 1.6&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;面向 Stage 的调度器，负责计算每个 job 的 DAG，并将 DAG 图划分为不同的 stage，对哪些 RDD 以及相应输出进行记录，寻找一个运行相应 job 所需要的最小 stage。然后将 &lt;code&gt;stage&lt;/code&gt; 以 &lt;code&gt;TaskSet&lt;/code&gt; 的形式提交到下层的 &lt;code&gt;TaskScheduler&lt;/code&gt; 进行具体的 task 调度。每个 &lt;code&gt;TaskSet&lt;/code&gt; 包含整个可以独立运行的 task，这些 task 能够利用集群上已有的数据立即运行起来，如果集群上已有的数据已经不存在了，那么当前 task 就会失败。&lt;/p&gt;
&lt;p&gt;Spark 的 stage 以 RDD 的 shuffle 为界进行划分。窄依赖的 RDD 操作会被穿起来放到一个 task 中，比如 &lt;code&gt;map()&lt;/code&gt;, &lt;code&gt;filter()&lt;/code&gt; 这样的操作。但是需要使用到 shuffle 依赖的操作，需要多个 stage（至少一个将中间文件写到特定的地方，另外一个从特定的地方进行读取）。每个 Stage，只会对其他 Stage 有 shuffle 依赖，在同一个 stage 中会进行很多计算。实际的将计算串起来的操作在 RDD.compute 中完成。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;DAGScheduler&lt;/code&gt; 同样会基于缓存状态决定 task 希望运行在那（preferred location），如果 shuffle 输出文件丢失造成的 Stage 失败，会重新被提交。在 &lt;strong&gt;Stage 内部&lt;/strong&gt; 的不是由 shuffle 文件丢失造成的失败，由 &lt;code&gt;TaskScheduler&lt;/code&gt; 来完成，&lt;code&gt;TaskScheduler&lt;/code&gt; 会在取消整个 stage 前进行小部分重试。&lt;br&gt;
    
    </summary>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="source_code" scheme="http://yoursite.com/tags/source-code/"/>
    
      <category term="dagscheduler" scheme="http://yoursite.com/tags/dagscheduler/"/>
    
  </entry>
  
  <entry>
    <title>GC 引用计数法</title>
    <link href="http://yoursite.com/2017/09/22/GC-%E5%BC%95%E7%94%A8%E8%AE%A1%E6%95%B0%E6%B3%95/"/>
    <id>http://yoursite.com/2017/09/22/GC-引用计数法/</id>
    <published>2017-09-22T12:48:06.000Z</published>
    <updated>2017-10-16T12:19:17.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文将将介绍 GC 算法中最基本的三种算法的第二种：引用计数法。</p>
<h1 id="GC-引用计数法"><a href="#GC-引用计数法" class="headerlink" title="GC 引用计数法"></a>GC 引用计数法</h1><p>GC 本来是一种“释放已经没有被引用的对象的机制”，人们自然就想到，通过记录所有对象的引用次数来进行，这就是 GC 引用计数法。</p>
<p>引用计数法，一句话来说就是，记录每个对象被引用的多少次，如果引用次数为零，就编程“垃圾”了，可以被回收。</p>
<p>下面给出引用计数法的基本代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">//分配对象</div><div class="line">new_object(size) &#123;</div><div class="line">    obj = pickup_chunk(size, $free_list) //从 free_list 中查找一个符合大小的块</div><div class="line">    </div><div class="line">    if (obj == NULL) //分配失败</div><div class="line">        allocation_fail()</div><div class="line">    else //分配成功</div><div class="line">        obj.ref_cnt = 1 //更新引用计数</div><div class="line">        return ojb</div><div class="line">&#125;</div><div class="line"></div><div class="line">//更新指针</div><div class="line">update_ptr(ptr, obj) &#123;</div><div class="line">    inc_ref_cnt(obj)  // 更新 obj 的引用</div><div class="line">    dec_ref_cnt(*ptr) // 更新 *ptr 的引用，注意这两句话的顺序，思考是否可以调换顺序</div><div class="line">    *ptr = obj</div><div class="line">&#125;</div><div class="line"></div><div class="line">inc_ref_cnt(obj) &#123;</div><div class="line">    obj.ref_cnt++</div><div class="line">&#125;</div><div class="line"></div><div class="line">dec_ref_cnt(obj) &#123;</div><div class="line">    obj.ref_cnt-- // 更新引用计数</div><div class="line">    if(obj.ref_cnt == 0) //如果该对象变成了 “垃圾”，进行回收</div><div class="line">        for (child: children(obj)) //更新所有子节点的引用</div><div class="line">            dec_ref_cnt(*child)</div><div class="line">        reclaim(obj)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>GC 引用计数法的大致流程如上所述，其中 update_ptr 可以用下图描述</p>
<h1 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h1><p>对于引用计数法，我们可以来看看它到底有哪些优缺点呢？</p>
<h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ol>
<li><p>可即刻回收垃圾。</p>
<p> 在引用计数法中，每个对象都知道自己的被引用数，一旦计数变为 0，就会被链接到空闲链表中。换句话说，所有对象在被标记为垃圾的同时就会被回收。</p>
</li>
<li><p>最大暂停时间短</p>
<p> 只有更新指针的时候才会执行垃圾回收，</p>
</li>
<li></li>
</ol>
<h1 id="优化方案"><a href="#优化方案" class="headerlink" title="优化方案"></a>优化方案</h1>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文将将介绍 GC 算法中最基本的三种算法的第二种：引用计数法。&lt;/p&gt;
&lt;h1 id=&quot;GC-引用计数法&quot;&gt;&lt;a href=&quot;#GC-引用计数法&quot; class=&quot;headerlink&quot; title=&quot;GC 引用计数法&quot;&gt;&lt;/a&gt;GC 引用计数法&lt;/h1&gt;&lt;p&gt;GC 本来是
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>GC 标记-清除算法</title>
    <link href="http://yoursite.com/2017/09/17/GC-%E6%A0%87%E8%AE%B0-%E6%B8%85%E9%99%A4%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2017/09/17/GC-标记-清除算法/</id>
    <published>2017-09-17T00:51:40.000Z</published>
    <updated>2017-09-17T12:36:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>GC 的世界中有三种基本的算法，分别是：</p>
<ul>
<li>标记清除</li>
<li>引用计数</li>
<li>GC 复制</li>
</ul>
<p>其他的 GC 算法都是在这三种算法上进行修改，优化得来。本文将要介绍的是 标记-清除 算法。</p>
<a id="more"></a>
<h1 id="标记清除算法介绍"><a href="#标记清除算法介绍" class="headerlink" title="标记清除算法介绍"></a>标记清除算法介绍</h1><p>像字面意思一样，标记-清除算法分为两步：标记和清除。其中标记和清除使用伪代码分别可以写出来如下：</p>
<p>标记</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">mark_phase() &#123;</div><div class="line">    for (r: $roots)</div><div class="line">       mark(*r)</div><div class="line">&#125;</div><div class="line"></div><div class="line">mark(obj)&#123;</div><div class="line">    if(obj.mark == FALSE)</div><div class="line">        obj.mark == TRUE</div><div class="line">        for(child: children(obj))</div><div class="line">            mark(*child)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>清除</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">sweep_phase() &#123;</div><div class="line">    sweeping = $head_start    //从堆头开始清除</div><div class="line">    while (sweeping &lt; $head_end)</div><div class="line">        if (sweeping.mark == TRUE) //如果当前对象是活跃对象</div><div class="line">            sweeping.mark = FALSE</div><div class="line">        else  //如果当前对象是可清除对象，则将其加入到 free_list 中</div><div class="line">            sweeping.next = $free_list   </div><div class="line">            $free_list = sweep</div><div class="line">        sweeping += sweeping.size</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>对于标记清除算法，最基本的，将某个对象是否已经被标记，以及对象的大小等等这些信息都保存在头部，从上面的代码中，我们可以知道头部至少有三个域：标记位(名为 mark)、对象大小（名为 size）以及下一个对象的头部地址（名为 next）– 有 size 和 next 两个域，是因为下一个对象不一定在物理上是连续的。如下图所示</p>
<img src="/2017/09/17/GC-标记-清除算法/heap.png" alt="heap.png" title="">
<p>其中淡蓝色的表示还在使用的空间，白色的表示空闲空间。</p>
<h2 id="标记算法的选取"><a href="#标记算法的选取" class="headerlink" title="标记算法的选取"></a>标记算法的选取</h2><p>在标记部分，我们从 root 节点触发，然后逐一标记哪个节点不再使用，哪些节点还需要在继续存活。在上述代码中，我们给出的是 DFS 搜素算法，那么对于这一块我们可以比较 DFS 和 BFS 的区别，大致可以用下图表示：</p>
<img src="/2017/09/17/GC-标记-清除算法/BFS_DFS.jpeg" alt="BFS_DFS.jpeg" title=""> 
<p>主要区别在于：<strong>DFS 需要保存的内存使用量更低</strong></p>
<h2 id="分配内存"><a href="#分配内存" class="headerlink" title="分配内存"></a>分配内存</h2><p>所有的标记、清除等都是为了分配内存服务的，如果我们不需要再分配内存的话，那么就可以不进行前面哪些活动了。下面说说如何分配内存的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">new_obj(size) &#123;</div><div class="line">    chunk = pickup_chunk(size, $free_list) // 从上面的空闲列表中找一个合适的内存块</div><div class="line">    if (chunk != NULL)</div><div class="line">        return chunk</div><div class="line">    else</div><div class="line">        allocation_fail() //内存不够</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上面的代码表示整个分配内存的过程，其中 <code>pickup_chunk</code> 表示从空闲列表中找出一个合适的内存块，返回给申请者。</p>
<p>对于找出一个 <strong>合适</strong> 的内存块，我们已知的至少有三种方法：</p>
<ul>
<li>First-fit 返回最先找到的一个内存块</li>
<li>Best-fit  找到一个大于需要内存的最小内存块</li>
<li>Worst-fit 找到一个大于需要内存的最大内存块</li>
</ul>
<p>上面三种每一种都有不同的应用场景，也各有优劣。</p>
<h2 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h2><p>对于清除阶段，我们会遍历堆中所有的内存区域，将不需要的加入到 <code>free_list</code> 中，对于连续出现的两个内存块，我们并没有进行任何操作，那么下一次我们申请内存的时候，可能会由于没有足够大的内存块而失败。比如我们有两个大小分别为 3，4 的内存块，然后，我们需要申请一个内存大小为 5 的内存块，在之前的算法中是会失败的。这就牵涉到清除阶段的合并了，将连续的内存块进行合并形成大的内存块。上面的清除阶段算法改成如下所示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">sweep_phase() &#123;</div><div class="line">    sweeping = $head_start    //从堆头开始清除</div><div class="line">    while (sweeping &lt; $head_end)</div><div class="line">        if (sweeping.mark == TRUE) //如果当前对象是活跃对象</div><div class="line">            sweeping.mark = FALSE</div><div class="line">        else  //如果当前对象是可清除对象，则将其加入到 free_list 中</div><div class="line">            if(sweeping == $free_list + $free_list.size) //邻接块的合并</div><div class="line">                $free_list.size + sweeping.size</div><div class="line">            else</div><div class="line">                sweeping.next = $free_list</div><div class="line">                $free_list = sweep</div><div class="line">        sweeping += sweeping.size</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h1><p>基本算法基本描述完成，接下来可以看看这种算法的优缺点分别是什么，以及是否有办法进行优化</p>
<p>优点：</p>
<ol>
<li>实现简单。算法简单，能够明显知道是否有问题，而且更容易和其他算法进行结合</li>
<li>与保守式 GC 算法兼容。在 保守式 GC 算法中，对象是不能被移动的，刚好 GC 标记-清除算法不需要移动对象。</li>
</ol>
<p>缺点：</p>
<ol>
<li>碎片化。在标记-清除的过程中，会不断的产生碎片，虽然在清除阶段有合并过程，但还是不够。</li>
<li>分配速度。由于在 标记-清除 算法 中分块不是连续的，因此每次分配都需要遍历空闲列表，找到足够大的分块，最坏的情况需要每次都遍历整个空闲列表。</li>
<li>与写时复制技术不兼容。标记-清除算法中的标记阶段，会修改对象的头部，因此和写时复制技术不兼容，可能导致很多不必要的复制。</li>
</ol>
<h1 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h1><p>既然我们了解到了该算法的优缺点，那么在此基础上如何进行改进呢？</p>
<p>针对上面的缺点分别有如下几点改进</p>
<h2 id="多个空闲链表"><a href="#多个空闲链表" class="headerlink" title="多个空闲链表"></a>多个空闲链表</h2><p>简单的说，就是将原来一个空闲链表变成现在的多个空闲链表，加快分配的速度。<br>我们之前分配内存的时候，需要查询整个空闲链表来寻找一个合适的内存块，现在我们将不同大小的内存块挂在不同的链表下面，这样我们就能够直接去合适的链表中查找了。如下图所示</p>
<img src="/2017/09/17/GC-标记-清除算法/multilink.jpeg" alt="multilink.jpeg" title="">
<p>我们有用于 2 个字的空闲链表，有用于 3 个字的空闲链表。这样当需要分配 3 个字的空间时，我们直接去对应的链表查找即可。</p>
<p>这里有一个注意的点，需要保持多少个链表呢？一般来说，根据经验，一般会将小于某个阈值的分别生成一个链表，大于该阈值的所有内存块放到一个链表中。比如所有小于 100 字的都有一个单独的链表，而所有大于等于 100 字的内存块都挂在 100 字这个链表下。</p>
<h2 id="BiBOP-法"><a href="#BiBOP-法" class="headerlink" title="BiBOP 法"></a>BiBOP 法</h2><p>另外一种优化方法，就是将大小相近的内存块放到一起，如下图所示</p>
<img src="/2017/09/17/GC-标记-清除算法/bibop.jpeg" alt="bibop.jpeg" title="">
<p>这样的形式，我们也知道去哪个地方查找需要的内存块，但是这个方法有一点不好，就是会形成很多内存碎片，比如我们分配的很多大小为 2 个字的空间，但是所有的申请中最小的都是 3 个字，这个时候这些 2 个字的空间都是浪费的</p>
<h2 id="位图标记"><a href="#位图标记" class="headerlink" title="位图标记"></a>位图标记</h2><p>位图标记法主要改善的是“和 copy-on-write 技术不兼容”，将标记位从头部抽离出来了。如下图所示</p>
<img src="/2017/09/17/GC-标记-清除算法/bitmap.jpeg" alt="bitmap.jpeg" title="">
<p>这样标记的时候就不会修改头部位置了。伪代码大致如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">mark(obj) &#123;</div><div class="line">    obj_num = (obj - $heap_start) / WORD_LENGTH</div><div class="line">    index = obj_num / WORD_LENGTH</div><div class="line">    offset = obj_num % WORD_LENGTH</div><div class="line">    </div><div class="line">    if (($bitmap_tbl[index] &amp; (1 &lt;&lt; offset)) == 0) // 未被标记</div><div class="line">        $bitmap_tbl[index] |= (1 &lt;&lt; offset)</div><div class="line">        for (child: children(obj))</div><div class="line">            mark(*child)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<blockquote>
<p>在参考条目[1] 中有描述这个算法的第二个优势：清除操作更高效。描述如下：以往的清除操作都必须遍历整个堆，把非活动对象连接到空闲链表，同时取消活动对象的标志位。</p>
</blockquote>
<p>我的理解现在还是需要遍历整个堆，而且需要取消标志位，只是现在标志位变成了连续的，这样处理起来会高效一点。</p>
<p>另外，如果有多个堆的情况下，就需要多个 bitmap 了</p>
<h2 id="延迟清除法"><a href="#延迟清除法" class="headerlink" title="延迟清除法"></a>延迟清除法</h2><p>在清除操作中，我们需要遍历整个堆，也就是处理时间和堆的大小成正比。在清除阶段，内存空间不能访问，这就牵涉到最大暂停时间。而延迟清除法（lazy sweep）则可以缩短清除操作导致的最大暂停时间。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">new_obj(size) &#123;</div><div class="line">    chunk = lazy_sweep(size) //先通过延迟清除法，查找是否以满足条件的内存块</div><div class="line">    if (chunk != NULL)</div><div class="line">        return chunk</div><div class="line"></div><div class="line">    mark_phase() //标记阶段</div><div class="line">    </div><div class="line">    chunk = lazy_sweep(size) //再次通过延迟清除法，寻找一个满足条件的内存块</div><div class="line">    if (chunk != NULL)</div><div class="line">        return chunk</div><div class="line">    </div><div class="line">    allocation_fail()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>至于 <code>lazy_sweep</code> 函数如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">lazy_sweep(size) &#123;</div><div class="line">    while($sweeping &lt; $headp_end)</div><div class="line">        if ($sweeping.mark == TRUE) //TRUE 表示活动对象</div><div class="line">            $sweeping.mark = FALSE</div><div class="line">        else if ($sweeping.size &gt;= size)</div><div class="line">            chunk = $sweeping</div><div class="line">            $sweeping += $sweeping.size</div><div class="line">            return chunk</div><div class="line">        $sweeping += $sweeping.size</div><div class="line">    $sweeping = $head_start</div><div class="line">    return NULL</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>注意，其中的 <code>sweeping</code> 时候全局变量，因此遍历的起始位置是上次结束的地方。</p>
<p>延迟清除法有一个缺点就是。如果空闲内存块和活动对象块基本分成两个大的部分的话（如下图所示），那么对于某次清除活动对象周围的空间时，就会增加暂停时间。</p>
<img src="/2017/09/17/GC-标记-清除算法/lazy-sweep.png" alt="lazy-sweep.png" title="">
<p>上图中淡蓝色的表示活动对象，白色的表示空闲对象，当清除到活动对象附近的时候，就会增加本次的最大暂停时间。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li>《垃圾回收的算法与实现》第二章</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;GC 的世界中有三种基本的算法，分别是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;标记清除&lt;/li&gt;
&lt;li&gt;引用计数&lt;/li&gt;
&lt;li&gt;GC 复制&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其他的 GC 算法都是在这三种算法上进行修改，优化得来。本文将要介绍的是 标记-清除 算法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="GC" scheme="http://yoursite.com/tags/GC/"/>
    
      <category term="java" scheme="http://yoursite.com/tags/java/"/>
    
      <category term="标记-清除" scheme="http://yoursite.com/tags/%E6%A0%87%E8%AE%B0-%E6%B8%85%E9%99%A4/"/>
    
      <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="总结" scheme="http://yoursite.com/tags/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>tmux 简单使用指南</title>
    <link href="http://yoursite.com/2017/07/14/tmux-%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
    <id>http://yoursite.com/2017/07/14/tmux-简单使用指南/</id>
    <published>2017-07-14T06:21:37.000Z</published>
    <updated>2017-07-14T09:37:38.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Tmux-的简单使用说明"><a href="#Tmux-的简单使用说明" class="headerlink" title="Tmux 的简单使用说明"></a>Tmux 的简单使用说明</h1><blockquote>
<p>工欲善其事，必先利其器</p>
</blockquote>
<p>Tmux 是一个多窗口管理程序。可以让用户在一个地方管理多个终端。而不需要在不同的终端间来回切换。</p>
<h2 id="在-Mac-下如何安装"><a href="#在-Mac-下如何安装" class="headerlink" title="在 Mac 下如何安装"></a>在 Mac 下如何安装</h2><p>直接使用 <code>brew install tmux</code> 就可以了，如果没有 brew，则需要先安装 brew，然后再执行上述命令。</p>
<h2 id="简单使用流程"><a href="#简单使用流程" class="headerlink" title="简单使用流程"></a>简单使用流程</h2><p>首先，需要了解 tmux 中的几个概念。session，window 以及 pane。这几者的关系如下，tmux 中可以起多个 session，每个 session 可以启动多个 window，然后每个 window 可以启动多个 pane。</p>
<p>这里给一个基本的流程</p>
<ol>
<li><p>启动 tmux（默认会启动一个 session）<br>使用 <code>tmux</code> 启动 tmux，使用 <code>exit</code> 退出 tmux，session 的命名默认是从 0 开始，一直往上加</p>
</li>
<li><p>在 session 中启动一个 window<br><code>PREFIX c</code> 会在当前 session 中创建一个 window, 其中 <code>PREFIX</code> 表示 tmux 中的命令前缀符（该条命令表示，先按下 PREFIX，然后按下 c)，</p>
</li>
<li><p>在启动的 window 中创建一个 pane<br><code>PREFIX %</code> 竖直方向切分一个 window，<code>PREFIX &quot;</code> 横向切分一个 window。这样就能够在 window 中创建 pane 了。基本的这些就够了。</p>
</li>
<li><p>如何在 session，window，pane 中进行移动<br>能够创建 session，window，pane 了，接下来就是如何在 session，window，pane 间进行移动了。<br><code>PREFIX s</code> 会列出所有 session，然后进行具体的选择（可以上下移动光标，然后按 ENTRER 确定）<br><code>PREFIX w</code> 可以列出所有的 window，然后进行具体的筛选<br><code>PREFIX n</code> 可以切换到下一个 window<br><code>PREFIX p</code> 可以切换到上一个 window<br><code>PREFIX &amp;</code> 可以关闭当前 window<br><code>PREFIX o</code> 可以在 pane 之间进行跳转<br><code>tmux ls</code> 会列出当前所有的 session（在非 tmux 环境下）</p>
</li>
</ol>
<h2 id="自定义-tmux"><a href="#自定义-tmux" class="headerlink" title="自定义 tmux"></a>自定义 tmux</h2><p>tmux 的配置文件可以保存在两个地方</p>
<ol>
<li>/etc/tmux.conf</li>
<li>~/.tmux.conf</li>
</ol>
<p>其中 2 的优先级会更高，1 的影响面更广</p>
<h2 id="接下来做什么"><a href="#接下来做什么" class="headerlink" title="接下来做什么"></a>接下来做什么</h2><p>上面的仅仅是一个入门文档，也就是最少基本知识，接下来就是多实践。推荐一本小书《tmux productive mouse-free development》</p>
<img src="/2017/07/14/tmux-简单使用指南/tmux_pic.png" alt="tmux_pic.png" title="">
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Tmux-的简单使用说明&quot;&gt;&lt;a href=&quot;#Tmux-的简单使用说明&quot; class=&quot;headerlink&quot; title=&quot;Tmux 的简单使用说明&quot;&gt;&lt;/a&gt;Tmux 的简单使用说明&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;工欲善其事，必先利其器&lt;/p&gt;
&lt;
    
    </summary>
    
    
      <category term="tmux, tools" scheme="http://yoursite.com/tags/tmux-tools/"/>
    
  </entry>
  
  <entry>
    <title>风险不仅仅是事件发生的概率</title>
    <link href="http://yoursite.com/2017/06/20/%E9%A3%8E%E9%99%A9%E4%B8%8D%E4%BB%85%E4%BB%85%E6%98%AF%E4%BA%8B%E4%BB%B6%E5%8F%91%E7%94%9F%E7%9A%84%E6%A6%82%E7%8E%87/"/>
    <id>http://yoursite.com/2017/06/20/风险不仅仅是事件发生的概率/</id>
    <published>2017-06-20T14:05:59.000Z</published>
    <updated>2017-06-20T14:57:02.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="风险不仅仅是事件发生的概率"><a href="#风险不仅仅是事件发生的概率" class="headerlink" title="风险不仅仅是事件发生的概率"></a>风险不仅仅是事件发生的概率</h1><blockquote>
<p>风险可以定义为 = 事件结果对你的影响 * 事件发生的概率</p>
</blockquote>
<p>风险在生活中处处存在，可能我们会想冒个险没啥关系，反正发生的概率小，而且在某些时候会有高收益/回报伴随这风险，这个时候就更有诱惑力了，总有人希望通过冒险得到高回报，但这恰恰是不可取的，是非常危险的。</p>
<a id="more"></a>
<h2 id="到底能不能闯红灯"><a href="#到底能不能闯红灯" class="headerlink" title="到底能不能闯红灯"></a>到底能不能闯红灯</h2><blockquote>
<p>如果 遇到/见到过一次严重的交通事故，产生过后怕的感觉，那么就再也不会在这件事上冒险了</p>
</blockquote>
<pre><code>当你你急着过马路，但是现在是红灯，你会选择等待吗？如果这个时候旁边有人正在闯红灯，你还会等待吗？
</code></pre><p>上面的问题只能自己回答自己，回答很容易，但是实践起来就不那么容易了。</p>
<p>闯红灯是生活中很常见的事情，但是却隐藏了非常大的安全隐患。因为一旦发生交通事故，对自己来说将是不能承担的后果。</p>
<p>有人会问：那绿灯的时候就不会发生交通事故了吗？绿灯照样可能发生交通事故啊。</p>
<p>是的，这句话没问题，绿灯同样可能发生交通事故，但是有两点：</p>
<ol>
<li>闯红灯发生交通事故的责任怎么算</li>
<li>闯红灯和不闯红灯发生交通事故的概率比</li>
</ol>
<p>不闯红灯，并不能完全避免交通事故的发生 – 意外并不能完全避免，只能尽量降低其发生的可能性 – 只是将可能性降低而已，因为这事的后果是不能承担的。</p>
<h2 id="投资要不要冒险"><a href="#投资要不要冒险" class="headerlink" title="投资要不要冒险"></a>投资要不要冒险</h2><p>不同的投资标的，风险和回报率是不一样的。每个人能承受的风险能力也是不一样的。</p>
<pre><code>投资一百万，到底算不算冒险？
</code></pre><p>这是一个因人而异的问题，只有自己能回答，因为每个人的资金大小不一样，所以一百万对每个人的意义是不一样的。</p>
<p>大家都想挣钱，甚至挣快钱，这都没有问题，不过要考虑了解两点：</p>
<ol>
<li>高回报率可能伴随这高风险，这个风险是自己能承受的吗？</li>
<li>快速的挣到一大笔钱后，自己有足够的心里能力可以承受吗？ – 这一点是很重要，很重要，很重要，但是很难用语言描述清楚。短时间内得到或者失去一大笔钱对一个人的心里都会造成很大的影响。</li>
</ol>
<p>还有就是，投资基本做不到百分之一百的有把握，所以不要 all in。all in 只要失败一次就全跪了。</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ol>
<li>风险真的是风险吗？是自己真正思考过后的风险，还是听别人说的风险？</li>
<li>知道有风险的存在，可以怎么利用风险吗？</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;风险不仅仅是事件发生的概率&quot;&gt;&lt;a href=&quot;#风险不仅仅是事件发生的概率&quot; class=&quot;headerlink&quot; title=&quot;风险不仅仅是事件发生的概率&quot;&gt;&lt;/a&gt;风险不仅仅是事件发生的概率&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;风险可以定义为 = 事件结果对你的影响 * 事件发生的概率&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;风险在生活中处处存在，可能我们会想冒个险没啥关系，反正发生的概率小，而且在某些时候会有高收益/回报伴随这风险，这个时候就更有诱惑力了，总有人希望通过冒险得到高回报，但这恰恰是不可取的，是非常危险的。&lt;/p&gt;
    
    </summary>
    
      <category term="想清楚" scheme="http://yoursite.com/categories/%E6%83%B3%E6%B8%85%E6%A5%9A/"/>
    
    
      <category term="风险" scheme="http://yoursite.com/tags/%E9%A3%8E%E9%99%A9/"/>
    
      <category term="概率" scheme="http://yoursite.com/tags/%E6%A6%82%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Streaming 程序调用 Producer.close hang 住问题追查复盘</title>
    <link href="http://yoursite.com/2017/06/03/Streaming-%E7%A8%8B%E5%BA%8F%E8%B0%83%E7%94%A8-Producer-close-hang-%E4%BD%8F%E9%97%AE%E9%A2%98%E8%BF%BD%E6%9F%A5%E5%A4%8D%E7%9B%98/"/>
    <id>http://yoursite.com/2017/06/03/Streaming-程序调用-Producer-close-hang-住问题追查复盘/</id>
    <published>2017-06-03T02:44:55.000Z</published>
    <updated>2017-06-03T03:43:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文作为一个问题追查过程的复盘记录，主要希望找出自己在解决问题中可以优化改进的地方。以后遇到问题，能够快速的进行定位，解决。</p>
<a id="more"></a>
<h2 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h2><p>一个 Spark Streaming 作业从 Kafka 消费数据，写往 ES，在 Spark Streaming 作业中会采集一些 metric 指标发往一个特定的 topic A。每次往 A 发送完数据后会调用 <code>producer.close()</code> 方法，看到的现象为：作业启动一段时间之后 hang 住，类似下图</p>
<img src="/2017/06/03/Streaming-程序调用-Producer-close-hang-住问题追查复盘/hang_job.jpg" alt="hang_job.jpg" title="">
<h2 id="排查问题的过程"><a href="#排查问题的过程" class="headerlink" title="排查问题的过程"></a>排查问题的过程</h2><ol>
<li>看到现象后，知道作业 hang 住了，希望能找到为什么 hang 住。找到该作业的 executor 地址（如下图所示）</li>
</ol>
<img src="/2017/06/03/Streaming-程序调用-Producer-close-hang-住问题追查复盘/executor.jpg" alt="executor.jpg" title="">
<p>然后登录到机器上，通过 lsof 查看对应的进程，再通过 jstack dump 出具体的线程栈信息。由于第一次解决线程 hang 住的问题，得到栈信息后，暂时无从下手，然后 google <code>jvm 线程 hang 住</code> 等关键词，检查死锁 – 发现没有。</p>
<p>发现线程有 <code>RUNNABLE</code>，<code>WAITING</code>，<code>TIMED_WAITING</code> 等状态，然后一个个查看这些状态分别代表啥意思。到这就不知道怎么继续了 – 中间在 Spark Streaming 微信群里请教各路大神，有人说遇到链接关不掉的情况 – 多次重复查看 jstack 出来的信息，发现有一个 WAITING 线程在等待锁，具体如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">java.lang.Thread.State: WAITING (on object monitor)</div><div class="line">        at java.lang.Object.wait(Native Method)</div><div class="line">        - waiting on &lt;0x00000000c52cfba0&gt; (a org.apache.kafka.common.utils.KafkaThread)</div><div class="line">        at java.lang.Thread.join(Thread.java:1281)</div><div class="line">        - locked &lt;0x00000000c52cfba0&gt; (a org.apache.kafka.common.utils.KafkaThread)</div><div class="line">        at java.lang.Thread.join(Thread.java:1355)</div><div class="line">        at org.apache.kafka.clients.producer.KafkaProducer.close(KafkaProducer.java:422)</div><div class="line">        at org.elasticsearch.hadoop.rest.KafkaProducer.close(DSLKafkaProducer.java:60)</div></pre></td></tr></table></figure>
<p>然后对照到代码，在 Producer.close() 中有一句代码如下 <code>ioThread.join()</code>，猜测是 ioThread 一直没有执行完毕导致的。</p>
<ol>
<li><p>注释掉 producer.close() 这一句代码之后，重新上线运行 Spark Streaming 作业，发现没有再次出现问题。大致确定问题出在 <code>producer.close()</code> 这里。但是不确定更深层次的问题是啥。期间猜测是由于 producer 发送数据的时候需要有 leader 确认（配置有关），然后将这个配置修改为无需 leader 确认立即返回，但是依然会导致作业 hang 住。然后阅读源码，发现 <code>producer.close()</code> 方法做了两件事：1）将还未发送出去的数据发送出去，2）等待正在发送的数据完成。暂时没有找到造成 <code>ioThread</code> 线程 hang 住的原因。暂时不知道具体 hang 住的地方在哪，至此暂时告一段落。</p>
</li>
<li><p>再次跟进该问题，尝试找出造成线程 hang 住的原因，尝试 jdb attach 到具体的线程。得到如下信息：</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&gt; thread 0x1</div><div class="line">kafka-producer-network-thread | producer-12[1] where</div><div class="line">	  [1] sun.nio.ch.EPollArrayWrapper.epollWait (native method)</div><div class="line">	  [2] sun.nio.ch.EPollArrayWrapper.poll (EPollArrayWrapper.java:269)</div><div class="line">	  [3] sun.nio.ch.EPollSelectorImpl.doSelect (EPollSelectorImpl.java:79)</div><div class="line">	  [4] sun.nio.ch.SelectorImpl.lockAndDoSelect (SelectorImpl.java:87)</div><div class="line">	  [5] sun.nio.ch.SelectorImpl.select (SelectorImpl.java:98)</div><div class="line">	  [6] org.apache.kafka.common.network.Selector.select (Selector.java:328)</div><div class="line">	  [7] org.apache.kafka.common.network.Selector.poll (Selector.java:218)</div><div class="line">	  [8] org.apache.kafka.clients.NetworkClient.poll (NetworkClient.java:192)</div><div class="line">	  [9] org.apache.kafka.clients.producer.internals.Sender.run (Sender.java:191)</div><div class="line">	  [10] org.apache.kafka.clients.producer.internals.Sender.run (Sender.java:135)</div><div class="line">	  [11] java.lang.Thread.run (Thread.java:745)</div></pre></td></tr></table></figure>
<p>到这里暂时不知道该怎么继续往下查了，知道在这里 hang 住了，但是暂时不知道怎么继续往下查，看着屏幕发呆，然后想着这个问题或许别人也遇到过，就从上面的 栈信息 中抽取一部分关键词进行 google，得到信息在 kafka 0.8.2.1 中 producer.close() 在某些情况下会 hang 住，详情参考 <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-19+-+Add+a+request+timeout+to+NetworkClient" target="_blank" rel="external">KIP-19</a>，在 kafka 0.9.0.0 中提供一个带超时的 close 方法进行修复。</p>
<h2 id="问题复盘"><a href="#问题复盘" class="headerlink" title="问题复盘"></a>问题复盘</h2><ol>
<li><p>在知道作业 hang 住的情况，又不了解相应调试的情况下，能否快速了解定位问题的方法，能否询问其他人快速的定位问题，或者如何通过搜索引擎快速的获取自己需要的知识。这里自己有个小私心 – 觉得这是测试的作业，想保留现场，通过自己的努力完全把问题解决，好提升自己的能力。另外自己如何在平时积累一些查问题的经验（这次发现官方文档真是个好东西）</p>
</li>
<li><p>通过微信群询问是一个方法，但是提问需要有技巧，要能够提炼出自己的问题，以及自己进行了哪些尝试，有什么思考，而不是做伸手党。</p>
</li>
<li>为什么到最后才想着 Google 相关信息，而不是在知道 producer.close() 导致作业 hang 住的时候就 Google 相关信息。</li>
<li>对 Java 排查问题的工具非常不熟练，在平时需要自己模拟各种 case 进行练手。jstack, jvisualvm, jdb 等都是第一次使用，这些工具需要在平时进行熟练。</li>
<li>对常见的库或通用的写法要有一定的了解，比如看到 <code>org.apache.kafka.common.network.Selector.poll</code> 是否能想到没有超时而导致一直 hang 住，这些平时需要积累（思考这个怎么积累？）</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文作为一个问题追查过程的复盘记录，主要希望找出自己在解决问题中可以优化改进的地方。以后遇到问题，能够快速的进行定位，解决。&lt;/p&gt;
    
    </summary>
    
      <category term="实时计算" scheme="http://yoursite.com/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
      <category term="复盘" scheme="http://yoursite.com/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/%E5%A4%8D%E7%9B%98/"/>
    
      <category term="problem_solve" scheme="http://yoursite.com/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/%E5%A4%8D%E7%9B%98/problem-solve/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="spark_streaming" scheme="http://yoursite.com/tags/spark-streaming/"/>
    
      <category term="kafka" scheme="http://yoursite.com/tags/kafka/"/>
    
      <category term="thinking" scheme="http://yoursite.com/tags/thinking/"/>
    
      <category term="problem_solve" scheme="http://yoursite.com/tags/problem-solve/"/>
    
  </entry>
  
  <entry>
    <title>如何在不重启 Spark Streaming 作业的情况下，增加消费的 topic</title>
    <link href="http://yoursite.com/2017/06/01/%E5%A6%82%E4%BD%95%E5%9C%A8%E4%B8%8D%E9%87%8D%E5%90%AF-Spark-Streaming-%E4%BD%9C%E4%B8%9A%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%EF%BC%8C%E5%A2%9E%E5%8A%A0%E6%B6%88%E8%B4%B9%E7%9A%84-topic/"/>
    <id>http://yoursite.com/2017/06/01/如何在不重启-Spark-Streaming-作业的情况下，增加消费的-topic/</id>
    <published>2017-06-01T15:22:51.000Z</published>
    <updated>2017-06-03T03:37:01.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>本文所有和 kafka 相关操作都基于 Direct 消费模式</p>
</blockquote>
<p>在 Spark Streaming 作业中，每个作业会消费一个或多个 topic，但是这些 topic 需要在作业启动之前确定好，在作业运行中不能进行调整，之前<a href="https://klion26.github.io/2016/11/01/spark-streaming-topic-partition-change-auto-adaptive/" target="_blank" rel="external">修改了源码</a>做到了自适应 topic partition 扩容的情况，但是无法动态调整消费的 topic。现在需要在不重启作业的情况下，动态调整消费的 topic。</p>
<a id="more"></a>
<h2 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h2><p>回顾之前自适应 partition 调整的方案，落到源码层面最终以 partition 为最小消费单元，而不是 topic。因此动态的调整消费的 topic 在理论上就是可行的 – 假设作业已经消费的 topic 为 A，在自适应 partition 扩容的时候，我们是增加了 A 的某些 partition，那么我们同样可以增加 B topic 的 partition，其中 B topic 是作业之前没有消费的。</p>
<p>动态调整 partition 的方案中，只需要将现在消费的 parition 数不断的对齐现在 kafka 上相应 topic 的 partition 数目即可。动态调整作业消费的 topic 则需要有一个地方存储作业消费的 topic 数目，然后将这个信息周期性的同步给作业即可 – 可以理解前者使用 kafka 作为存储介质，保存了 topic 的 partition 数目。</p>
<p>本方案中，选择 zookeeper 作业作为存储 topic 的介质。希望动态调整 topic 的时候，修改 zookeeper 中对应路径下的节点即可。然后作业定时的访问 zookeeper 的特定路径同步需要消费的 topic 数目即可。示意图如下</p>
<p><img src="https://c1.staticflickr.com/5/4275/34202432474_eb409ae6a5.jpg" alt=""></p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>方案确定了，直接修改下上次自适应 partition 扩容的代码即可 – 本方案只实现了增加 topic 的功能，当前消费的 topic 不会被删除，如果需要的话可以自行修改源码满足这一点。</p>
<p>在 <code>DirectKafkaInputStream</code> 的 <code>compute</code> 函数开始处添加如下逻辑</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">val topic = currentOffsets.head._1.topic</div><div class="line">var addedTopic : Set[String] = Set()</div><div class="line">val topics = getTopicsForJob()</div><div class="line">for (i &lt;- topics) &#123;</div><div class="line">      if (!i.equals(topic)) &#123;</div><div class="line">            addedTopic = addedTopic + i</div><div class="line">       &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">if (addedTopic.nonEmpty) &#123;</div><div class="line">     val topicLeaders = MTKafkaUtils.getPartitions(kafkaParams, addedTopic)</div><div class="line">     val largestOffset = MTKafkaUtils.getLeaderOffsets(kafkaParams, topicLeaders, OffsetRequest.LatestTime)</div><div class="line"></div><div class="line">     currentOffsets = currentOffsets ++ largestOffset</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>然后增加一个获取所有 topic 的函数，下面 Constants 包中使用了一些常量，自行替换即可</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">private def getTopicsForJob() : List[String] = &#123;</div><div class="line">        val jobName = SparkEnv.get.conf.get(Constants.JOB_PREFIXED_NAME_KEY)  //这个是提交 job 时添加的一个参数，用于区分每个作业，会当作 zk 中路径的一级</div><div class="line">        val zkHostPort: String =  &quot;xxxxxxxxx&quot;</div><div class="line">        val zkClient = new ZkClient(zkHostPort, Constants.DEFAULT_SESSION_TIMEOUT, Constants.DEFAULT_CONNECTION_TIMEOUT, new ZkOffsetSerializer) //ZkOffsetSerializer 自己实现了一个简单的序列化，反序列化类，就用了 String.getBytes 和 new String()</div><div class="line"></div><div class="line">        if (!zkClient.exists(s&quot;$&#123;Constants.ROOT_PATH_OF_MULTI_TOPIC_PER_JOB&#125;/$&#123;jobName&#125;&quot;)) &#123;</div><div class="line">									            ZkUtils.updatePersistentPath(zkClient, s&quot;$&#123;Constants.ROOT_PATH_OF_MULTI_TOPIC_PER_JOB&#125;/$&#123;jobName&#125;&quot;, s&quot;$&#123;jobName&#125;&quot;);</div><div class="line">														        &#125;</div><div class="line"></div><div class="line">        zkClient.getChildren(s&quot;$&#123;Constants.ROOT_PATH_OF_MULTI_TOPIC_PER_JOB&#125;/$&#123;jobName&#125;&quot;).asScala.toList</div><div class="line">&#125;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文所有和 kafka 相关操作都基于 Direct 消费模式&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在 Spark Streaming 作业中，每个作业会消费一个或多个 topic，但是这些 topic 需要在作业启动之前确定好，在作业运行中不能进行调整，之前&lt;a href=&quot;https://klion26.github.io/2016/11/01/spark-streaming-topic-partition-change-auto-adaptive/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;修改了源码&lt;/a&gt;做到了自适应 topic partition 扩容的情况，但是无法动态调整消费的 topic。现在需要在不重启作业的情况下，动态调整消费的 topic。&lt;/p&gt;
    
    </summary>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="spark_streaming" scheme="http://yoursite.com/tags/spark-streaming/"/>
    
      <category term="kafka" scheme="http://yoursite.com/tags/kafka/"/>
    
      <category term="zookeeper" scheme="http://yoursite.com/tags/zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>从源码级别分析 metric-core 的抽样算法</title>
    <link href="http://yoursite.com/2017/05/29/%E4%BB%8E%E6%BA%90%E7%A0%81%E7%BA%A7%E5%88%AB%E5%88%86%E6%9E%90-metric-core-%E7%9A%84%E6%8A%BD%E6%A0%B7%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2017/05/29/从源码级别分析-metric-core-的抽样算法/</id>
    <published>2017-05-29T09:45:22.000Z</published>
    <updated>2017-06-03T03:37:50.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://metrics.dropwizard.io" target="_blank" rel="external">metric-core</a> 是一个 java metric 库，用于统计 JVM 层面以及 服务级别 的各种 metric 信息。其中 metric-core 是其核心模块，代码量不多，总共 44 个文件，5700 行左右代码（包括注释）。算是一个很小的开源项目了。由于 metric 在所有项目中都非常重要，因此选择通读该项目，本文分析 metrci-core 中的抽样算法。</p>
<a id="more"></a>
<h2 id="metric-core-中的抽样算法"><a href="#metric-core-中的抽样算法" class="headerlink" title="metric-core 中的抽样算法"></a>metric-core 中的抽样算法</h2><p>在 metric-core 中总共有四种抽样算法，分别是 <code>ExponentiallyDecayingReservoir</code>, <code>SlidingTimeWindowReservoir</code>, <code>SlidingWindowReservoir</code>, <code>UniformReservoir</code>，其中后面三个抽样算法比较常规，也通常能见到，第一个则出于一篇论文<code>Forward Decay: A Practical Time Decay Model for Streaming Systems</code>，本文会通过源码分析自己对于这种抽样算法的理解。本文暂时只分析后面三种抽样算法，对于第一种，我会单独用一篇文章进行分析。</p>
<h3 id="UniformReservoir-算法"><a href="#UniformReservoir-算法" class="headerlink" title="UniformReservoir 算法"></a>UniformReservoir 算法</h3><p>该算法来自于论文<code>Random Sampling with a Reservoir</code>，讲述了一种随机抽样的方法，主要思想是使用一个固定的“蓄水池”装满需要数量的样本，如果当前“蓄水池”未满，将接下来的样本直接放入“蓄水池”，如果“蓄水池”已满，则随机从”蓄水池“中挑选一个样本进行替换（也可能不进行替换），这样在理论上能够保证所有的样本以同样的概率被选中。代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">public void update(long value) &#123;</div><div class="line">	final long c = count.incrementAndGet();//获得当前”蓄水池“的大小</div><div class="line">	if (c &lt;= values.length()) &#123; //如果”蓄水池“未满，直接将当前样本放入</div><div class="line">		values.set((int) c - 1, value);</div><div class="line">	&#125; else &#123;</div><div class="line">		final long r = nextLong(c);//随机挑选一个数据（这个随机挑选的数可能在&quot;蓄水池”中，也可能不在“蓄水池”中</div><div class="line">		if (r &lt; values.length()) &#123;//如果随机挑选的样本，在”蓄水池“中，则进行替换</div><div class="line">			values.set((int) r, value);</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>为了能够更好的理解，先使用样例如下。假设现在总共来了 10 个数 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]，而“蓄水池“大小为 3. 那么”蓄水池”的 <strong>一种可能</strong> 变化如下（说是一种可能的变化，因为这里面牵涉到概率）</p>
<ul>
<li>[1]</li>
<li>[1, 2]</li>
<li>[1, 2, 3]</li>
<li>[1, 2, 4]  # 当 4 来的时候，发现“蓄水池”已满，然后从中筛选一个进行替换掉，假设我们替换掉 3</li>
<li>[1, 5, 4] # 当 5 来的时候，发现“蓄水池”已满，然后从中筛选一个进行替换掉，假设这次我们替换掉 2</li>
<li>[1, 5, 4] # 当 6 来的时候，发现“蓄水池”已满，我们打算从之前的数字中筛选一个进行替换，这个时候假设我们得到的下标是 3 或者 4，发现下标为 3 和 4 的数字不在“蓄水池”中（“蓄水池”的最大下标为 2 – 从 0 开始），因此不进行替换，所以本次“蓄水池”不变</li>
<li>[7, 5, 4] # 当 7 来的时候，发现“蓄水池”已满，随机一个下标，我们得到 0,那么将 7 放置到下标为 0 的位置</li>
<li>[8, 5, 4] # 同上</li>
<li>[8, 5, 9] # 同上</li>
<li>[10, 5, 9] # 同上<h3 id="SlidingWindowReservoir-抽样算法"><a href="#SlidingWindowReservoir-抽样算法" class="headerlink" title="SlidingWindowReservoir 抽样算法"></a>SlidingWindowReservoir 抽样算法</h3><code>SlidingWindowReservoir</code> 抽样算法则以最近的 N 个样本作为整个数据集的子集，这样简单直接，对于数据波动不大，或者窗口大小 N 足够大的情况下，该算法会有较好的效果。代码如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">public synchronized void update(long value) &#123;//加锁保证线程安全</div><div class="line">	        //每次替换掉最旧的数据，保证”蓄水池“中的数据是最近的 N 个样本</div><div class="line">	        measurements[(int) (count++ % measurements.length)] = value;</div><div class="line">			    &#125;</div></pre></td></tr></table></figure>
<h3 id="SlidingTimeWindowReservoir-抽样算法"><a href="#SlidingTimeWindowReservoir-抽样算法" class="headerlink" title="SlidingTimeWindowReservoir 抽样算法"></a>SlidingTimeWindowReservoir 抽样算法</h3><p>该算法是上面移动窗口算法的变种，保留的是最近 N 时间单位（支持 TimeUnit 的所有时间单位）内的数据，而不是最近的 N 个数据。代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">public void update(long value) &#123;</div><div class="line">//每 TRIM_THRESHOLD 次操作之后会进行一次 trim() 操作</div><div class="line">	if (count.incrementAndGet() % TRIM_THRESHOLD == 0) &#123;</div><div class="line">		trim();</div><div class="line">	&#125;</div><div class="line">			        //直接将该值加入到 ”蓄水池“ 中</div><div class="line">	measurements.put(getTick(), value);</div><div class="line">	&#125;</div><div class="line">//获得当前的时间</div><div class="line">private long getTick() &#123;</div><div class="line">	for (; ; ) &#123;</div><div class="line">		final long oldTick = lastTick.get();</div><div class="line">		final long tick = clock.getTick() * COLLISION_BUFFER;</div><div class="line">		// ensure the tick is strictly incrementing even if there are duplicate ticks</div><div class="line">		final long newTick = tick - oldTick &gt; 0 ? tick : oldTick + 1;</div><div class="line">		if (lastTick.compareAndSet(oldTick, newTick)) &#123;</div><div class="line">			return newTick;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">private void trim() &#123;</div><div class="line">	measurements.headMap(getTick() - window).clear();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这三种算法中，第二种和第三种是大家都很容易想到的，实现起来也很简单，第一种进行简单推导也不难，也算是一种现成的算法“蓄水池抽样”。</p>
<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>如果某个系统每天会有 N 个人请求（N 不确定），需要从这些人中等概率的抽出 K 个中奖者，那么应该怎么做呢？是否可以使用上面抽样算法中的一种呢？</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://metrics.dropwizard.io&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;metric-core&lt;/a&gt; 是一个 java metric 库，用于统计 JVM 层面以及 服务级别 的各种 metric 信息。其中 metric-core 是其核心模块，代码量不多，总共 44 个文件，5700 行左右代码（包括注释）。算是一个很小的开源项目了。由于 metric 在所有项目中都非常重要，因此选择通读该项目，本文分析 metrci-core 中的抽样算法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="code" scheme="http://yoursite.com/tags/code/"/>
    
      <category term="java" scheme="http://yoursite.com/tags/java/"/>
    
      <category term="metric" scheme="http://yoursite.com/tags/metric/"/>
    
      <category term="reservior" scheme="http://yoursite.com/tags/reservior/"/>
    
      <category term="metric-core" scheme="http://yoursite.com/tags/metric-core/"/>
    
  </entry>
  
  <entry>
    <title>Streaming 中 Receiver 相关源码分析</title>
    <link href="http://yoursite.com/2017/05/19/Streaming-%E4%B8%AD-Receiver-%E7%9B%B8%E5%85%B3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2017/05/19/Streaming-中-Receiver-相关源码分析/</id>
    <published>2017-05-19T03:52:54.000Z</published>
    <updated>2017-06-03T03:58:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文基于 spark 1.6.2<br>本次的源码全来自 <code>org.apache.spark.streaming.receiver</code> 这个 package 下，包括 <code>BlockGenerator.scala</code>, <code>RateLimiter</code>, <code>ReceiverdBlock.scala</code>, <code>ReceivedBlockHandler.scala</code>, <code>Receiver.scala</code>, <code>ReceiverSupervisor.scala</code>, <code>ReceiverSupervisorImpl.scala</code></p>
<a id="more"></a>
<p>其中 <code>Receiver</code> 是所有接受数据的父类，主要定义一些接口，用户只需要继承 <code>Receiver</code>，然后实现其中的接口就行。</p>
<p><code>ReceiverSupervisor</code> 则是负责和协调 <code>Receiver</code> 和其他组件，定义了一些接口，然后 <code>ReceiverSupervisorImpl</code> 是 <code>ReceiverSupervisor</code> 的具体实现，主要实现了协调其他组件（包括 <code>ReceivedBlockHandler</code> 和 <code>BlockGenerator</code> <code>BlockGeneratorListener</code> 以及远端 RPC 服务等）和 <code>Receiver</code> 的逻辑。</p>
<p><code>BlockGenerator</code> 则主要负责接受 <code>Receiver</code> 接受到的数据，然后存储成 block（具体的有 <code>ReceivedBlockHandler</code> 负责），会起两个线程来做相应的事情，一个是定时的将接受到的数据生成 block，一个是将 block push 给 <code>ReceivedBlockHandler</code> 进行存储，具体的 block 管理则通过 Spark core 的 block 模块来进行管理。</p>
<p><code>ReceivedBlockHandler</code> 则负责将 block 保存到具体的地方，包括指定的 storageLevel 以及 write ahead log。</p>
<p>整个 Receiver 端的代码结构简化版如下所示，其中 Receiver 包含一个 ReceiverSupervisor 对象，ReceiverSupervisor 负责和 BlockGenerator 以及 ReceivedBlockHandler 交互。用户继承 Receiver，实现具体的接受数据的逻辑即可，对于数据接受之后，怎么处理，都通过 ReceiverSupervisor 中转给 BlockGenerator 来处理（BlockGenerator 会有一个定时器用于生成 block，还有一个单独的线程用于将生成的 block push 给 BlockManager）</p>
<img src="/2017/05/19/Streaming-中-Receiver-相关源码分析/Receiver_sample.png" alt="Receiver_sample.png" title="">
<p>整个 Receiver 端的详细代码结构图如下所示<br><img src="/2017/05/19/Streaming-中-Receiver-相关源码分析/Receiver.png" alt="Receiver.png" title=""></p>
<h1 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h1><ol>
<li>为什么需要将 <code>Receiver</code> 和 <code>ReceiverSupervisor</code> 进行分开呢，下面提供这两个类的函数对比图（其中第一列表示 <code>Receiver</code> 的所有函数；后面几列表示 <code>ReceiverSupervisor</code> 的所有函数，同一行的函数表示有相<img src="/2017/05/19/Streaming-中-Receiver-相关源码分析/Receiver_ReceiverSupervisor.png" alt="Receiver_ReceiverSupervisor.png" title="">
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文基于 spark 1.6.2&lt;br&gt;本次的源码全来自 &lt;code&gt;org.apache.spark.streaming.receiver&lt;/code&gt; 这个 package 下，包括 &lt;code&gt;BlockGenerator.scala&lt;/code&gt;, &lt;code&gt;RateLimiter&lt;/code&gt;, &lt;code&gt;ReceiverdBlock.scala&lt;/code&gt;, &lt;code&gt;ReceivedBlockHandler.scala&lt;/code&gt;, &lt;code&gt;Receiver.scala&lt;/code&gt;, &lt;code&gt;ReceiverSupervisor.scala&lt;/code&gt;, &lt;code&gt;ReceiverSupervisorImpl.scala&lt;/code&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="分布式系统" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="实时计算" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
      <category term="源码阅读" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="spark_streaming" scheme="http://yoursite.com/tags/spark-streaming/"/>
    
      <category term="receiver" scheme="http://yoursite.com/tags/receiver/"/>
    
      <category term="source_code" scheme="http://yoursite.com/tags/source-code/"/>
    
  </entry>
  
  <entry>
    <title>Python 代码实践小结</title>
    <link href="http://yoursite.com/2017/05/10/Python-%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5%E5%B0%8F%E7%BB%93/"/>
    <id>http://yoursite.com/2017/05/10/Python-代码实践小结/</id>
    <published>2017-05-10T03:59:21.000Z</published>
    <updated>2017-06-03T04:07:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近写了较多的 Python 脚本，将最近自己写的脚本进行一个总结，其中有些是 Python 独有的，有些是所有程序设计中共有的：</p>
<ol>
<li>考虑使用 Logger（logger 怎么配置，需要输出哪些信息 – 可以反向考虑，自己看到这个 logger 的时候想了解什么信息）</li>
<li>传递的数据结构如何考虑（是否对调用方有先验知识的要求，比如返回一个 Tuple，则需要用户了解 tuple 中元素的顺序，这样情况是否应该进行封装；），数据结构定义清楚了，很多东西也就清楚了。</li>
<li>如何操作数据库（可以学习 sqlalchemy，包括 core 和 orm 两种 api）</li>
<li>异常如何处理（异常应该分开捕获 – 可以清楚的知道什么情况下导致的，异常之后应该打印日志说明出现什么问题，如果情况恶劣需要进行异常再次抛出或者报警）</li>
<li>所有获取资源的地方都应该做 check（a. 没有获取到会怎么办；b.获取到异常的怎么办）</li>
<li>所有操作资源的地方都应该检查是否操作成功</li>
<li>每个函数都应该简短，如果函数过长应该进行拆分（有个建议值，函数包含的行数应该在 20-30 行之间，具体按照这个规范做过一次之后就会发现这样真好）</li>
<li>使用 class 之后，考虑重构 <code>__str__</code> 函数，用户打印输出，如果对象放到 collection 中之后，需要实现 <code>__repr__</code> 函数，用于打印整个 collection 的时候，直观显示（如果不实现 <code>__str__</code>，会调用 <code>__repr__</code>)</li>
<li>如果有些资源会发生变化，可以单独抽取出来，做成函数，这样后续调用就可以不用改变了</li>
</ol>
<p>上述总结肯定有片面的地方，也有不全的地方，欢迎指出</p>
<a id="more"></a>
<p>附上一份 Python2.7 代码（将一些私有的东西进行了修改）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div></pre></td><td class="code"><pre><div class="line"># -*- coding:utf-8 -*-</div><div class="line"></div><div class="line">from sqlalchemy import create_engine</div><div class="line">import logging</div><div class="line">from logging.config import fileConfig</div><div class="line">import requests</div><div class="line">import Clinet # 私有的模块</div><div class="line"></div><div class="line">fileConfig(&quot;logging_config.ini&quot;)</div><div class="line">logger = logging.getLogger(&quot;killduplicatedjob&quot;)</div><div class="line"></div><div class="line">#配置可以单独放到一个模块中</div><div class="line">DB_USER = &quot;xxxxxxx&quot;</div><div class="line">DB_PASSWORD = &quot;xxxxxxxx&quot;</div><div class="line">DB_PORT = 111111</div><div class="line">DB_HOST_PORT = &quot;xxxxxxxxxx&quot;</div><div class="line">DB_DATA_BASE = &quot;xxxxxxxxxxx&quot;</div><div class="line"></div><div class="line">REST_API_URL = &quot;http://sample.com&quot;</div><div class="line"></div><div class="line">engine = create_engine(&quot;mysql://%s:%s@%s:%s/%s&quot; % (DB_USER, DB_PASSWORD, DB_HOST_PORT, DB_PORT, DB_DATA_BASE))</div><div class="line"></div><div class="line"></div><div class="line"># 这个 class 是为了在函数间传递时，不需要使用方了解属性的具体顺序而写的，也可以放到一个单独的模块中</div><div class="line">class DuplicatedJobs(object):</div><div class="line">    def __init__(self, app_id, app_name, user):</div><div class="line">        self.app_id = app_id</div><div class="line">        self.app_name = app_name</div><div class="line">        self.user = user</div><div class="line"></div><div class="line">     def __repr__(self):</div><div class="line">        return &apos;[appid:%s, app_name:%s, user:%s]&apos; % (self.app_id, self.app_name, self.user)</div><div class="line"></div><div class="line"></div><div class="line">	def find_duplicated_jobs():</div><div class="line">		logger.info(&quot;starting find duplicated jobs&quot;)</div><div class="line">		(running_apps, app_name_to_user) = get_all_running_jobs()</div><div class="line">		all_apps_on_yarn = get_apps_from_yarn_with_queue(get_resource_queue())</div><div class="line"></div><div class="line">		duplicated_jobs = []</div><div class="line">		for app in all_apps_on_yarn:</div><div class="line">			(app_id, app_name) = app</div><div class="line"></div><div class="line">			if app_id not in running_apps:</div><div class="line">				if not app_name.startswith(&quot;test&quot;):</div><div class="line">					logger.info(&quot;find a duplicated job, prefixed_name[%s] with appid[%s]&quot; % (app_name, app_id))</div><div class="line">					user = app_name_to_user[app_name]</div><div class="line">					duplicated_jobs.append(DuplicatedJobs(app_id, app_name, user))</div><div class="line">	            else:</div><div class="line">				    logger.info(&quot;Job[%s] is a test job, would not kill it&quot; % app_name)</div><div class="line"></div><div class="line">	logger.info(&quot;Find duplicated jobs [%s]&quot; % duplicated_jobs)</div><div class="line"></div><div class="line">    return duplicated_jobs</div><div class="line"></div><div class="line"></div><div class="line">def get_apps_from_yarn_with_queue(queue):</div><div class="line">	param = &#123;&quot;queue&quot;: queue&#125;</div><div class="line">	r = requests.get(REST_API_URL, params=param)</div><div class="line">	apps_on_yarn = []</div><div class="line">	try:</div><div class="line">		jobs = r.json().get(&quot;apps&quot;)</div><div class="line">		app_list = jobs.get(&quot;app&quot;, [])</div><div class="line">		for app in app_list:</div><div class="line">			app_id = app.get(&quot;id&quot;)</div><div class="line">			name = app.get(&quot;name&quot;)</div><div class="line">			apps_on_yarn.append((app_id, name))</div><div class="line"></div><div class="line">	 except Exception as e: #Exception 最好进行单独的分开，针对每一种 Exception 进行不同的处理</div><div class="line">		logger.error(&quot;Get apps from Yarn Error, message[%s]&quot; % e.message)</div><div class="line"></div><div class="line">	logger.info(&quot;Fetch all apps from Yarn [%s]&quot; % apps_on_yarn)</div><div class="line"></div><div class="line">	return apps_on_yarn</div><div class="line"></div><div class="line"></div><div class="line">def get_all_running_jobs():</div><div class="line">	job_infos = get_result_from_mysql(&quot;select * from xxxx where xx=yy&quot;)</div><div class="line"></div><div class="line">	app_ids = []</div><div class="line">	app_name_to_user = &#123;&#125;</div><div class="line">	for (topology_id, topology_name) in job_infos:</div><div class="line">		status_set = get_result_from_mysql(&quot;select * from xxxx where xx=yy&quot;)</div><div class="line">		application_id = status_set[0][0]</div><div class="line">		if &quot;&quot; != application_id:</div><div class="line">			configed_resource_queue = get_result_from_mysql(&quot;select * from xxxx where xx=yy&quot;)</div><div class="line">			app_ids.append(application_id)</div><div class="line">	        app_name_to_user[topology_name] = configed_resource_queue[0][0].split(&quot;.&quot;)[1]</div><div class="line"></div><div class="line">	logger.info(&quot;All running jobs appids[%s] topology_name2user[%s]&quot; % (app_ids, app_name_to_user))</div><div class="line">	return app_ids, app_name_to_user</div><div class="line"></div><div class="line"></div><div class="line">def kill_duplicated_jobs(duplicated_jobs):</div><div class="line">	for job in duplicated_jobs:</div><div class="line">	app_id = job.app_id</div><div class="line">	app_name = job.app_name</div><div class="line">	user = job.user</div><div class="line">	logger.info(&quot;try to kill job[%s] with appid[%s] for user[%s]&quot; % (app_name, app_id, user))</div><div class="line">	try:</div><div class="line">		Client.kill_job(app_id, user)</div><div class="line">		logger.info(&quot;Job[%s] with appid[%s] for user[%s] has been killed&quot; % (app_name, app_id, user))</div><div class="line">	except Exception as e:</div><div class="line">		logger.error(&quot;Can&apos;t kill job[%s] with appid[%s] for user[%s]&quot; % (app_name, app_id, user))</div><div class="line"></div><div class="line"></div><div class="line">def get_result_from_mysql(sql):</div><div class="line">	a = engine.execute(sql)</div><div class="line">	return a.fetchall() </div><div class="line"></div><div class="line"></div><div class="line"># 因为下面的资源可能发生变化，而且可能包含一些具体的逻辑，因此单独抽取出来，独立成一个函数</div><div class="line">def get_resource_queue():</div><div class="line">	return &quot;xxxxxxxxxxxxx&quot;</div><div class="line"></div><div class="line">if __name__ == &quot;__main__&quot;:</div><div class="line">	kill_duplicated_jobs(find_duplicated_jobs())</div></pre></td></tr></table></figure>
<p>其中 logger 配置文件如下（对于 Python 的 logger，官方文档写的非常好，建议读一次，并且实践一次）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">[loggers]</div><div class="line">keys=root, simpleLogger</div><div class="line"></div><div class="line">[handlers]</div><div class="line">keys=consoleHandler, logger_handler</div><div class="line"></div><div class="line">[formatters]</div><div class="line">keys=formatter</div><div class="line"></div><div class="line">[logger_root]</div><div class="line">level=WARN</div><div class="line">handlers=consoleHandler</div><div class="line"></div><div class="line">[logger_simpleLogger]</div><div class="line">level=INFO</div><div class="line">handlers=logger_handler</div><div class="line">propagate=0</div><div class="line">qualname=killduplicatedjob</div><div class="line"></div><div class="line">[handler_consoleHandler]</div><div class="line">class=StreamHandler</div><div class="line">level=WARN</div><div class="line">formatter=formatter</div><div class="line">args=(sys.stdout,)</div><div class="line"></div><div class="line">[handler_logger_handler]</div><div class="line">class=logging.handlers.RotatingFileHandler</div><div class="line">level=INFO</div><div class="line">formatter=formatter</div><div class="line">args=(&quot;kill_duplicated_streaming.log&quot;, &quot;a&quot;, 52428800, 3,)</div><div class="line"></div><div class="line">[formatter_formatter]</div><div class="line">format=%(asctime)s %(name)-12s %(levelname)-5s %(message)s</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近写了较多的 Python 脚本，将最近自己写的脚本进行一个总结，其中有些是 Python 独有的，有些是所有程序设计中共有的：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;考虑使用 Logger（logger 怎么配置，需要输出哪些信息 – 可以反向考虑，自己看到这个 logger 的时候想了解什么信息）&lt;/li&gt;
&lt;li&gt;传递的数据结构如何考虑（是否对调用方有先验知识的要求，比如返回一个 Tuple，则需要用户了解 tuple 中元素的顺序，这样情况是否应该进行封装；），数据结构定义清楚了，很多东西也就清楚了。&lt;/li&gt;
&lt;li&gt;如何操作数据库（可以学习 sqlalchemy，包括 core 和 orm 两种 api）&lt;/li&gt;
&lt;li&gt;异常如何处理（异常应该分开捕获 – 可以清楚的知道什么情况下导致的，异常之后应该打印日志说明出现什么问题，如果情况恶劣需要进行异常再次抛出或者报警）&lt;/li&gt;
&lt;li&gt;所有获取资源的地方都应该做 check（a. 没有获取到会怎么办；b.获取到异常的怎么办）&lt;/li&gt;
&lt;li&gt;所有操作资源的地方都应该检查是否操作成功&lt;/li&gt;
&lt;li&gt;每个函数都应该简短，如果函数过长应该进行拆分（有个建议值，函数包含的行数应该在 20-30 行之间，具体按照这个规范做过一次之后就会发现这样真好）&lt;/li&gt;
&lt;li&gt;使用 class 之后，考虑重构 &lt;code&gt;__str__&lt;/code&gt; 函数，用户打印输出，如果对象放到 collection 中之后，需要实现 &lt;code&gt;__repr__&lt;/code&gt; 函数，用于打印整个 collection 的时候，直观显示（如果不实现 &lt;code&gt;__str__&lt;/code&gt;，会调用 &lt;code&gt;__repr__&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;如果有些资源会发生变化，可以单独抽取出来，做成函数，这样后续调用就可以不用改变了&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上述总结肯定有片面的地方，也有不全的地方，欢迎指出&lt;/p&gt;
    
    </summary>
    
      <category term="语言学习" scheme="http://yoursite.com/categories/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="计算机基础" scheme="http://yoursite.com/categories/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="code" scheme="http://yoursite.com/tags/code/"/>
    
      <category term="logger" scheme="http://yoursite.com/tags/logger/"/>
    
  </entry>
  
  <entry>
    <title>从现在开始写作</title>
    <link href="http://yoursite.com/2017/04/15/%E4%BB%8E%E7%8E%B0%E5%9C%A8%E5%BC%80%E5%A7%8B%E5%86%99%E4%BD%9C/"/>
    <id>http://yoursite.com/2017/04/15/从现在开始写作/</id>
    <published>2017-04-15T04:07:32.000Z</published>
    <updated>2017-06-03T04:11:28.000Z</updated>
    
    <content type="html"><![CDATA[<p>这里的写作不特指写长篇大论的文章</p>
<h2 id="为什么要写作"><a href="#为什么要写作" class="headerlink" title="为什么要写作"></a>为什么要写作</h2><blockquote>
<p>写作是了帮助自己更好的思考，提高自己的效率。</p>
</blockquote>
<p>首先，每个人同一时刻能记住的东西有限，而做一件事情可能需要考虑的条件往往会比较多，将所有的情况写到一张纸上，就能在需要的时候看到自己需要的条件。相信每个人都有这样的体验：做数学题的时候，将所有的已知条件，或者自己推导出来的结论写到草稿纸上，往往能更快的解出这道题目。这也是写作帮助自己思考的一个直接。</p>
<p>其次，我遇到过这种情况，自己想着是这么回事，可是写下来的时候，发现无从下笔，根本写不出来，写出来之后，也很难向别人清楚、准确的表达自己的意思。这归根结底还是没有思考清楚导致的，而写作可以帮助我整理自己的思路。</p>
<p>然后，当过客服的人肯定有一个体会，客服非常耗时间，而这些客服的问题大部分是差不多的，一对一的客服是非常低效的，就算在群组里面进行客服，其实很多后来遇到同样问题的人也是不看群历史记录（或者不知道以前有人遇到过类似的问题）的。这种情况下，虽然文档不是最优解，但能够省出自己足够多的时间，实际上这是一件杠杆率非常高（产出投入比高）的事件。</p>
<a id="more"></a>
<h2 id="写什么"><a href="#写什么" class="headerlink" title="写什么"></a>写什么</h2><p>既然明确了写作的目的，那么写什么也就清楚了。</p>
<ol>
<li><p>写对自己有用的东西</p>
<ul>
<li>读书笔记。看完一本书之后，可以写一个读书笔记，或者用思维导图整理书中内容，整理的过程是帮助自己加深书中思想印象的过程，可以帮助自己将整本书串起来，有一个全局的了解，也知道自己哪些地方暂时不太清楚。</li>
<li>自己解决问题的总结。总结可以帮助自己复盘 – 有复盘才有改进的方向；备忘。我曾经在公司内网上记录了自己解决某个问题的过程与解决方案，后来有不少遇到同样问题的同事联系我，每个人的问题相似，但每个人的问题又都不一样，解决别人的问题，实际上也能扩大自己的知识储备。</li>
<li>写教程 – 教是最好的学。在自己写教程的过程中，实际上也是自己再教自己一次相应的知识，对掌握的知识了解更深，对掌握不够的知识，去学习了解。</li>
<li>自己对某件事的思考过程。将自己的思考过程写下来，能够更好的了解自己是怎么考虑问题的，在哪些地方可以进行改善，哪些地方是自己没有考虑到的，能够帮助自己更好的提高思考质量。</li>
</ul>
</li>
<li><p>写对别人有用的东西</p>
</li>
</ol>
<blockquote>
<p>解决问题的思路和方案，以及教程对别人也是有用的。其他对别人有用的东西，暂时还在思考中。</p>
</blockquote>
<h2 id="怎么写"><a href="#怎么写" class="headerlink" title="怎么写"></a>怎么写</h2><blockquote>
<p>用笔写</p>
</blockquote>
<p>为什么写，以及写什么都考虑清楚之后，怎么写就不再是个问题了。如果你习惯写在纸质的本子上，那么买一个自己喜欢的本子，一只自己喜欢的笔，直接写就好了；如果你习惯使用电脑或者手机写，那么新建一个文件夹，在里面写就好了。</p>
<p>写的东西并不一定要公开，对于暂时不成熟的东西，或者私密的东西，保存在一个只有自己能看的地方就好了；对于成熟的东西，或者可以公开的东西，公开发表就好了。公开发表有一个好处，得到更多的反馈，这些反馈是非常利于自己进步的。</p>
<p>另外，有一个很简单的小技巧。自己维护一个素材记录本（可以是纸质的本子，可以是电子产品 – 建议购买正版），里面记录自己觉得不错的句子，例子，表达方式等等，偶尔翻一翻，或者打算写作的时候进行一下搜索。</p>
<h2 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h2><ol>
<li>写的不好怎么办  <blockquote>
<p>没有人一生下来就会吃饭吗？也没有人长大之后不会吃饭</p>
</blockquote>
</li>
<li>不知道怎么写开头怎么办<blockquote>
<p>如果不写开头的话，知道怎么写吗？知道的话，那就不要写开头了，为什么一定要写开头呢</p>
</blockquote>
</li>
<li>要写多少才合适呢<blockquote>
<p>没有一个统一的结论写多少才合适，只要能将自己想表达的东西讲清楚就行。写作最终受益的是自己，这不是在考试，不要为了凑字数而写。</p>
</blockquote>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这里的写作不特指写长篇大论的文章&lt;/p&gt;
&lt;h2 id=&quot;为什么要写作&quot;&gt;&lt;a href=&quot;#为什么要写作&quot; class=&quot;headerlink&quot; title=&quot;为什么要写作&quot;&gt;&lt;/a&gt;为什么要写作&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;写作是了帮助自己更好的思考，提高自己的效率。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;首先，每个人同一时刻能记住的东西有限，而做一件事情可能需要考虑的条件往往会比较多，将所有的情况写到一张纸上，就能在需要的时候看到自己需要的条件。相信每个人都有这样的体验：做数学题的时候，将所有的已知条件，或者自己推导出来的结论写到草稿纸上，往往能更快的解出这道题目。这也是写作帮助自己思考的一个直接。&lt;/p&gt;
&lt;p&gt;其次，我遇到过这种情况，自己想着是这么回事，可是写下来的时候，发现无从下笔，根本写不出来，写出来之后，也很难向别人清楚、准确的表达自己的意思。这归根结底还是没有思考清楚导致的，而写作可以帮助我整理自己的思路。&lt;/p&gt;
&lt;p&gt;然后，当过客服的人肯定有一个体会，客服非常耗时间，而这些客服的问题大部分是差不多的，一对一的客服是非常低效的，就算在群组里面进行客服，其实很多后来遇到同样问题的人也是不看群历史记录（或者不知道以前有人遇到过类似的问题）的。这种情况下，虽然文档不是最优解，但能够省出自己足够多的时间，实际上这是一件杠杆率非常高（产出投入比高）的事件。&lt;/p&gt;
    
    </summary>
    
      <category term="我的生活" scheme="http://yoursite.com/categories/%E6%88%91%E7%9A%84%E7%94%9F%E6%B4%BB/"/>
    
      <category term="想清楚" scheme="http://yoursite.com/categories/%E6%88%91%E7%9A%84%E7%94%9F%E6%B4%BB/%E6%83%B3%E6%B8%85%E6%A5%9A/"/>
    
    
      <category term="写作" scheme="http://yoursite.com/tags/%E5%86%99%E4%BD%9C/"/>
    
      <category term="成长" scheme="http://yoursite.com/tags/%E6%88%90%E9%95%BF/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming 统一在每分钟的 00 秒消费 Kafka 数据的问题解决</title>
    <link href="http://yoursite.com/2017/02/16/spark-streaming-consume-kafka-at-00-second-of-every-minute/"/>
    <id>http://yoursite.com/2017/02/16/spark-streaming-consume-kafka-at-00-second-of-every-minute/</id>
    <published>2017-02-16T15:44:31.000Z</published>
    <updated>2017-06-03T03:38:41.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="现象"><a href="#现象" class="headerlink" title="现象"></a>现象</h2><p>一批 Spark Streaming 会统一在每分钟的 00 秒开始消费 kafka 数据</p>
<a id="more"></a>
<h2 id="问题排查"><a href="#问题排查" class="headerlink" title="问题排查"></a>问题排查</h2><p>这一批作业的功能就是从 kafka 消费数据，进行转化后存储到外部可靠介质中。所有作业的 <code>batchDuration</code> 都设置为 60s。<br>我们追踪代码可以得到在 <code>JobGenerator</code> 中有一个变量 <code>timer : RecurringTimer</code>，改变量用于定时的启动 task 去消费数据。<br>从 <code>RecurringTimer#getStartTime</code> 我们可以得到作业第一个 batch 的启动时间，后续的 batch 启动时间则是在第一个 batch 的启动时间上加上 <code>batchDuration</code> 的整数倍。<br>第一个 batch 的起动时间实现如下：<br><code>(math.floor(clock.getTimeMillis().toDouble / period) + 1).toLong * period</code><br>其中 <code>clock.getTimeMillis()</code> 是当前时间，period 是<code>batchDuration</code> 的毫秒表示法。通过上述公式，我们可以知道作业的启动时间会对齐到 <code>batchDuration</code>，而我们把这一批作业的 <code>batchDuration</code> 都设置为 60s，因此都会在每分钟的 00 秒开始消费 kafka 数据。</p>
<h2 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h2><p>我们可以通过下面两种方式进行解决</p>
<ol>
<li>设置不同的 <code>batchDuration</code></li>
<li>改写 <code>RecurringTimer#getStartTime</code> 的逻辑，在上述对齐的时间基础上加上一个 [0, period) 范围内的随机数</li>
</ol>
<p>我们知道在上述两种解决方案中，第一种，不同作业还是会在某一时刻重合，而且这个重合的时间点不可控，可能是作业运行一小时后，可能是运行一天后，也可能是运行一周后。而第二种作业则是可控的，在作业启动时就决定了。因此这里我们采用第二种方案。</p>
<p>本文采用了一种新的排版方式，在进行实验，如果效果好的好，后续大部分内容都会以这种形式进行发布</p>
<div class="markdown-here-wrapper" style="font-size: 16px; line-height: 1.8em; letter-spacing: 0.1em;" data-md-url="http://www.klion26.com/wp-admin/post-new.php"></div>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;现象&quot;&gt;&lt;a href=&quot;#现象&quot; class=&quot;headerlink&quot; title=&quot;现象&quot;&gt;&lt;/a&gt;现象&lt;/h2&gt;&lt;p&gt;一批 Spark Streaming 会统一在每分钟的 00 秒开始消费 kafka 数据&lt;/p&gt;
    
    </summary>
    
      <category term="分布式系统" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="spark_streaming" scheme="http://yoursite.com/tags/spark-streaming/"/>
    
      <category term="kafka" scheme="http://yoursite.com/tags/kafka/"/>
    
      <category term="problem_solve" scheme="http://yoursite.com/tags/problem-solve/"/>
    
      <category term="batchDuration" scheme="http://yoursite.com/tags/batchDuration/"/>
    
      <category term="JobGenerator" scheme="http://yoursite.com/tags/JobGenerator/"/>
    
      <category term="RecurringTimer" scheme="http://yoursite.com/tags/RecurringTimer/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming 往 HDFS 追加 LZO 文件</title>
    <link href="http://yoursite.com/2017/01/15/spark-streaming-e5-be-80-hdfs-e8-bf-bd-e5-8a-a0-lzo-e6-96-87-e4-bb-b6/"/>
    <id>http://yoursite.com/2017/01/15/spark-streaming-e5-be-80-hdfs-e8-bf-bd-e5-8a-a0-lzo-e6-96-87-e4-bb-b6/</id>
    <published>2017-01-15T07:38:17.000Z</published>
    <updated>2017-06-03T03:39:14.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>将数据从 Kafka 同步到 Hive，并且目标格式希望是 lzo。我们通过 Spark Streaming 做这件事，将文件写成 lzo 格式，并且添加索引。</p>
<a id="more"></a>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>要实现将数据从 Kafka 同步到 Hive 的功能，我们通过将数据直接写到 HDFS 路径来解决，由于担心小文件太多的问题（一个 batch 一个文件的话，可能造成小文件太多，对 HDFS 造成非常大的压力），所以我们通过追加的方式写 HDFS 文件。</p>
<p>往 HDFS 追加写文件的方式，我们在前面一篇文章中描述了具体的方案。但是对于格式为 LZO 的文件，我们发现一个现象：通过 Hive 查询，只能查到第一个 batch 的数据（也就是说所有 append 的数据都不能被查询到）。这是因为 LZO 文件会在关闭的时候在文件末尾添加一个块结束标记符，导致解析的时候只能读取到块结束符之前的数据（Linux 自带的 lzop 文件可以解析包含块结束符的文件）。到这里我们有两个思路：</p>
<pre><code>1. 在 Hive 层面进行修改，将 Hive 使用的 InputFormat 重新实现，从而可以解析 multipart 的文件；
2. 通过某种方式将文件进行追加，但是文件的中间不会出现结束块的标记符。
</code></pre><p>由于第一种方式影响较大，实现起来周期较长，所以这里采用第二种方法。</p>
<p>我们考虑如何做到往 HDFS 写完数据之后，文件流不进行关闭，在我们需要关闭的时候再手动关闭。也就是说同一个 Executor 上的多 batch 公用同一个文件流。</p>
<p>查看<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#design-patterns-for-using-foreachrdd" target="_blank" rel="external">官方文档</a>我们可以得到这是可以实现的，也就是文档中的 ConnectionPool 实现的方式，这可以做到在同一个 Executor 上执行的多个 batch 公用同一个文件流（个人觉得这里也可以从 JVM 的层面来考虑，就是利用了 static 变量的声明周期以及可访问范围）。</p>
<p>当我们手动关闭某个文件的时候，再考虑将这个文件 move 到特定的地方（Hive 表对应的 HDFS 路径），然后添加索引，大致框架就完成了。当然这也仅仅是一个框架，需要处理的细节问题还有很多。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;需求&quot;&gt;&lt;a href=&quot;#需求&quot; class=&quot;headerlink&quot; title=&quot;需求&quot;&gt;&lt;/a&gt;需求&lt;/h2&gt;&lt;p&gt;将数据从 Kafka 同步到 Hive，并且目标格式希望是 lzo。我们通过 Spark Streaming 做这件事，将文件写成 lzo 格式，并且添加索引。&lt;/p&gt;
    
    </summary>
    
      <category term="实时计算" scheme="http://yoursite.com/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
    
      <category term="spark_streaming" scheme="http://yoursite.com/tags/spark-streaming/"/>
    
      <category term="kafka" scheme="http://yoursite.com/tags/kafka/"/>
    
      <category term="append" scheme="http://yoursite.com/tags/append/"/>
    
      <category term="hdfs" scheme="http://yoursite.com/tags/hdfs/"/>
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
      <category term="lzo" scheme="http://yoursite.com/tags/lzo/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming Ran out of messages before reaching ending offset 异常</title>
    <link href="http://yoursite.com/2016/12/16/spark-streaming-ran-out-of-messages-before-reaching-ending-offset/"/>
    <id>http://yoursite.com/2016/12/16/spark-streaming-ran-out-of-messages-before-reaching-ending-offset/</id>
    <published>2016-12-16T06:23:13.000Z</published>
    <updated>2017-06-03T03:39:33.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="现象"><a href="#现象" class="headerlink" title="现象"></a>现象</h2><p>Spark Streaming 处理数据过程中遇到 <code>Ran out of messages before reaching ending offset</code> 异常，导致程序一直 hang 住（因为我们希望接上次消费从而不丢失数据）</p>
<a id="more"></a>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>通过异常栈信息，我们知道异常从 KafkaRDD.scala#211 行抛出，下面是相应代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">206 override def getNext(): R = &#123;</div><div class="line">	  if (iter == null || !iter.hasNext) &#123;</div><div class="line">208        iter = fetchBatch</div><div class="line">      &#125;</div><div class="line">210      if (!iter.hasNext) &#123;</div><div class="line">211        assert(requestOffset == part.untilOffset, errRanOutBeforeEnd(part))</div><div class="line">212        finished = true</div><div class="line">           null.asInstanceOf[R]</div><div class="line">		&#125; else &#123;</div><div class="line">		   	val item = iter.next()</div><div class="line">			if (item.offset &gt;= part.untilOffset) &#123;</div><div class="line">217 	        assert(item.offset == part.untilOffset, errOvershotEnd(item.offset, part))</div><div class="line">				finished = true</div><div class="line">				null.asInstanceOf[R]</div><div class="line">			&#125; else &#123;</div><div class="line">				requestOffset = item.nextOffset</div><div class="line">				messageHandler(new MessageAndMetadata(</div><div class="line">				part.topic, part.partition, item.message, item.offset, keyDecoder, valueDecoder))</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">226    &#125;</div></pre></td></tr></table></figure>
<p>通过分析，我们知道这个地方是实际从 Kafka 读取数据的逻辑，首先会调用 <code>fetchBatch</code> 函数（208 行），然后进行逻辑判断，数据是否读取完毕，是否发生异常</p>
<p>其中 211 行的异常表示还未读取到 part.untilOffset 但是当前迭代器中没有数据了；217 行表示当前读取的数据如果超过了 part.untilOffset ，那么在这个时候退出当前 batch（offset 从 fromOffset 逐次加一增加的，正常的逻辑肯定会和 part.untilOffset 相等）</p>
<p>我们知道异常从 211 行抛出来的，也知道了异常的最直接原因，那么这个原因是什么造成的呢？</p>
<p>211 行的代码执行了，也就是 210 行的 if 语句未 true，这样的话，207 行的逻辑也应该为 true。这样的话 iter 就是 fetchBatch 返回的迭代器了。接下来我们看看 fetchBatch 的代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">188 private def fetchBatch: Iterator[MessageAndOffset] = &#123;</div><div class="line">189      val req = new FetchRequestBuilder()</div><div class="line">190         .addFetch(part.topic, part.partition, requestOffset, kc.config.fetchMessageMaxBytes)</div><div class="line">			.build()</div><div class="line">192      val resp = consumer.fetch(req)</div><div class="line">	   	 handleFetchErr(resp)</div><div class="line">		// kafka may return a batch that starts before the requested offset</div><div class="line">		 resp.messageSet(part.topic, part.partition)</div><div class="line">196       .iterator</div><div class="line">          .dropWhile(_.offset &lt; requestOffset)</div><div class="line">		&#125;</div></pre></td></tr></table></figure></p>
<p>我们发现 192 行会通过 consumer 从 kafka 获取数据，本次从哪获取数据，以及获取多少分别由 190 行的 <code>topic</code>, <code>partition</code> 和 <code>kc.config.fetchMessageMaxBytes</code> 指定。我们查看 <code>kc.config.fetchMessageMaxBytes</code>，发现默认使用的是 1M</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ConsumerConfig.scala</div><div class="line">29 val FetchSize = 1024 * 1024</div><div class="line"></div><div class="line">114 val fetchMessageMaxBytes = props.getInt(&quot;fetch.message.max.bytes&quot;, FetchSize)</div></pre></td></tr></table></figure>
<p>从这里我们知道每次从 kafka 上最多获取 1M 的数据（这也是为什么需要在 <code>KafkaRDD.getNext</code> 函数的开头通过 <code>iter.hasNext()</code> 来判断是否需要调用 <code>fetchBatch</code> </p>
<p>然后看到 fetchBatch 函数对应的 196 行，获取迭代器作为返回值，查看相应代码，跳转到 <code>ByteBufferMessageSet.scala</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">139 override def iterator: Iterator[MessageAndOffset] = internalIterator()</div><div class="line">145 private def internalIterator(isShallow: Boolean = false): Iterator[MessageAndOffset] = &#123;</div><div class="line">		    new IteratorTemplate[MessageAndOffset] &#123;</div><div class="line">				        ......</div><div class="line">152      def makeNextOuter: MessageAndOffset = &#123;</div><div class="line">             // if there isn&apos;t at least an offset and size, we are done</div><div class="line">			if (topIter.remaining &lt; 12)</div><div class="line">				return allDone()</div><div class="line">			    val offset = topIter.getLong()</div><div class="line">			    val size = topIter.getInt()</div><div class="line">		        if(size &lt; Message.MinHeaderSize)</div><div class="line">			         throw new InvalidMessageException(&quot;Message found with corrupt size (&quot; + size + &quot;)&quot;)</div><div class="line">					</div><div class="line">160       // we have an incomplete message</div><div class="line">161       if(topIter.remaining &lt; size)</div><div class="line">162         return allDone()</div><div class="line">		....</div><div class="line">185     &#125;</div></pre></td></tr></table></figure>
<p>从 161 行我们可以看出，如果读取的消息是一条不完整的，那么本次不处理，默认本次消息读取完成。<br>上面所有的链条穿起来就抛出了我们文章开始的异常。</p>
<pre><code>1. 从 kafka 读取 1M 的数据（默认大小）
2. 发现读取的数据不完整（这个消息的大小大于 1M），所以本次读取的 迭代器 为空
3. 发现迭代器为空，但是当前的 offset 和 part.untilOffset 不想等，抛出异常
</code></pre><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>通过设置 kafkaParam 的参数 <code>fetch.message.max.bytes</code> 就行了，我们设置成 2M（大于一条数据的最大值即可），就能够运行成功了</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;现象&quot;&gt;&lt;a href=&quot;#现象&quot; class=&quot;headerlink&quot; title=&quot;现象&quot;&gt;&lt;/a&gt;现象&lt;/h2&gt;&lt;p&gt;Spark Streaming 处理数据过程中遇到 &lt;code&gt;Ran out of messages before reaching ending offset&lt;/code&gt; 异常，导致程序一直 hang 住（因为我们希望接上次消费从而不丢失数据）&lt;/p&gt;
    
    </summary>
    
      <category term="实时计算" scheme="http://yoursite.com/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="kafka" scheme="http://yoursite.com/tags/kafka/"/>
    
      <category term="spark-streaming" scheme="http://yoursite.com/tags/spark-streaming/"/>
    
      <category term="exception" scheme="http://yoursite.com/tags/exception/"/>
    
      <category term="ran_out_of_messages" scheme="http://yoursite.com/tags/ran-out-of-messages/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming 从指定时间戳开始消费 kafka 数据</title>
    <link href="http://yoursite.com/2016/12/02/spark-streaming-consume-kafka-message-from-specify-timestamp/"/>
    <id>http://yoursite.com/2016/12/02/spark-streaming-consume-kafka-message-from-specify-timestamp/</id>
    <published>2016-12-02T11:27:49.000Z</published>
    <updated>2017-06-03T03:39:56.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>从指定时间戳（比如 2 小时）开始消费 Kafka 数据</p>
<a id="more"></a>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>我们知道通过 Kafka 的 API 可以得到指定时间戳对应数据所在的 segment 的起始 offset。那么就可以通过这个功能来粗略的实现需求。</p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>我们知道 <code>KafkaUitls.createDirectStream</code> 这个接口可以指定起始点的 offset，那么我们需要做的就变成如下三步：</p>
<ol>
<li>获取 <code>topic</code> 对应的 <code>TopicAndPartitions</code>，得到当前 topic 有多少 partition</li>
<li>从 Kafka 获取每个 partition 指定时间戳所在 segment 的起始 offset</li>
<li>将步骤 2 中的 offset 作为参数传入 <code>createDirectStream</code> 即可<br>通过查看源码，我们知道步骤 1 和步骤 2 中的功能在 <code>org.apache.spark.streaming.kafka.KafkaCluster</code> 中都已经有现成的函数了：<code>getPartitions</code> 和 <code>getLeaderOffsets</code>，分别表示获取指定 topic 的 partition 以及获取 partition 指定时间戳所在的 segment 的起始 offset，那么我们需要做的就是如何调用这两个函数实现我们的功能了。</li>
</ol>
<p>我们知道 <code>KafkaCluster</code> 的作用域是 <code>private[spark]</code> 所以我们需要在自己的代码中使用 <code>package org.apache.spark(.xxx ... .yyy)</code>(小括号中表示可以省略）来限定自己的代码，因此我们可以将步骤 1 和步骤 2 中的功能实现如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">package org.apache.spark.streaming.kafka</div><div class="line">......      //省略其他不相关的代码</div><div class="line"></div><div class="line">def getPartitions(kafkaParams: Map[String, String], topics: Set[String]): Either[Err, Set[TopicAndPartition]] = &#123;</div><div class="line">        val kc = new KafkaCluster(kafkaParams)</div><div class="line">        kc.getPartitions(topics)    //我们可以在这里处理错误，也可以将错误继续往上传递</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    def getLeaderOffsets(kafkaParams: Map[String, String], topicAndPartitions: Set[TopicAndPartition], before: Long) : Map[TopicAndPartition, Long]  = &#123;</div><div class="line">        val kc = new KafkaCluster(kafkaParams)</div><div class="line">        val leaderOffsets = kc.getLeaderOffsets(topicAndPartitions, before)</div><div class="line">        if (leaderOffsets.isLeft) &#123;  //在本函数内部处理错误，如果有错误抛出异常</div><div class="line">            throw new RuntimeException(s&quot;### Exception when MTKafkaUtils#getLeaderOffsets $&#123;leaderOffsets.left.get&#125; ###&quot;)</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        leaderOffsets.right.get.map &#123; case (k, v) =&gt; (k, v.offset)&#125;  //将 Map[TopicAndPartition, LeaderOffset] 转变为 Map[TopicAndPartition, Long]（Long 为对应 partition 的 offset，从 LeaderOffset 中获取）</div><div class="line">    &#125;</div></pre></td></tr></table></figure>
<p>步骤 3 直接传入参数即可，就可以从指定时间戳开始消费 Kafka 数据了</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;需求&quot;&gt;&lt;a href=&quot;#需求&quot; class=&quot;headerlink&quot; title=&quot;需求&quot;&gt;&lt;/a&gt;需求&lt;/h2&gt;&lt;p&gt;从指定时间戳（比如 2 小时）开始消费 Kafka 数据&lt;/p&gt;
    
    </summary>
    
      <category term="分布式系统" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="实时计算" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
    
      <category term="kafka" scheme="http://yoursite.com/tags/kafka/"/>
    
      <category term="spark-streaming" scheme="http://yoursite.com/tags/spark-streaming/"/>
    
      <category term="specify_timestamp" scheme="http://yoursite.com/tags/specify-timestamp/"/>
    
      <category term="timestamp" scheme="http://yoursite.com/tags/timestamp/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming 往 HDFS 写文件，自定义文件名</title>
    <link href="http://yoursite.com/2016/11/26/spark-streaming-e5-be-80-hdfs-e5-86-99-e6-96-87-e4-bb-b6-ef-bc-8c-e8-87-aa-e5-ae-9a-e4-b9-89-e6-96-87-e4-bb-b6-e5-90-8d/"/>
    <id>http://yoursite.com/2016/11/26/spark-streaming-e5-be-80-hdfs-e5-86-99-e6-96-87-e4-bb-b6-ef-bc-8c-e8-87-aa-e5-ae-9a-e4-b9-89-e6-96-87-e4-bb-b6-e5-90-8d/</id>
    <published>2016-11-26T08:09:25.000Z</published>
    <updated>2017-06-03T03:40:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>将 kafka 上的数据实时同步到 HDFS，不能有太多小文件</p>
<a id="more"></a>
<h2 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h2><p>Spark Streaming 支持 RDD#saveAsTextFile，将数据以 <strong>纯文本</strong> 方式写到 HDFS，我们查看 RDD#saveAsTextFile 可以看到</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">RDD.rddToPairRDDFunctions(r)(nullWritableClassTag, textClassTag, null)</div><div class="line">      .saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path)</div></pre></td></tr></table></figure>
<p>从上面这句话我们可以知道，首先将 RDD 转化为 PariRDD，然后再调用 saveAsHadoopFile 函数进行实际的操作。上面的语句中 <code>r</code> 是原始 RDD，<code>nullWritableClassTag</code> 和 <code>textClassTag</code> 表示所写数据的类型，使用 <code>nullWritableClassTag</code> 是因为 HDFS 不会将这个数据进行实际写入（pariRDD 是 (K,V) 类型， 我们只需要写入 V），从效果上看就只写如后面的一个字段。<code>TextOutputFormat</code> 是一个格式化函数，后面我们再来看这个函数，<code>NullWritable</code> 则表示一个占位符，同样是这个字段不需要实际写入 HDFS，<code>Text</code> 表示我们将写入文本类型的数据。</p>
<p>我们看到 <code>TextOutputFormat</code> 这个类中有一个函数是 <code>RecordWriter</code> 用于操作没一条记录的写入，代码如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">public RecordWriter&lt;K, V&gt; getRecordWriter(FileSystem ignored, JobConf job, String name, Progressable progress) throws IOException &#123;</div><div class="line">	boolean isCompressed = getCompressOutput(job);</div><div class="line">	String keyValueSeparator = job.get(&quot;mapreduce.output.textoutputformat.separator&quot;, &quot;\t&quot;);</div><div class="line">	if(!isCompressed) &#123;</div><div class="line">	    Path codecClass1 = FileOutputFormat.getTaskOutputPath(job, name);</div><div class="line">		FileSystem codec1 = codecClass1.getFileSystem(job);</div><div class="line">		FSDataOutputStream file1 = codec1.create(codecClass1, progress);</div><div class="line">		return new TextOutputFormat.LineRecordWriter(file1, keyValueSeparator);</div><div class="line">	&#125; else &#123;</div><div class="line">	    Class codecClass = getOutputCompressorClass(job, GzipCodec.class);</div><div class="line">		CompressionCodec codec = (CompressionCodec)ReflectionUtils.newInstance(codecClass, job);</div><div class="line">		Path file = FileOutputFormat.getTaskOutputPath(job, name + codec.getDefaultExtension());</div><div class="line">		FileSystem fs = file.getFileSystem(job);</div><div class="line">		FSDataOutputStream fileOut = fs.create(file, progress);</div><div class="line">		return new TextOutputFormat.LineRecordWriter(new DataOutputStream(codec.createOutputStream(fileOut)), keyValueSeparator);</div><div class="line">		&#125;</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<p>文件中分为两部分：1）压缩文件，2）非压缩文件。然后剩下的事情就是打开文件，往文件中写数据了。</p>
<p>说到压缩文件，就和写 lzo 格式关联起来了，因为 lzo 格式就是压缩的，那么我们从哪拿到这个压缩的格式的呢？实际上 PariRDDFunctions#saveAsHadoopFile 还可以传入压缩格式类，函数原型如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">def saveAsHadoopFile[F &lt;: OutputFormat[K, V]](</div><div class="line">    path: String,</div><div class="line">    codec: Class[_ &lt;: CompressionCodec])(implicit fm: ClassTag[F]): Unit = self.withScope &#123;</div></pre></td></tr></table></figure>
<p>这里第二个参数表示压缩的类。如果需要我们传入一个压缩类即可，如 <code>classOf[com.hadoop.compression.lzo.LzopCodec]</code> 最终这个参数会传给 <code>TextOutputFormat#RecordWriter</code>.</p>
<p>至此，我们以及可以写 lzo 格式的文件了。但是还没有结束，因为会产生小文件，每个 RDD 的每个 partition 都会在 HDFS 上产生一个文件，而且这些文件大小非常小，就形成了很多小文件，这对 HDFS 的压力会非常大。我们需要解决这个问题</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;需求&quot;&gt;&lt;a href=&quot;#需求&quot; class=&quot;headerlink&quot; title=&quot;需求&quot;&gt;&lt;/a&gt;需求&lt;/h2&gt;&lt;p&gt;将 kafka 上的数据实时同步到 HDFS，不能有太多小文件&lt;/p&gt;
    
    </summary>
    
      <category term="分布式系统" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="实时计算" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="spark-streaming" scheme="http://yoursite.com/tags/spark-streaming/"/>
    
      <category term="append" scheme="http://yoursite.com/tags/append/"/>
    
      <category term="hdfs" scheme="http://yoursite.com/tags/hdfs/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming 自适应上游 kafka topic partition 数目变化</title>
    <link href="http://yoursite.com/2016/11/01/spark-streaming-topic-partition-change-auto-adaptive/"/>
    <id>http://yoursite.com/2016/11/01/spark-streaming-topic-partition-change-auto-adaptive/</id>
    <published>2016-11-01T08:56:13.000Z</published>
    <updated>2017-06-03T03:40:54.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Spark Streaming 作业在运行过程中，上游 topic 增加 partition 数目从 A 增加到 B，会造成作业丢失数据，因为该作业只从 topic 中读取了原来的 A 个 partition 的数据，新增的 B-A 个 partition 的数据会被忽略掉。</p>
<a id="more"></a>
<h2 id="思考过程"><a href="#思考过程" class="headerlink" title="思考过程"></a>思考过程</h2><p>为了作业能够长时间的运行，一开始遇到这种情况的时候，想到两种方案：</p>
<ol>
<li>感知上游 topic 的 partition 数目变化，然后发送报警，让用户重启</li>
<li>直接在作业内部自适应上游 topic partition 的变化，完全不影响作业<br>方案 1 是简单直接，第一反应的结果，但是效果不好，需要用户人工介入，而且需要删除 checkpoint 文件</li>
</ol>
<p>方案 2 从根本上解决问题，用户不需要关心上游 partition 数目的变化，但是第一眼会觉得较难实现。</p>
<p>方案 1 很快被 pass 掉，因为人工介入的成本太高，而且实现起来很别扭。接下来考虑方案 2.</p>
<p>Spark Streaming 程序中使用 Kafka 的最原始方式为 <code>KafkaUtils.createDirectStream</code> 通过源码，我们找到调用链条大致是这样的</p>
<p><span style="color: #0000ff;"><strong><code>KafkaUtils.createDirectStream</code></strong></span>   –&gt;   <strong><span style="color: #0000ff;"><code>new DirectKafkaInputDStream</code></span></strong> –&gt; 最终由 <code>DirectKafkaInputDStream#compute(validTime : Time)</code> 函数来生成 KafkaRDD。</p>
<p>而 KafkaRDD 的 partition 数和 <strong><span style="color: #0000ff;">作业开始运行时</span></strong> topic 的 partition 数一致，topic 的 partition 数保存在 currentOffsets 变量中，currentOffsets 是一个 Map[TopicAndPartition, Long]类型的变量，保存每个 partition 当前消费的 offset 值，但是作业运行过程中 currentOffsets 不会增加 key，就是说不会增加 KafkaRDD 的 partition，这样导致每次生成 KafkaRDD 的时候都使用 <span style="color: #0000ff;"><strong>作业开始运行时</strong></span> topic 的 partition 数作为 KafkaRDD 的 partition 数，从而会造成数据的丢失。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>我们只需要在每次生成 KafkaRDD 的时候，将 currentOffsets 修正为正常的值（往里面增加对应的 partition 数，总共 B-A 个，以及每个增加的 partition 的当前 offset 从零开始）。</p>
<ul>
<li>第一个问题出现了，我们不能修改 Spark 的源代码，重新进行编译，因为这不是我们自己维护的。想到的一种方案是继承 DirectKafkaInputDStream。我们发现不能继承 DirectKafkaInputDStream 该类，因为这个类是使用 <code>private[streaming]</code> 修饰的。</li>
<li>第二个问题出现了，怎么才能够继承 DirectKafkaInputDStream，这时我们只需要将希望继承 DirectKafkaInputDStream 的类放到一个单独的文件 F 中，文件 F 使用 <code>package org.apache.spark.streaming</code> 进行修饰即可，这样可以绕过不能继承 DirectKafkaInputDStream 的问题。这个问题解决后，我们还需要修改 <code>Object KafkaUtils</code>，让该 Object 内部调用我们修改后的 DirectKafkaInputDStream（我命名为 MTDirectKafkaInputDStream)</li>
<li>第三个问题如何让 Spark 调用 MTDirectKafkaInputDStream，而不是 DirectKafkaInputDStream，这里我们使用简单粗暴的方式，将 KafkaUtils 的代码 copy 一份，然后将其中调用 DirectKafkaInputDStream 的部分都修改为 MTDirectKafkaInputDStream，这样就实现了我们的需要。当然该文件也需要使用 <code>package org.apache.spark.streaming</code> 进行修饰</li>
<li>第二个和第三个问题的解决方案在  中国 Spark 技术峰会 2016  上，广点通的 林立伟 有提及，后续会进行尝试<br>总结下，我们需要做两件事</li>
</ul>
<ol>
<li>修改 DirectKafkaInputDStream#compute 使得能够自适应 topic 的 partition 变更</li>
<li>修改 KafkaUtils，使得我们能够调用修改过后的 DirectKafkaInputDStream</li>
</ol>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div></pre></td><td class="code"><pre><div class="line">package org.apache.spark.streaming.kafka.mt</div><div class="line"></div><div class="line">import com.meituan.data.util.Constants</div><div class="line">import com.meituan.service.inf.kms.client.Kms</div><div class="line">import kafka.common.&#123;ErrorMapping, TopicAndPartition&#125;</div><div class="line">import kafka.javaapi.&#123;TopicMetadata, TopicMetadataRequest&#125;</div><div class="line">import kafka.javaapi.consumer.SimpleConsumer</div><div class="line">import kafka.message.MessageAndMetadata</div><div class="line">import kafka.serializer.Decoder</div><div class="line">import org.apache.spark.streaming.&#123;StreamingContext, Time&#125;</div><div class="line">import org.apache.spark.streaming.kafka.&#123;DirectKafkaInputDStream, KafkaRDD&#125;</div><div class="line"></div><div class="line">import scala.collection.JavaConverters._</div><div class="line">import scala.util.control.Breaks._</div><div class="line">import scala.reflect.ClassTag</div><div class="line"></div><div class="line">/**</div><div class="line">  * Created by qiucongxian on 10/27/16.</div><div class="line">  */</div><div class="line">class MTDirectKafkaInputDStream[</div><div class="line">  K: ClassTag,</div><div class="line">  V: ClassTag,</div><div class="line">  U &lt;: Decoder[K]: ClassTag,</div><div class="line">  T &lt;: Decoder[V]: ClassTag,</div><div class="line">  R: ClassTag](</div><div class="line">    @transient ssc_ : StreamingContext,</div><div class="line">    val MTkafkaParams: Map[String, String],</div><div class="line">    val MTfromOffsets: Map[TopicAndPartition, Long],</div><div class="line">    messageHandler: MessageAndMetadata[K, V] =&gt; R</div><div class="line">) extends DirectKafkaInputDStream[K, V, U, T, R](ssc_, MTkafkaParams , MTfromOffsets, messageHandler) &#123;</div><div class="line">    private val kafkaBrokerList : String = &quot;host1:port1,host2:port2,host3:port3&quot; //根据自己的情况自行修改</div><div class="line"></div><div class="line">    override def compute(validTime: Time) : Option[KafkaRDD[K, V, U, T, R]] = &#123;</div><div class="line">      /**</div><div class="line">        * 在这更新 currentOffsets 从而做到自适应上游 partition 数目变化</div><div class="line">        */</div><div class="line">        updateCurrentOffsetForKafkaPartitionChange()</div><div class="line">        super.compute(validTime)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    private def updateCurrentOffsetForKafkaPartitionChange() : Unit = &#123;</div><div class="line">      val topic = currentOffsets.head._1.topic</div><div class="line">      val nextPartitions : Int = getTopicMeta(topic) match &#123;</div><div class="line">          case Some(x) =&gt; x.partitionsMetadata.size()</div><div class="line">          case _ =&gt; 0</div><div class="line">      &#125;</div><div class="line">      val currPartitions = currentOffsets.keySet.size</div><div class="line"></div><div class="line">      if (nextPartitions &gt; currPartitions) &#123;</div><div class="line">        var i = currPartitions</div><div class="line">        while (i &lt; nextPartitions) &#123;</div><div class="line">           currentOffsets = currentOffsets + (TopicAndPartition(topic, i) -&gt; 0)</div><div class="line">           i = i + 1</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">      logInfo(s&quot;######### $&#123;nextPartitions&#125;  currentParttions $&#123;currentOffsets.keySet.size&#125; ########&quot;)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    private def getTopicMeta(topic: String) : Option[TopicMetadata] = &#123;</div><div class="line">        var metaData : Option[TopicMetadata] = None</div><div class="line">        var consumer : Option[SimpleConsumer] = None</div><div class="line"></div><div class="line">        val topics = List[String](topic)</div><div class="line">        val brokerList = kafkaBrokerList.split(&quot;,&quot;)</div><div class="line">        brokerList.foreach(</div><div class="line">          item =&gt; &#123;</div><div class="line">            val hostPort = item.split(&quot;:&quot;)</div><div class="line">            try &#123;</div><div class="line">              breakable &#123;</div><div class="line">                  for (i &lt;- 0 to 3) &#123;</div><div class="line">                      consumer = Some(new SimpleConsumer(host = hostPort(0), port = hostPort(1).toInt,</div><div class="line">                                            soTimeout = 10000, bufferSize = 64 * 1024, clientId = &quot;leaderLookup&quot;))</div><div class="line">                      val req : TopicMetadataRequest = new TopicMetadataRequest(topics.asJava)</div><div class="line">                      val resp = consumer.get.send(req)</div><div class="line"></div><div class="line">                      metaData = Some(resp.topicsMetadata.get(0))</div><div class="line">                      if (metaData.get.errorCode == ErrorMapping.NoError) break()</div><div class="line">                  &#125;</div><div class="line">              &#125;</div><div class="line">            &#125; catch &#123;</div><div class="line">              case e =&gt; logInfo(s&quot; ###### Error in MTDirectKafkaInputDStream $&#123;e&#125; ######&quot;)</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        )</div><div class="line">        metaData</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在修改过后的 KafkaUtils 文件中，将所有的 <code>DirectKafkaInputDStream</code> 都替换为 <code>MTDirectKafkaInputDStream</code> 即可</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;Spark Streaming 作业在运行过程中，上游 topic 增加 partition 数目从 A 增加到 B，会造成作业丢失数据，因为该作业只从 topic 中读取了原来的 A 个 partition 的数据，新增的 B-A 个 partition 的数据会被忽略掉。&lt;/p&gt;
    
    </summary>
    
      <category term="分布式系统" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="实时计算" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
    
      <category term="kafka" scheme="http://yoursite.com/tags/kafka/"/>
    
      <category term="spark-streaming" scheme="http://yoursite.com/tags/spark-streaming/"/>
    
      <category term="auto-adaptive" scheme="http://yoursite.com/tags/auto-adaptive/"/>
    
      <category term="partition" scheme="http://yoursite.com/tags/partition/"/>
    
      <category term="topic" scheme="http://yoursite.com/tags/topic/"/>
    
  </entry>
  
  <entry>
    <title>要多快才能跑完一场马拉松</title>
    <link href="http://yoursite.com/2016/10/26/e8-a6-81-e5-a4-9a-e5-bf-ab-e6-89-8d-e8-83-bd-e8-b7-91-e5-ae-8c-e4-b8-80-e5-9c-ba-e9-a9-ac-e6-8b-89-e6-9d-be/"/>
    <id>http://yoursite.com/2016/10/26/e8-a6-81-e5-a4-9a-e5-bf-ab-e6-89-8d-e8-83-bd-e8-b7-91-e5-ae-8c-e4-b8-80-e5-9c-ba-e9-a9-ac-e6-8b-89-e6-9d-be/</id>
    <published>2016-10-26T11:43:18.000Z</published>
    <updated>2017-06-03T03:41:23.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="完成一场马拉松的最慢速度"><a href="#完成一场马拉松的最慢速度" class="headerlink" title="完成一场马拉松的最慢速度"></a>完成一场马拉松的最慢速度</h2><p>工作后身边跑马拉松的人突然就多起来了，或许你也蠢蠢欲动，但是一看到半程马拉松有 21 公理，全称马拉松 42 公理，就提前打退堂鼓了。那么你有没有想过</p>
<pre><code>到底要多快我们才能跑完一场 半程/全程 马拉松？
</code></pre><a id="more"></a>
<p>我们来算一算到底需要多快才能才可以跑完一场马拉松，鉴于体力原因，假设我们开始想完成一场半程马拉松，那么我们需要在 3 小时内跑完 21 公理，也就是说每小时需要跑完 7 公理，这样算还是不够直观，我们换一种方式，我们计算每公里平均最长耗时 M</p>
<pre><code>M * 21 公里 = 3 小时
</code></pre><p>这样，我们得到 M 的值为 3 * 60 / 21 约等于  8.57 分钟，即 8 分钟 34 秒。这个值告诉我们平均 8 分钟 34 秒跑完一公里  – 也就是快走的速度 – 以这个速度就能跑完一场半程马拉松比赛。</p>
<h2 id="最慢速度的作用"><a href="#最慢速度的作用" class="headerlink" title="最慢速度的作用"></a>最慢速度的作用</h2><p>我们知道了跑完一场半程马拉松，最慢平均速度是 8 分 34 秒。</p>
<pre><code>那么我们知道这个速度有什么用呢？
</code></pre><p>让我们从心底知道我们能完成这件事，这并不是一件只有少数人才能做的事情，并不需要你在体育方面有超过常人能力，只要你身体健康就行。卡耐基在《人性的优点》里面介绍一个应对恐惧的方法也是类似的：</p>
<pre><code>把你恐惧的事情会导致的所有最坏可能性都一一罗列出来，然后一一检查它们。
</code></pre><p>这个方法的好处是让你知道，就算最坏情况也就这样，让你从无边的恐惧中解放出来。</p>
<h2 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h2><p>如果你自己能够一个人跑完十公里，那么你的体力就能跑完半程马拉松。</p>
<p>跑马拉松是一项群体运动，你会被大家带着跑，但是大家需要找到适合自己的节奏，根据自己的实际情况来确定你能跑完全程的速度。</p>
<p>如果在比赛过程中有任何不适，要量力而行，千万不要硬撑。</p>
<p>最后不建议在雾霾天跑马拉松。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;完成一场马拉松的最慢速度&quot;&gt;&lt;a href=&quot;#完成一场马拉松的最慢速度&quot; class=&quot;headerlink&quot; title=&quot;完成一场马拉松的最慢速度&quot;&gt;&lt;/a&gt;完成一场马拉松的最慢速度&lt;/h2&gt;&lt;p&gt;工作后身边跑马拉松的人突然就多起来了，或许你也蠢蠢欲动，但是一看到半程马拉松有 21 公理，全称马拉松 42 公理，就提前打退堂鼓了。那么你有没有想过&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;到底要多快我们才能跑完一场 半程/全程 马拉松？
&lt;/code&gt;&lt;/pre&gt;
    
    </summary>
    
      <category term="想清楚" scheme="http://yoursite.com/categories/%E6%83%B3%E6%B8%85%E6%A5%9A/"/>
    
    
      <category term="最坏情况" scheme="http://yoursite.com/tags/%E6%9C%80%E5%9D%8F%E6%83%85%E5%86%B5/"/>
    
      <category term="最慢速度" scheme="http://yoursite.com/tags/%E6%9C%80%E6%85%A2%E9%80%9F%E5%BA%A6/"/>
    
      <category term="马拉松" scheme="http://yoursite.com/tags/%E9%A9%AC%E6%8B%89%E6%9D%BE/"/>
    
  </entry>
  
</feed>
