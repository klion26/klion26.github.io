<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>klion26</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2017-12-24T03:11:30.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>klion26</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>millwheel</title>
    <link href="http://yoursite.com/2017/12/22/millwheel/"/>
    <id>http://yoursite.com/2017/12/22/millwheel/</id>
    <published>2017-12-22T13:27:22.000Z</published>
    <updated>2017-12-24T03:11:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文属于 MillWheel 论文的阅读与翻译稿，部分地方使用自己的语言进行描述，部分地方进行翻译（附带原文）<br>讲述 MillWhell 的编程模型以及具体实现。MillWhell 的逻辑时间（logical time）使得开发基于时间的聚合应用更方便，从设计之初，MillWhell 就考虑了容错性以及扩展性。</p>
<a id="more"></a>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>在 Google，流处理系统需要做到 fault tolerance，persistent state 以及 scalability。</p>
<p>MillWhell 是为了流处理，低延迟而量身定做的（tailored specifically to streaming, low-latency systems). </p>
<p>MillWhell 的 API 保证幂等性，从用户角度来看就是保证了恰好一次的语义。<br>Furthermore， the API that MillWheel provides for record processing handles each record in an idempotent fashion, making record delivery occur exactly once from the user’s perspective.</p>
<h2 id="Motivation-and-Requirements"><a href="#Motivation-and-Requirements" class="headerlink" title="Motivation and Requirements"></a>Motivation and Requirements</h2><p>需要实时的分析搜索的高峰和波谷（spike and dip）<br>现在的做法，缓存 1 秒的 batch，然后使用模型预测与真实数据进行比较，如果相差很大，就有大概率出现了高峰/波谷。</p>
<p>存储：高峰/波谷 的持续时间不固定，因此需要保存一定的时间</p>
<p>Low Watermarks：需要区分「数据延迟」 和 「没有数据」，这样也能容忍数据的晚到（真实世界的数据是乱序的）</p>
<p>恰好一次：至少一次（at least once）可能导致波峰的出现</p>
<p>requirements of MillWhell：</p>
<ul>
<li>低延迟（Data should be available to consumers as soon as it is published i.e. there are no system-intrinsic barriers to ingesting inputs and providing output data).</li>
<li>提供易用的状态保存抽象接口（Persistent state abstractions should be available to user code, and should be integrated into the system’s overall consistency model.)</li>
<li>处理乱序（Out-of-order data should be handled gracefully by the system.)</li>
<li>系统提供单调递增的 low watermark（A monotonically increasing low watermark of data timestamps should be computed by the system.)</li>
<li>可扩展性好（Latency should stay constant as the system scales to more than machines.)</li>
<li>恰好一次的语义（The system should provide exactly-once delivery of records)</li>
</ul>
<h2 id="System-Overview"><a href="#System-Overview" class="headerlink" title="System Overview"></a>System Overview</h2><p>MillWheel 是一个基于用户定义逻辑以及输入输出数据的图（MillWheel is a graph of user-defined transformations on input data that produces output data.)<br>这些转换被称为计算，计算需要能够在框架层面做好负载均衡的分布式处理（用户不需要关系）</p>
<p>MillWheel 的输入输出组有 (key, value, timestamp) 三元组组成，其中 <code>key</code> 是系统内部有意义的元数据结构，<code>value</code> 表示真实数据的字节数组，可以是任何的值，<code>timestamp</code> 可以是用户设定的任何值（一般和墙上时间近似）MillWheel 会根据三元组进行 low watermark 的具体计算</p>
<p>用户的计算会形成一套具体的数据流图（a pipeline of user computations will form a data flow graph, as outputs from one computation become inputs for another, and so on.)</p>
<p>用户可以在不重启系统的情况下，动态的添加计算逻辑（Users can add and remove computations from a topology dynamically, without needing to restart the entire system.)</p>
<p>MillWhell 对每条数据提供幂等的处理（MillWheel makes record processing idempotent with regard to the framework API. <strong>As long as applications use the state and communication abstractions provided by the system, failures and retries are hidden from user code.</strong>)</p>
<p>在计算过程中，用户代码可以放到到每个 key 以及每个 computation 对应的存储内容</p>
<h2 id="Core-Concepts"><a href="#Core-Concepts" class="headerlink" title="Core Concepts"></a>Core Concepts</h2><p>MillWheel 提供了流计算的基本元素，同时提供了清晰的抽象（MillWheel surfaces the essential elements of a streaming system, while providing clean abstractions.）</p>
<h3 id="Computations"><a href="#Computations" class="headerlink" title="Computations"></a>Computations</h3><p>用户逻辑定义在 computation 中，提供了用户代码的封装（Application logic lives in computations, which encapsulate arbitrary user code.)</p>
<p>MillWhell 会在接受到数据的时候触发用户逻辑，包括连接外部系统，操作其他的 MillWheel 计算以及进行输出（Computation code is invoked upon receipt of input data, at which point user-defined actions are triggered, including contacting external systems, manipulating other MillWheel primitives, or outputting data.)</p>
<p>需要用户保证外部系统的幂等性（If external systems are contacted, it is up to the user to ensure that the effects of their code on these systems is idempotent.)</p>
<p>Computation 应该针对单个 <code>key</code> 进行，而且不能对 <code>key</code> 在不同机器上的分布有预期（Computation code is written to operate in the context of a single key, and is agnostic to the distribution of keys among different machines.)</p>
<p>保证同一个 key 之间是顺序执行的，不同的 key 可以并行执行。</p>
<h3 id="Keys"><a href="#Keys" class="headerlink" title="Keys"></a>Keys</h3><p>在 MillWheel 中 <code>key</code> 是用于聚合和比较的核心抽象（Keys are the primary abstraction for aggregation and comparison between different records in MillWheel)<br>每条记录会根据用户的规则分配一个具体的 <code>key</code>，computation 的逻辑运行在特定的 <code>key</code> 的空间内，同时也只能访问对应 <code>key</code> 的状态数据</p>
<h3 id="Streams"><a href="#Streams" class="headerlink" title="Streams"></a>Streams</h3><p>Stream 是 MillWheel 系统中任意两个 computation 中间的转换机制（Streams are the delivery mechanism between different computations in MillWheel.) </p>
<p>每个 computation 会订阅 0 个或多个 stream，同时会输出到至少 1 个 stream，不同 computation 间的数据流转由系统进行保证（A computation subscribes to zero or more input streams and publishes one or more output streams, and the system guarantees delivery along theses channels)</p>
<h3 id="Persistent-State"><a href="#Persistent-State" class="headerlink" title="Persistent State"></a>Persistent State</h3><p>在 MillWheel 系统中，状态是一个不透明的字节数组，由用户提供序列化和反序列化的逻辑。状态数据的存储系统有 BigTable/Spanner 提供。</p>
<h3 id="Low-Watermarks"><a href="#Low-Watermarks" class="headerlink" title="Low Watermarks"></a>Low Watermarks</h3><p>low watermark 提供了未来数据最小时间戳的界限（The low watermark for a computation provides a bound on the timestamp of future records arriving at that computation)</p>
<p><strong>定义</strong>: low watermark 的定义通过数据流水线递归进行给出（We provide a recursive definition of low watermarks based on a pipeline’s data flow)，给定一个 computation，A，它最老的工作由正在做的工作中，timestamp 最小的工作给出（Given a computation, A, let the oldest work of A be a timestamp corresponding to the oldest unfinished (in-flight, stored, or pending-delivery) record in A.). 在这个定义下 A 的 low watermark 可以写成 <code>min(oldest work of A, low watermark of C: C outputs to A)</code>，也就是所有上游的 low watermark 以及自己的最老（不一定是最早开始的）工作的时间</p>
<p>low watermark 由外部系统进行打点（Low watermark values are seeded by injector, which send data into MillWheel from external systems)</p>
<p>在真实世界中，对外部系统的延迟往往是一个估计值，因此需要能够容忍一小部分落后与 low watermark 的数据</p>
<p>用户可以假定自己获取的数据是全量的（这个假定对 low watermark 很重要）（By waiting for the low watermark of a computation to advance past a certain value, the user can determine that they have a complete picture of their data up to that time)</p>
<p>用户可以给每条记录赋予一个不小于输入源数据时间戳的任意值（When assigning timestamps to new or aggregate records, it is up to the user to pick a timestamp no smaller than any of the source records.)</p>
<h3 id="Timers"><a href="#Timers" class="headerlink" title="Timers"></a>Timers</h3><p>Timers 是一段针对每个 <code>key</code> 的定时触发（基于墙上时间或者 low watermark）的回调逻辑（Timers are per-key programmatic hooks that trigger at a specific wall time or low watermark value.)</p>
<p>Timer 一旦设定，将由框架保证按时间戳递增的触发（Once set, timers are guaranteed to fire in increasing timestamp order)</p>
<p>Timer 会有记录日志以及相应状态保存，同样提供恰好一次的语义（They – Timers – are journaled in persistent state and can survive process restarts and machine failures. When a timer fires, it runs the specified user function and has the same exactly-once guarantee as input records)</p>
<h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">class Computation &#123;</div><div class="line">	//Hooks called by the system</div><div class="line">	void ProcessRecord(Record data);</div><div class="line">	void ProcessTimer(Timer timer);</div><div class="line"></div><div class="line">	//Accessors for other abstractions.</div><div class="line">	void SetTimer(String tag, int64 time);</div><div class="line">	void ProduceRecord(Record data, String stream);</div><div class="line">	StateType MutablePersistentState();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>用户通过重载 <code>ProcessRecord</code> 和 <code>ProcessTimer</code> 来做具体的工作</p>
<h3 id="Computation-API"><a href="#Computation-API" class="headerlink" title="Computation API"></a>Computation API</h3><p>两个用户逻辑的入口是 <code>ProcessRecord</code> 和 <code>ProcessTimer</code>（The two main entry point into user code are provided by the ProcessRecord and ProcessTimer hooks) </p>
<p>系统接受到外部 RPC 之后（输入）会调用用户逻辑，用户逻辑可以访问状态数据，然后系统将输出传给下游（The MillWheel system invokes user-defined processing hooks in response to incoming RPCs. User code accesses state, timers, and productions through the framework API. The framework performs any actual RPCs and state modifications)</p>
<h3 id="Injector-and-Low-Watermark-API"><a href="#Injector-and-Low-Watermark-API" class="headerlink" title="Injector and Low Watermark API"></a>Injector and Low Watermark API</h3><p>系统会为每个 Computation 中待处理的任务计算一个 low watermark（At the system layer, each computation calculates a low watermark value for all of its pending work(in-progress and queued deliveries).</p>
<p>所有的状态也都可以被赋予一个时间戳（Persistent state can also be assigned a timestamp value(e.g. the trailing edge of an aggregation window.) 这都由系统自动建立，从而为用户提供透明的 timer 语义 – 用户一般不需要和底层的 low watermark 进行交互，而是通过时间戳进行操作 low watermark（This is rolled up automatically by the system in order to provide API semantics for timers in a transparent way – users rarely interact with low watermarks in computation code, but rather manipulate them indirectly through timestam assignation to records.)</p>
<p><strong>Injectors</strong>: Injectors 从外部系统读入数据到 MillWheel（Injectors bring external data into MillWheel)<br>Injector 负责最初的 low watermark 生成，因此可以间隙性的往下游发送传递的 low watermark 表示当前传递的情况（Since injectors seed low watermark values for the rest of the pipeline, they are able to publish an <em>injector low watermark</em> that propagates to any subscribers among their output streams, reflecting their potential deliveries along those stream.</p>
<p>一个 Injector 可能会由多个进程处理，因此 injector 的 low watermark 由所有这些进程共同决定（这些进程中最小的 low watermark）（An injector can be distributed across multiple processes, such that the aggregate low watermark of these processes is used as the injector low watermark.)</p>
<p>用户可以自定义一组预期的输出进程，用于度量系统的健壮性以及网络的延迟情况（The user can specify an expected set of injector processes, making this metric robust against process failures and network outages.)</p>
<h2 id="Fault-tolerance"><a href="#Fault-tolerance" class="headerlink" title="Fault tolerance"></a>Fault tolerance</h2><h3 id="Delivery-Guarantees"><a href="#Delivery-Guarantees" class="headerlink" title="Delivery Guarantees"></a>Delivery Guarantees</h3><p>MillWheel 编程模型的简洁性通过由框架提供幂等操作，而不需要用户自己实现幂等操作来完成。这可以大大减少用户的编程复负担。（Much of the conceptual simplicity of MillWheel’s programming model hinges upon its ability to take non-idempotent user code and run it as if it were idempotent. By removing this requirement from computation authors, we relieve them of a significant implementation burden.)</p>
<h4 id="恰好一次（Exactly-Once-Delivery）"><a href="#恰好一次（Exactly-Once-Delivery）" class="headerlink" title="恰好一次（Exactly-Once Delivery）"></a>恰好一次（Exactly-Once Delivery）</h4><p>每个 computation 接受到数据后的处理步骤如下：</p>
<ul>
<li>数据会和上次的去重数据进行比较，如果有重复则跳过（The record is checked against deduplication data from previous deliveries; duplicates are discarded.)</li>
<li>执行用户代码，生成针对 timer，state，production 的变化（User code is run for the input record, possibly resulting in pending changes to timers state, and productions.)</li>
<li>将上一步的变化保存到后端存储（pending changes are committed to the backing store)</li>
<li>ACK 上游(Senders are ACKed）</li>
<li>发送给下游（Pending downstream productions are sent）</li>
</ul>
<p>作为优化，可能某个 checkpoint 点会针对多条记录进行（As an optimization, the above operations may be coalesced into a single checkpoint for multiple records.)</p>
<p>MillWheel 中的数据在被 ACK 之前会一直重发，从而保证 At-Least-Once，这是 Exactly-Once 的前提（Deliveries in MillWheel are retried until they are ACKed in order to meet our at-least-once requirement, which is a prerequisite for exactly-once.)</p>
<p>这就引入了如下一个问题，如果下游在正常更新状态后，ACK 前退出了，就会接受到两条同样的数据，因此需要对数据进行去重（However, this introduces the case where a receiver may crash before it has a chance to ACK the input record, even if it has persisted the state corresponding to successful processing of that record. In this case, we must prevent duplicate processing when the sender retries its delivery.)</p>
<p>MillWheel 给每条产生的数据赋予一个全局唯一的 ID，利用原子操作的状态更新包含该 ID 来进行去重操作（The system assigns unique ID to all records at production time. We identify duplicate records by including this unique ID for the record in the same atomic write as the state modification.) </p>
<p>如果接受到已经成功处理的消息，系统比较后直接丢弃并 ACK（防止无休止的重试）（If the same record is later retried, we can compare it to the journaled ID, and discard and ACK the duplicate (lest it continue to retry indefinitely)</p>
<p>由于内存有限，不会将所有处理过的数据保存在内存，因此保存一个 Bloomfilter 保存我们见过的数据，如果 Bloomfiler 返回结果为空，则需要读取后端存储检查是否为重复数据。Record ID 会在确保所有发送方都已经重试完成后进行回收（Since we cannot necessarily store all duplication data in-memory, we maintain a Bloom filter of known record fingerprints, to provide a fast path for records that we must read the backing store to determine whether a record is a duplicate. Record IDs for past deliveries are garbage collected after MillWheel can guarantee that all internal senders have finished retrying.) 对于 Injector，系统会延迟回收 ID（For injectors that frequently deliver late data, we delay this garbage collection by a corresponding slack value (typically on the order of a few hours).</p>
<h4 id="Strong-productions"><a href="#Strong-productions" class="headerlink" title="Strong productions"></a>Strong productions</h4><p>由于 MillWheel 处理的数据可能是乱序以及不确定性的，因此会在数据产生后，发往下游以及更新状态数据前进行 checkpoint（Since MillWheel handles inputs that are not necessarily ordered or deterministic, we checkpoint produced records before delivery in the same atomic write as state modification.)</p>
<p>这种在记录产生之前进行 checkpoint 的模式被称为 strong production(We call this pattern of checkpointing before record production strong productions.)</p>
<p>如果没有 checkpoint 的话，可能导致同一个逻辑窗口的数据不一致（Without a checkpoint, it would be possible for that computation to produce a window count downstream, but crash before saving its state. Once the computation came back up, it might receive another record (and add it to the count) before producing the same aggregate, creating a record that was bit-wise distinct from its predecessor but corresponded to the same logical window.)</p>
<p>MillWheel 使用 Bigtable 作为存储系统，Bigtable 对用户屏蔽了内部细节（相对于 读-修改-写 的操作），使得 checkpoint 就像 log 的行为一样。当一个进程启动的时候，checkpoint 会被重新读入内存并重放，当处理完成后 checkpoint 会被删除。（We use a storage system such as Bigtable, which efficiently implements blind writes (as opposed to read-modify-write operations), making checkpoints mimic the behavior of a log. When a process restarts, the checkpoints are scanned into memory and replayed. Checkpoint data is deleted once these productions are successful.)</p>
<h4 id="6-1-3-Weak-Productions-and-Idempotency"><a href="#6-1-3-Weak-Productions-and-Idempotency" class="headerlink" title="6.1.3 Weak Productions and Idempotency"></a>6.1.3 Weak Productions and Idempotency</h4><p>结合 Strong productions 以及 exactly-once 的消息传递性可以保证很多计算都是幂等的，包括系统层面的重试（Taken together, the combination of strong productions and exactly-once delivery makes many computations idempotent with regard to system-level retries.)</p>
<p>有些计算就算没有这些保证就已经是幂等的了（However, some computations may already be idempotent, regardless of the presence of these guarantees (which come with a resource and latency cost).) </p>
<p>因此 Strong productions 以及/或者 exactly-once 可以从用户层面取消（Depending on the semantic needs of an application, strong productions and/or exactly-once can be disabled by the user at their discretion.)</p>
<p>对于 <em>weak productions</em>, 我们不会在发送数据前进行 checkpoint，而是在保存状态前乐观的向下游广播数据，（For <em>weak production</em>, rather than checkpointing record productions before delivery, we broadcast downstream deliveries optimistically, prior to persisting state). 从经验得到，这会引入一个新的问题：整个流水线的完成时间会严重耦合，且依赖下游的 ACK（Empirically, this introduces a new problem, in that the completion times of consecutive stages of the pipeline are now strictly coupled as they wait for downstream ACKs of records.) 结合机器的故障率，端到端的延迟会随着流水线的深度增加而增加（Combined with the possibility of machine failure, this can greatly increase end-to-end latency for straggler productions as pipeline depth increases.) 例如，我们假设每台机器发生故障的概率为 1%，那么我们遇到故障的概率会随着流水线的深度递增而递增 – 对于深度为 5 的流水线，我们有 5% 的概率遇到至少一次机器故障（For example, if we assume (rather pessimistically) that there is a 1% chance that any machine will fail during a given minute, then the probability that we will be waiting on at least one failure increases disastrously with pipeline depth – for a pipeline of depth 5, a given production could have nearly a 5% chance of experiencing a failure every minute!）我们通过对部分延迟的数据流进行 checkpoint 来解决这一问题，允许这些接受者对他们的上游进行 ACK。通过这种选择性的 checkpoint 方式，我们既可以降低端到端的延迟，也可以减少所有资源的使用（We ameliorate this by checkpointing a small percentage of straggler pending productions, allowing those stages to ACK their senders. By selectively checkpoint in this way, we can both improve end-to-end latency and reduce overall resource consumption.)在 Figure 11 中，我们描述了这种 checkpoint 的具体实现。（In Figure 11, we show this checkpointing mechanism in action.) Computation A 的下游是 Computation B，而 Computation B 则马上将数据发送给下游 Computation C（Computation A produces to Computation B, which immediately produces to Computation C.) 然而 Computation C 的 ACK 则很慢，所以 Computation B 会在 1 秒的延迟之后进行 checkpointing。这样之后，Computation B 能够对上游 Computation A 进行 ACK，允许 Computation A 释放所占用的资源（However, Computation C is slow to ACK, so Computation B checkpoints the production after 1-second delay. Thus, Computation B can ACK the delivery from Computation A, allowing A to free any resources associated with the production.) 就算 Computation B 在后续阶段重启了，也能够从之前 checkpoint 的地方进行恢复，然后重新发送数据给下游 Computation C，而且没有数据丢失。（Even when Computation B subsequently restarts, it is able to recover the record from the checkpoint and retry delivery to Computation C, with no data loss.)</p>
<img src="/2017/12/22/millwheel/figure11_checkpoint.jpg" alt="figure11_checkpoint.jpg" title="">
<p>上面这种松散模式更适合具有幂等性的流水线，因为这样重试不会影响正确性，下游也是可以进行重试的。现实世界中一个幂等的例子就是无状态的过滤，任何时候输入相同的数据，会产生同样的输出。（The above relaxations would be appropriate in the case of a pipeline with idempotent computations, since retries would not affect correctness, and downstream production would also be retry-agnostic. A real-world example of an idempotent computation is a stateless filter, where repeated deliveries along input streams will not change the result.)</p>
<h3 id="6-2-State-Manipulation-状态操作"><a href="#6-2-State-Manipulation-状态操作" class="headerlink" title="6.2 State Manipulation/状态操作"></a>6.2 State Manipulation/状态操作</h3><p>关于 MillWheel 的实现机制中如何操作状态，我们讨论了保存到后端存储的“hard” 状态，也讨论了保存在内存的“软”状态。我们需要满足如下几条对用户的保证：（In implementing mechanism to manipulate user state in MillWheel, we discuss both the “hard” state and that is persisted to our backing store and the “soft” state which includes any in-memory caches or aggregates. We must satisfy the following user-visible guarantess:)</p>
<ul>
<li>系统不会丢失数据（The system does not loss data)</li>
<li>更新状态保证恰好一次的语义（Updates to state must obey exactly-once semantics)</li>
<li>系统保存的数据在任何时候都应该是一致的（All persisted data throughout the system must be consistent at any given point in time)</li>
<li>Low watermarks 必须能够反应系统中排队的请求（Low watermarks must reflect all pending state in the system.)</li>
<li>对于特定的 key，它相关的定时器必须按序触发（Timers must fire in-order for a given key.)</li>
</ul>
<p>为了避免保存的状态数据不一致（比如触发器，用户状态，以及生产 checkpoints 之间），我们将每个 key 对应的状态数据更新封转为一个原子操作。（To avoid inconsistencies in persisted state (e.g. between timers, user state, and production checkpoints), we wrap all per-key updates in a single atomic operation.)这样可以适应系统任何时候的失败和中断（This results in resiliency against process failures and other unpredictable events that may interrupt the process at any given time.) 正如前面所说的，同一个操作中保证恰好一次的数据更新，并且对每个 key 的一致性状态数据提供保证（As mentioned previously, exactly-once data is updated in this same operation, adding it to the per-key consistency envelope)</p>
<p>由于计算会在机器之间迁移（负载均衡，故障或者其他原因），影响数据一致性的最大原因变为僵尸写入进程以及残留网络写入后端存储的概率（As work may shift between machines (due to load balancing, failures, or other reasons) a major threat to our data consistency is the possibility of zombie writers and network remnants issuing stale writes to our backing store.)。 为了解决这个问题，我们给每个 writer 附上一个序列化 token，后端存储会跟进这个 token 来验证写入是否合理。（To address this possibility, we attach a sequencer token to each write, which the mediator of the backing store checks for validity before allowing the write to commit.) 每个新工作进程首先将之前的工作进程设置为失效，这样就不会受之前进程的影响了。（New workers invalidate any extant sequencers before starting work, so that no remnant writes can succeed thereafter.) 这个序列号作为一个租约执行机制，就像 Centrifuge 系统一样。（The sequencer is functioning as a lease enforcement mechanism, in a similar manner to the Centrifuge system.) 因此，我们能够保证同一时刻点，对于特定的 key 只会有一个进程在进行写入。（Thus, we can guarantee that, for a given key, only a single worker can write to that key at a particular point in time.)</p>
<p>这种单写入进程的保证对软状态的维护同样重要，而且它不能通过事务进行处理（This single-writer guarantee is also critical to the maintenance of soft state, and it cannot be guaranteed by depending on transactions.) 以缓存写入缓慢为例：如果在构建缓存之后，来自另外一个进程的僵尸进程还在写入，则会导致缓存的不一致。（Take the case of a cache of pending timers: if a remnant write from another process could alter the persisted timer state after said cache was built, the cache would be inconsistent.)图 12 描述了这种情况，图中僵尸进程 B 由于外部因素，写入后端存储的事务没有按时到达后端。在事务实际开始前，B 的后继者，B-prime，执行定时器的扫描。在扫描完成之后，事务完成且 A 被 ACK，这样 B-prime 就拥有一个不完整的计时器状态。（This situation is illustrated by Figure 12, where a zombie process (B) issues a transaction that is delayed on the wire, in response to a production from A. Before the transaction begins, B’s successor, B-prime, performs its initial scan of pending timers. After this scan completes, the transaction is applied and A is ACKed, leaving B-prime with incomplete timer state.) 丢失的定时器可能被无限期的孤立，造成相应的输出延迟，这对于延迟敏感的系统是不能接受的（The lost timer could be orphaned indefinitely, delaying any of its output actions by an arbitrary amount of time, Clearly, this is unacceptable for a latency-sensitive system.)<br><img src="/2017/12/22/millwheel/figure12_transaction.jpg" alt="figure12_transaction.jpg" title=""></p>
<p>此外，检测点也会遇到同样的情况，通过避免对后端存储的初始化扫描，系统的状态仍然是不可知的。（Furthermore, this same situation could occur with a checkpointed production, where it would remain unknown to the system by eluding an initial scan of the backing store.) 这种生产值直到产生 watermark 的时候才会被考虑到，而且这段时间内，我们可能向消费者报告一个错误的 watermark 值。另外，watermark 是单调递增的，所以我们无法对错误的 watermark 值进行纠正。违反 low watermark 的保证，则可能导致结果出错，其中包括提前触发定时器和产生不完整的窗口等。（This production would then note be accounted for in the low watermark until it was discovered, and in the intervening time, we might be reporting  an erroneous low watermark value to consumers. Furthermore, since our low watermarks are monotonically increasing, we are unable to correct an erroneous advancement in the value. By violating our low watermark guarantees, a variety of correctness violations could occur, including premature timer firings and incomplete window productions.)</p>
<p>为了能够快速的从故障中进行恢复，MillWheel 中的每个算子可以在任何时刻进行任何粒度状态的 checkpoint（实践中，跟进数据量的不同，一般会有亚秒级别的，或者记录级别的 checkpoint）（In order to quickly recover from unplanned process failures, each computation worker in MillWheel can checkpoint its state at an arbitrarily fine granularity (in practice, sub-second or per-record granularity is standard, depending on input volume). 我们使用始终一致的软状态运行我们在特定的情况 – 机器宕机或者负载不均衡 – 进行最少的 checkpoint 数据扫描。在扫描 checkpoint 数据的时候，通常是异步的，这样运行在扫描 checkpoint 的时候同事也提供计算。（Our use of always-consistent soft state allows us to minimize the number of occasions when we must scan these checkpoints to specific cases – machine failures or load-balancing events. When we do perform scans, these can often be asynchronous, allowing the computation to continue processing input records while the scan progresses.)</p>
<h2 id="7-System-implementation"><a href="#7-System-implementation" class="headerlink" title="7 System implementation"></a>7 System implementation</h2><h3 id="7-1-Architecture"><a href="#7-1-Architecture" class="headerlink" title="7.1 Architecture"></a>7.1 Architecture</h3><p>MillWheel 部署在一组动态主机服务器上作为分布式系统提供服务。流水线中的每个计算会运行在一台或多态机器上，不同机器上的流通过 RPC 进行传递。在每台机器上，MillWheel 会管理输入以及进程级别的元数据，并且根据需要委托给相应的用户计算。（MillWheel deployments run as distributed systems on a dynamic set of host servers. Each computation in a pipeline runs on one or more machines, and streams are delivered via RPC. On each machine, the MillWheel system marshals incoming work and manages process-level metadata, delegating to the appropriate user computation as necessary.)</p>
<p>负载分配和均衡由一个复制的主机进行处理，它将每个计算分成一组有用不同密钥的单元（这些密钥会涵盖所有的可能性），然后将它们分发到一批机器上。（Load distribution and balancing is handled by a replicated master, which divides each computation into a set of owned lexicographic key intervals (collectively covering all key possibilities) and assigns these intervals to a set of machines.) 对于 CPU 压力过大或内存压力过大的情况（通过标准的进程监控得到），可以将这些单元进行移动，分割或者合并它们。每个单元被分配了一个唯一的序列号，当这个单元被移动，分割或者合并之后，这个序列号就失效了。这个序列号的重要性已经在 6.2 中进行了讨论。（In response to increased CPU load or memory pressure (reported by a standard perprocess monitor), it can move these intervals around, split them, or merge them. Each interval is assigned a unique sequencer, which is invalidated whenever the interval is moved, split, or merged. The importance of this sequencer was discussed in Section 6.2)</p>
<p>对于状态的存储，MillWheel 使用类似 Bigtable 或者 Spanner 类似的系统，提供原子性的，行级更新。同一个 key 的定时器，延迟的生产，以及保存的状态保存在同一行（For persistent state, MillWheel uses a database like Bigtable or Spanner, which provides atomic, single-row updates. Timers, pending productions, and persistent state for a given key are all stored in the same row in the data store.)</p>
<p>当一个 key 的一个最小单元由于机器故障导致被重新分配到其他机器上上，MillWheel 能投通过从后端存储中读取元数据快速的进行恢复。最初会在内存建立相应的堆式数据结构，用于保存堆积的定时器以及 checkpoint 过的输出，这些数据在整个 key 的生命周期中和后端存储中保持一致。为了保证这个特性，我们保证了单实例写入的语义 – 6.2 节中有详细描述。（MillWheel recovers from machine failures efficiently by scanning metadata from this backing store whenever a a key interval is assigned to a new owner. This initial scan populates in-memory structures like the heap of pending timers and the queue of checkpointed productions, which are then assumed to be consistent with the backing store for the lifetime of the interval assignment. To support this assumption, we enforce single-writer semantics(per-computation worker) that are detailed in Section 6.2)</p>
<h3 id="7-2-Low-Watermarks"><a href="#7-2-Low-Watermarks" class="headerlink" title="7.2 Low Watermarks"></a>7.2 Low Watermarks</h3><p>为了保证数据的一致性，low watermark 必须被实现为一个子系统，而且能够全局能够访问且持续是正确的。我们将这实现为一个 central authority（类似 OOP），这个系统跟踪系统的所有 low watermark 且将他们记录到状态存储系统中，用以防止由于系统失败导致得到错误的 low watermark。（In order to ensure data consistency, low watermark must be implemented as a sub-system that is globally available and correct. We h have implemented this as a central authority (similar to OOP), which tracks all low watermark values in the system and journals them to persistent state, preventing the reporting of erroneous values in cases of process failure.)</p>
<p>当向 central authority 汇报数据的时候，每个进程会报自己工作的时间戳信息也带上。这包括已经 checkpoint 或者堆积的 production，堆积的定时器或者需要保存的状态信息。每个进程的效率依赖与我们内存中的保持一致性的数据结构，不需要去查询耗时更严重的后端存储。因为每个进程被分配给了每个 key 的一个最小单元，因此 low watermark 的更新也是以 key 的最小单元为单位而向 central authority 进行汇报。（When reporting to the central authority, each process aggregates timestamp information for all of its owned work. This includes any checkpointed or pending productions, as well as any pending timers or persisted state. Each process is able to do this efficiently by depending on the consistency of our in-memory data structures, eliminating the need to perform any expensive queries over the backing data store. Since processes are assigned work based on key intervals, low watermark updates are also bucketed into key intervals, and sent to the central authority.)</p>
<p>为了计算整个系统的 low watermark，这个 authority 必须能否访问所有堆积后以及处理过的工作的 low watermark 信息。当聚合每个进程的更新操作时，同时跟踪每个进程中的计算对应的区间所对应的 low watermark 值。如果某些 key 的最小单元丢失了，则丢失的单元的 low watermark 会被标记为上次得到的值，知道该单元的计算重新恢复为止。然后 authority 会在整个系统中对 low watermark 进行广播。（To accurately compute system low watermarks, this authority must have access to low watermark information for all pending and persisted work in the system. When aggregating per-process updates, it tracks the completeness of its information for each computation by building an interval map of low watermark values for the computation. If any interval is missing, then the low watermark corresponds to the last known value for the missing interval until it reports a new value. The authority then broadcasts low watermark values for all computations in the system.)</p>
<p>下游通过订阅它感兴趣的上游计算的 low watermark 值，从而计算它的整个输入的 low watermark 值。这个值由工作进程进行计算，而不是 authority 进行计算，与下面这个理由是一致的：central authority 的 low watermark 值应该不大于工作进程的 low watermark。通过有工作进行进行计算，central authority 的 low watermark 值就不会大于工作进程的 low watermark 值了，从而保证了这个属性。（Interested consumer computations subscribe to low watermark values for each of their sender computations, and thus compute the low watermark of their input as the minimum over these values. The reason that these minima are computed by the workers, rather than the central authority, is one of consistency: the central authority’s low watermark values should always be at least as conservative as those of the workers. Accordingly, by having workers compute the minima of their respective inputs, the authority’s low watermark never leads the workers’, and this property is preserved.)</p>
<p>为了能够在 central authority 保证一致性，我们给每个 low watermark 的更新附带上一个序列号。类似之前提到的单进程写入模式下更新本地 key 最小单元的状态，这个序列号保证只有最后的属主能够成功更新该 key 最小单元的 low watermark。为了扩展能力，authority 能够分布在多态机器上，每台 worker 机器上有一个或多个计算。经验上，这个能够在不损失性能的前提下扩展到 500000 个 key 最小单元。（To maintain consistency at the central authority, we attach sequencers to all low watermark updates. In a similar manner to our single-writer scheme for local updates to key interval state, these sequences ensure that only the latest owner of a given key interval can update its low watermark value. For scalability, the authority can be sharded across multiple machines, with one or more computations on each worker. Empirically, this can scale to 500000 key intervals with no loss in performance.)</p>
<p>在该系统中，我们还可以选择去除异常值，并为对速度特别要求的流水线提供启发式的 low watermark。比如，我们可以通过 99% 的记录时间戳得到一个 99% 的 low watermark。只对近似结果感兴趣的窗口消费者，可以利用这些 low watermark，从而避免等待那些晚到的数据。（Given a global summary of work in the system, we are able to optionally strip away outliers and offer heuristic low watermark values for pipelines that are more interested in speed than accuracy. For example, we can compute a 99% low watermark that corresponds to the progress of 99% of the record timestamps in the system. A windowing consumer that is only interested in approximate results could then use these low watermark values to operate with lower latency, having eliminated its need to wait on stragglers.)</p>
<p>总之，在我们的实现中，low watermark 对系统中的流没有时间顺序上的要求。low watermark 会反应正在进行的和已经保存的状态，通过建立一个全局的 low watermark 值来源，我们从逻辑上防止了不一致性，类似 low watermark 的回退。（In summary, our implementation of low watermarks does not require any sort of strict time ordering on streams in the system. Low watermarks reflect both in-flight and persisted state. By establishing a global source of truth for low watermark values, we prevent logical inconsistencies, like low watermarks moving backwards.)</p>
<h2 id="8-Evaluation"><a href="#8-Evaluation" class="headerlink" title="8 Evaluation"></a>8 Evaluation</h2><p>为了说明 MillWheel 的性能，我们提供了针对流处理系统关键指标的试验结果</p>
<h3 id="8-1-output-latency"><a href="#8-1-output-latency" class="headerlink" title="8.1 output latency"></a>8.1 output latency</h3><p>流处理系统中，延迟是对于性能的一个关键指标。MillWheel 系统支持低延迟的结果，并且随着系统的扩展而不增加延迟。我们使用了一个单 Stage 的消息传递，计算内容为排序的列子来进行说明 MillWheel 的性能。这类似于在连续计算中发生的多对多的 shuffle，是 MillWheel 中排序的会遇到的一个最差场景。Figure 13 展示了运行在 200 CPU 上的结果。消息传递的中位数延迟为 3.6 毫秒，95% 的延迟为 30 毫秒，这能够很好的满足 Google 内部对流系统的需求（95 线甚至在人类可能反应的时间内）（This resembles the many-to-many shuffle that occurs between successive computations that are keyed differently, and thus is a worst case of sorts for record delivery in MillWheel. Figure 13 shows the latency distribution for records when running over 200 CPUs. Median record delay is 3.6 milliseconds and 95th-percentile latency is 30 milliseconds, which easily fulfills the requirements for many streaming systems at Google (even 95th percentile is within human reaction time).<br><img src="/2017/12/22/millwheel/figure13_latency.jpg" alt="figure13_latency.jpg" title=""></p>
<p>这个测试是在关闭了 strong production 以及恰好一次的情况下进行的。当开启这两个特性的时候，延迟中位数变为 33.7 毫秒，95 线变为 93.8 毫秒。这个对比可以说明，计算本身的幂等性可以通过关闭这两个特性而大大降低延迟。（This test was performed with strong productions and exactly-once disabled. With both of these features enabled, median latency jumps up to 33.7 milliseconds and 95th-percentile latency to 93.8 milliseconds. This is a succinct demonstration of how idempotent computations can decrease their latency by disabling these two features.)</p>
<p>为了能够验证 MillWheel 的延迟不会随着系统的扩展而增加，我们将之前的单 Stage 实验跑在不同的 CPU 配置上，从 20 到 2000. Figure 14 展示了延迟的中位数基本保持不变。延迟的 99 线却会变糟（虽然都还在 100 毫秒以内）。然而，最差的延迟情况预计会随着规模的增加而增加 – 更多的资源意味着更多的出错可能。（To verify that MillWheel’s latency profile scales well with the system’s resource footprint, we ran the single-stage latency experiment with setups ranging in size from 20 CPUs to 2000 CPUs, scaling input proportionally. Figure 14 shows that median latency stays roughly constant, regardless of system size. 99th-percentile latency does get significantly worse (though still on the order of 100ms). However, tail latency is expected to degrade with scale – more machines mean that there are more opportunities for things to go wrong.)<br><img src="/2017/12/22/millwheel/figure14_latency_scalability.jpg" alt="figure14_latency_scalability.jpg" title=""></p>
<h3 id="8-2-Watermark-Lag"><a href="#8-2-Watermark-Lag" class="headerlink" title="8.2 Watermark Lag"></a>8.2 Watermark Lag</h3><p>虽然某些计算（比如 Zeitgeist 中的波峰波谷检测）不需要定时器，但是其他许多计算（比如 dip detection）使用 low watermark 来触发定时器进行输出。对于这些计算，low watermark 的延迟将会导致结果的不新鲜。由于 low watermark 从计算图的源头开始进行产生，我们期望 low watermark 的延迟与流水线上计算与输入端的距离成正比。我们使用 200 CPU 运行了一个 3 级 MillWheel 流水线，并且每秒钟计算一次每个计算的 low watermark 值。图 15 中，我们可以看到，第一阶段的 low watermark 延迟了 1.8 秒，但是，对于后续的阶段，每阶段的延迟增加了不到 200 毫秒。怎么减少 watermark 的延迟现在还是一个正在发展中的领域。（While some computations (like spike detection in Zeitgeist) do not need times, many computations (like dip detection) use timers to wait for the low watermark to advance before outputting aggregates. For these computations, the low watermark’s lag behind real time bounds the freshness of these aggregates. Since the low watermark propagates from injectors through the computation graph, we expect the lag of a computation’s low watermark to be proportional to its maximum pipeline distance from an injector. We ran a simple three-stage MillWheel pipeline on 200 CPUs, and polled each computation’s low watermark value once per second. In Figure 15, we can see that the first stage’s watermark lagged real time by 1.8 seconds, however, for subsequent stages, the lag increased per stage by less than 200ms. Reducing watermark lag is an active area of development.<br><img src="/2017/12/22/millwheel/figure15_lowwatermark.jpg" alt="figure15_lowwatermark.jpg" title=""></p>
<h3 id="Framework-Level-Caching"><a href="#Framework-Level-Caching" class="headerlink" title="Framework-Level Caching"></a>Framework-Level Caching</h3><p>由于 checkpoint 的频率很好，MillWheel 会产生非常多和存储层交互的流量。当使用 Bigtable 类似的系统时，读取的成本要高于写入，而 MillWheel 通过框架层面的缓存来解决这一问题。MillWheel 的一个常见用例是将数据缓存到存储引擎中，直到 low watermark 通过了窗口边界然后再获取这些数据进行聚合。这种使用模式对于存储系统中常见的 LRU 缓存是非常不利的，刚修改过的数据很可能并不是马上就需要读取的数据。MillWheel 知道它的数据怎么被使用，并且能够提供更好的缓存策略。在图 16 中，我们给出了工作进程和存储进程使用的 CPU 数与缓存大小的关系（出于商业机密考虑，CPU 使用率进行了标准化），增加缓存大小可以线性的增加 CPU 使用率（超过 550MB 的缓存，由于大部分数据已经被缓存，所有基本没有更大的作用）。在这个实验中，MillWheel 的缓存可以将 CPU 的使用率降低为原来的二分之一。<br><img src="/2017/12/22/millwheel/figure16_cache.jpg" alt="figure16_cache.jpg" title=""></p>
<h3 id="8-4-Real-world-Deployments"><a href="#8-4-Real-world-Deployments" class="headerlink" title="8.4 Real-world Deployments"></a>8.4 Real-world Deployments</h3><p>MillWheel 支持了 Google 内部的各种系统。为各种广告客户执行流连接，其中许多需要给客户展示低延迟的数据大盘。计费系统依赖于 MillWheel 的恰好一次保证。除了 Zeitgeist 之外，MillWheel 还提供一个广泛的异常检测服务，该服务被很多不同的团队看作开箱即用的服务使用。其他部署还包括了网络交换机以及集群运行状态监控。MillWheel 还支持了面向用户的工具，比如 Google 街景中的全景图像生成和处理。（MillWheel powers a diverse set of internal Google systems. It performs streaming joins for a variety of Ads customers, many of whom require low latency updates to customer-visible dashboards. Billing pipelines depend on MillWheel’s exactly-once guarantees. Beyond Zeitgeist, MillWheel powers a generalized anomaly-detection service that is used as a turnkey solution by many different teams. Other deployments include network switch and cluster health monitoring. MillWheel also powers user-facing tools like image panorama generation and image processing for Google Street View.)</p>
<p>同样还有 MillWheel 不适用的常见。单个操作太大导致在计算过程中对 checkpoint 不友好的情况不适合，因为系统的稳定依赖于动态负载均衡。如果负载均衡器遇到这样的算子，要么只能强制结束，然后调度到其他机器，或者等待它完成。前者浪费资源，后者则使机器过载。作为一个分布式系统，MillWheel 对于不同 key 之间不能够并行的场景也处理不好。如果整个流水线上 90% 的流量都和某个特定的 key 绑定在一起，那么某台机器必须能够处理该流水线的 90% 流量，这显然是不可取的。建议作业的开发人员避免会造成单台机器会处理很高流量的  key（比如使用用户使用的语言或者用于代理等），或者通过构建两级聚合来处理相应的事情。（There are problems that MillWheel is poorly suited for. Monolithic operations that are inherently resistant to checkpointing are poor candidates for inclusion in computation code, since the system’s stability depends on dynamic load balancing. If the load balancer encounters a host spot that coincides with such an operation, it must choose to either interrupt the operation, forcing it to restart, or wait until finishes. The former wastes resources, and the latter risks overloading a machines. As a distributed system, MillWheel does not perform well on problems that are not easily parallelized between different keys. If 90% of a pipeline’s traffic is assigned a single key, then one machine must handle 90% of the overall system load for that stream, which is clearly inadvisable. Computation authors are advised to avoid keys that are high-traffic enough to bottleneck on a single machine (such as a customer’s language or user-agent string), or build a two-phase aggregator.)</p>
<p>如果计算基于 low watermark 进行聚合，数据的延迟导致 low watermark 一直不能正常更新，MillWheel 的性能就会下降。这可能会导致系统中缓冲的数据产生数小时的偏差。通常情况下，内存使用量和数据倾斜成正比，因为作业依赖 low watermark 来熟悉缓冲区的数据。为了防止内存使用量不受限制的增长，有效的补救措施是通过等待注入较新的记录，直到 low watermark 继续前进，来限制系统中的总偏差。（If a computation is performing an aggregation based on low watermark timers, MillWheel’s performance degrades if data delays hold back low watermarks for large amounts of time. This can result in hours of skew over buffered records in the system. Oftentimes memory usage is proportional to skew, because an application depends on low watermark to flush this buffered data. To prevent memory usage from growing without bound, an effective remedy is to limit total skew in the system, by waiting to inject newer records until the low watermarks have advanced.)</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文属于 MillWheel 论文的阅读与翻译稿，部分地方使用自己的语言进行描述，部分地方进行翻译（附带原文）&lt;br&gt;讲述 MillWhell 的编程模型以及具体实现。MillWhell 的逻辑时间（logical time）使得开发基于时间的聚合应用更方便，从设计之初，MillWhell 就考虑了容错性以及扩展性。&lt;/p&gt;
    
    </summary>
    
    
      <category term="computing" scheme="http://yoursite.com/tags/computing/"/>
    
      <category term="streaming" scheme="http://yoursite.com/tags/streaming/"/>
    
      <category term="millwheel" scheme="http://yoursite.com/tags/millwheel/"/>
    
      <category term="paper" scheme="http://yoursite.com/tags/paper/"/>
    
  </entry>
  
  <entry>
    <title>tasksetmanager</title>
    <link href="http://yoursite.com/2017/12/03/tasksetmanager/"/>
    <id>http://yoursite.com/2017/12/03/tasksetmanager/</id>
    <published>2017-12-03T07:58:53.000Z</published>
    <updated>2017-12-03T09:49:01.000Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">/**</div><div class="line">* Schedules the tasks within a single TaskSet in the TaskSchedulerImpl. This class keeps track of</div><div class="line">* each task, retries tasks if they fail (up to a limited number of times), and</div><div class="line">* handles locality-aware scheduling for this TaskSet via delay scheduling. The main interfaces</div><div class="line">* to it are resourceOffer, which asks the TaskSet whether it wants to run a task on one node,</div><div class="line">* and statusUpdate, which tells it that one of its tasks changed state (e.g. finished).</div><div class="line">*</div><div class="line">* THREADING: This class is designed to only be called from code with a lock on the</div><div class="line">* TaskScheduler (e.g. its event handlers). It should not be called from other threads.</div><div class="line">*</div><div class="line">* @param sched           the TaskSchedulerImpl associated with the TaskSetManager</div><div class="line">* @param taskSet         the TaskSet to manage scheduling for</div><div class="line">* @param maxTaskFailures if any particular task fails this number of times, the entire</div><div class="line">*                        task set will be aborted</div><div class="line">*/</div></pre></td></tr></table></figure>
<p><code>TaskSetManager</code> 负责某个 TaskSet 的调度，对该 TaskSet 的所有 task 进行跟踪，如果有失败的 task，会负责重试（重试有上限），并且通过 delay scheduling（可以想想这个怎么实现的？） 实现 locality  locality-aware scheduling. 主要的接口有 <code>resourceOffer</code> – 用于判断一个 TaskSet 中的 task 是否需要运行到某个 node 上，<code>statusUpdate</code> – 用于跟踪 task 的状态变化。不是线程安全的。</p>
<a id="more"></a>
<p><code>dequeeTaskFromList(execId: String, list: ArrayBuffer[Int]): Option[Int]</code> 负责从对应的 list 中删除返回一个 pending Task，如果没有合适的 Task 就返回 None，该 function 中会将那些已经运行的 task 进行删除，会跳过所有的不能在对应 execId 上运行的 task（通过 executorIsBlacklisted(execId, index) 进行判断）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">while (indexOffset &gt; 0) &#123;</div><div class="line">	indexOffset -= 1</div><div class="line">	val index = list(indexOffset)</div><div class="line">	if (!executorIsBlacklisted(execId, index)) &#123;</div><div class="line">		// This should almost always be list.trimEnd(1) to remove tail</div><div class="line">		list.remove(indexOffset)</div><div class="line">		if (copiesRunning(index) == 0 &amp;&amp; !successful(index)) &#123;</div><div class="line">			return Some(index)</div><div class="line">	    &#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>dequeueTask(execId: String, host: String, maxLocality: TaskLocality.Value): Option[(Int, TaskLocality.Value, Boolean)])</code> 删除并返回一个可执行的 task，只返回符合 locality 约束的 task。首先逐个 locality 进行查找，如果有符合的 task 直接返回，否则返回一个合适的 推测执行的 task</p>
<p><code>executorIsBlacklisted(execId: String, taskId: Int): Boolean</code> 进行判断某个 execId 上能否运行对应的 task（如果之前这个 taskId 在这个 execId 上运行失败了，而且当前时间和之间失败的时间差小于阈值 <code>EXECUTOR_TASK_BLACKLIST_TIMEOUT</code>）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">def executorIsBlacklisted(execId: String, taskId: Int): Boolean = &#123;</div><div class="line">	if (failedExecutors.contains(taskId)) &#123;</div><div class="line">		val failed = failedExecutors.get(taskId).get</div><div class="line"></div><div class="line">		return failed.contains(execId) &amp;&amp;</div><div class="line">		clock.getTimeMillis() - failed.get(execId).get &lt; EXECUTOR_TASK_BLACKLIST_TIMEOUT</div><div class="line">	&#125;</div><div class="line"></div><div class="line">    false</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>dequeueSpeculativeTask(execId: String, host: String, locality: TaskLocality.Value): Option[(Int, TaskLocality.Value)]</code> 负责删除并返回一个 推测执行的 task，如果没有合适的就返回 None。逻辑就是遍历所有 task，然后看 task 是否能运行在特定的 TaskLocality 上，如果可以就返回，并且将该 task 从推测执行的 task list 中删除。TaskLocality 的顺序为 <code>{PROCESS_LOCAL, NODE_LOCAL, NO_PREF, RACK_LOCAL, ANY}</code></p>
<p>如何实施推测执行，逻辑在 <code>checkSpeculatableTasks</code> 函数中，</p>
<ol>
<li>如果该 TaskSetManager 变为 zoombie 了，或者只包含一个 task，就不推测执行（为什么一个 task 就不推测执行）</li>
<li>如果完成的 task 数大于等于 <code>minFinishedForSpeculation = (SPECULATION_QUANTILE * numTasks).floor.toInt</code>（其中 <code>SPECULATION_QUANTILE</code> 默认 0.75，可以通过 <code>spark.speculation.quantile</code> 设置）且有 task 成功执行过，则执行下面的步骤</li>
<li>将所有执行成功的 task 的执行时间进行排序，取第 <code>val medianDuration = durations(min((0.5 * tasksSuccessful).round.toInt, durations.size - 1))
val threshold = max(SPECULATION_MULTIPLIER * medianDuration, 100)</code>  threshold 作为临界值，对每个 task 进行检测。</li>
<li>如果该 task 还没有运行成功，运行时间超过 <code>threshold</code>，只有一个实例在跑，而且没有推测执行过，就进行推测执行</li>
<li>推测执行保证同一个 task 的不同实例不会调度到同一台主机上，且不会调度到以及被加进黑名单的主机中</li>
</ol>
<p><code>resourceOffer（execId: String,host: String,maxLocality:TaskLocality.TaskLocality): Option[TaskDescription]</code> 负责资源的实际分配，如果当前 taskSetManager 不是 zoombie 状态才进行处理。首先找出当前时间可以被调度到的最高 Locality，然后使用 <code>dequeuTask</code> 删除并找到一个符合条件的 task，如果找到就更新相关的状态数据（包括，更新现在正在运行的 task 有哪些，更新当前的 locality，然后将 task 所需要的文件等序列化，附加到一个 TaskDescription 结构中并且返回），并且通知 DAGScheduler 该 task 已经开始运行。如果序列化有问题，则直接抛异常。</p>
<p><code>getAllowedLocalityLevel(curTime: Long): TaskLocality.TaskLocality</code> 获取当前时间对应的一个 TaskLocality, 这里面会有时间等待（等待的时间就是每个 TaskLocality 的等待时间，默认 3s，可以配置）</p>
<p><code>handleTaskGettingResult</code> 主要进行状态标记，然后通知 DAGScheduler<br><code>canFetchMoreResults(size: Long): Boolean</code> 检测是否还能 fetch size 字节大小已经序列化后的数据，如果不能，就将该 taskSetManger 标记为 zoombie，并且通知 DAGScheduler 该 TaskSet 为失败</p>
<p><code>handleSuccessfulTask(tid: Long, result: DirectTaskResult[_]): Unit</code> 负责将一个 task 标记为成功，并且如果当前 TaskSet 所有 task 都运行完成，就标记为 zoombie 状态，并且通知 DAGScheduler。</p>
<p><code>handleFailedTask(tid: Long, state: TaskState, reason: TaskEndReason)</code> 将task 标记为失败，并且重新添加到 pendingTask 队列中，然后通知 DAGScheduler。根据失败的信息不同，做不同的处理：</p>
<ol>
<li>FetchFailure：直接想当前的 <code>tasksetManager</code> 标记为 zoombie，然后做一定的清理工作，就把当前的 <code>tasksetManager</code> 标记为成功</li>
<li>ExceptionFailure：如果是 <code>NotSerializableException</code> 就直接退出，否则会打印相应异常，然后进行重试</li>
<li>其他的异常，打印日志</li>
</ol>
<p><code>executorLost(execId: String, host: String, reason: ExecutorLossReason)</code> 负责处理 executorLost 的情况，由 taskSchedulerImp 调用。逻辑如下</p>
<ul>
<li>如果是 <code>ShufflleMapTask</code> 且没有开启 <code>externalShuffleServiceEnabled</code> 就进行如下操作：如果 task 以及成功了，就将这些 task 标记为失败，且进行重试（因为后续的 task 需要从这些 task 中获取数据）</li>
<li>如果是其他的，就直接调用 <code>handleFailedTask</code> 进行处理，然后重新计算 <code>locality</code></li>
</ul>
<p><code>getLocalityWait(level: TaskLocality.TaskLocality): Long =</code> 用于获取 locality, 代码如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">val defaultWait = conf.get(&quot;spark.locality.wait&quot;, &quot;3s&quot;)</div><div class="line">val localityWaitKey = level match &#123;</div><div class="line">	case TaskLocality.PROCESS_LOCAL =&gt; &quot;spark.locality.wait.process&quot;</div><div class="line">	case TaskLocality.NODE_LOCAL =&gt; &quot;spark.locality.wait.node&quot;</div><div class="line">	case TaskLocality.RACK_LOCAL =&gt; &quot;spark.locality.wait.rack&quot;</div><div class="line">	case _ =&gt; null</div><div class="line">&#125;</div><div class="line"></div><div class="line">if (localityWaitKey != null) &#123;</div><div class="line">	conf.getTimeAsMs(localityWaitKey, defaultWait)</div><div class="line">&#125; else &#123;</div><div class="line">	0L</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h3 id="疑问："><a href="#疑问：" class="headerlink" title="疑问："></a>疑问：</h3><ol>
<li>推测执行的时候，为什么 TaskSet 只有 1 个 task 的话就不推测执行</li>
<li>推测执行对每个 task 只会进行一次？<br> a. 可以被 推测执行多次，只执行一次的逻辑是使用 <code>speculatableTasks</code> 做检测的，当运行一个 推测执行的 task 后，该 task 就会从 <code>speculatableTasks</code> 进行删除，然后就可以进行推测执行了。严格的说法是，只运行同一个 task 的一个实例在“排队等待被推测执行”</li>
<li>每个 task 的 <code>preferredLocations</code> 怎么得到的？根据什么规则？每一个的含义又是什么，总共有 <code>{PROCESS_LOCAL, NODE_LOCAL, NO_PREF, RACK_LOCAL, ANY}</code> 这些可选项</li>
<li>每个 TaskSet 的所有 task 都是一样的 locality？</li>
<li>推测执行的时候，如果最后执行成功多个 task，会对结果有影响吗？怎么规避这种影响的</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;/**&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;* Schedules the tasks within a single TaskSet in the TaskSchedulerImpl. This class keeps track of&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;* each task, retries tasks if they fail (up to a limited number of times), and&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;* handles locality-aware scheduling for this TaskSet via delay scheduling. The main interfaces&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;* to it are resourceOffer, which asks the TaskSet whether it wants to run a task on one node,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;* and statusUpdate, which tells it that one of its tasks changed state (e.g. finished).&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;*&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;* THREADING: This class is designed to only be called from code with a lock on the&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;* TaskScheduler (e.g. its event handlers). It should not be called from other threads.&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;*&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;* @param sched           the TaskSchedulerImpl associated with the TaskSetManager&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;* @param taskSet         the TaskSet to manage scheduling for&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;* @param maxTaskFailures if any particular task fails this number of times, the entire&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;*                        task set will be aborted&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;*/&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;&lt;code&gt;TaskSetManager&lt;/code&gt; 负责某个 TaskSet 的调度，对该 TaskSet 的所有 task 进行跟踪，如果有失败的 task，会负责重试（重试有上限），并且通过 delay scheduling（可以想想这个怎么实现的？） 实现 locality  locality-aware scheduling. 主要的接口有 &lt;code&gt;resourceOffer&lt;/code&gt; – 用于判断一个 TaskSet 中的 task 是否需要运行到某个 node 上，&lt;code&gt;statusUpdate&lt;/code&gt; – 用于跟踪 task 的状态变化。不是线程安全的。&lt;/p&gt;
    
    </summary>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="source_code" scheme="http://yoursite.com/tags/source-code/"/>
    
      <category term="tasksetmanager" scheme="http://yoursite.com/tags/tasksetmanager/"/>
    
      <category term="task" scheme="http://yoursite.com/tags/task/"/>
    
      <category term="stage" scheme="http://yoursite.com/tags/stage/"/>
    
  </entry>
  
  <entry>
    <title>TaskScheduler</title>
    <link href="http://yoursite.com/2017/11/27/TaskScheduler/"/>
    <id>http://yoursite.com/2017/11/27/TaskScheduler/</id>
    <published>2017-11-27T13:02:24.000Z</published>
    <updated>2017-11-27T13:12:23.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>本文属于自己看源码后的记录</p>
</blockquote>
<p>与不同的后端调度器一起，进行 task 的调度（task 是 DAGScheduler 中划分的 Stage 中的具体任务），后端调度器包括 <code>LocalBackend</code>， <code>SparkDeploySchedulerBackend</code>，<code>MesosSchedulerBackend</code>，<code>YarnClientSchedulerBackend</code>， <code>YarnClusterSchedulerBackend</code>，<code>SimrSchedulerBackend</code>等</p>
<p>整个 TaskSchedulerImpl 比较简单，复杂的地方在于和各种 后端调度器联合，以及具体 <code>TasksetManager</code> 进行联合</p>
<a id="more"></a>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">TaskSchedulerImpl(</div><div class="line">	val sc: SparkContext, // SparkContext</div><div class="line">	val maxTaskFailures: Int, //每个 task 允许失败的最大次数</div><div class="line">	isLocal: Boolean = false) //是否本地运行</div></pre></td></tr></table></figure>
<p>对于没有传入 <code>maxTaskFailures</code> 参数的，使用 <code>sc.conf.getInt(&quot;spark.task.maxFailures&quot;, 4)</code>，可以使用配置参数，默认为 4 次。</p>
<p><code>TaskSchedulerImple</code> 的入口是 <code>submitTasks()</code> ，由 <code>DAGScheduler</code> 划分好 Stage 之后进行调用</p>
<p>在 <code>submitTasks</code> 中会进行资源的检测（是否申请到资源，标志位 <code>hasLaunchedTask</code> 由后面的 <code>resourceOffers</code> 进行设置），每 <code>STARVATION_TIMEOUT_MS</code> 检测一次，直到 <code>hasLaunchedTask</code> 为 True 为止，<code>STARVATION_TIMEOUT_MS</code> 默认 15s，可以通过 <code>spark.starvation.timeout</code> 进行配置，该函数会将当前的 taskSetManger 添加到整个 <code>ScheduleablePool</code> 中，然后通过 <code>backend.reviveOffer</code> 将 task 分配到 executor 上去</p>
<p>另外在 <code>TaskSchedulerImpl</code> 中会启动 推测执行的后台线程，默认 100ms 检测一次，可以通过 <code>spark.speculation.interval</code> 进行配置</p>
<p>每个 Task 默认使用的 CPU 数由 <code>CPUS_PER_TASK</code> 进行控制，默认为 1，可以通过 <code>spark.task.cpus</code> 进行设置</p>
<p><code>private val taskSetsByStageIdAndAttempt = new HashMap[Int, HashMap[Int, TaskSetManager]]</code> 根据 StageId 和 Attempt 来确定 taskSets</p>
<p><code>private val schedulingModeConf = conf.get(&quot;spark.scheduler.mode&quot;, &quot;FIFO&quot;)</code> task 调度的模式，默认为 FIFO</p>
<p><code>private[spark] var taskResultGetter = new TaskResultGetter(sc.env, this) --- Runs a thread pool that deserializes and remotely fetches (if necessary) task results</code> </p>
<p><code>executorsByHost = new HashMap[String, HashSet[String]]</code> 保存每个 Host 中运行的所有 executor</p>
<p><code>hostsByRack = new HashMap[String, HashSet[String]]</code> 不同 Rack 对于的 host</p>
<p><code>executorIdToHost = new HashMap[String, String]</code> 保存 executorId 到 host 的对于关系</p>
<p>另外还有 <code>backend: SchedulerBackend</code>， <code>dagScheduler: DAGScheduler</code>，<code>mapOutputTracker = SparkEnv.get.mapOutputTracker</code>，</p>
<p><code>initialize（）</code> 主要设置 backend 以及 schedulerPool，<br><code>start()</code> 启动 backend, 然后按需启动 推测执行的线程</p>
<p>Yarn-cluster 模式下的 backend 为 <code>org.apache.spark.scheduler.cluster.YarnClusterSchedulerBackend</code>，其中 backend 在 <code>SparkContext</code> 中初始化（根据运行的模式不同，初始化不同的 backend）</p>
<p><code>resourceOffers(offers: Seq[WorkerOffer]): Seq[Seq[TaskDescription]]</code> 用于分配资源，将 task 分配到指定的 executor 上，传入的参数是所有可以分配 task 的 executor，流程如下：</p>
<ol>
<li>首先更新 executorIdToHost，executorIdToRunningTaskIds 以及 hostsByRack，这些信息在分配 task 的时候会用到</li>
<li>将所有的 executor 进行打乱，防止全部 task 都分配到一个 executor 上（这里和下面的策略关联，下面使用 round-robin 进行分配），那么分配策略还有其他的吗？不同策略之间的对比是怎样的</li>
<li>根据具体的策略（FIFO 或者 FAIR 等）将所有 task 进行排序（如果本次有新加的 executor，那么所有的 taskset 都需要重新计算 locality）根据什么规则进行 locality 的重新计算？具体在 TaskSetManager 实现</li>
<li>通过 resourceOfferSingleTaskSet 进行分配</li>
</ol>
<p><code>resourceOfferSingleTaskSet(taskSet: TaskSetManager, maxLocality: TaskLocality, shuffledOffers: Seq[WorkerOffer], availableCpus: Array[Int],tasks: Seq[ArrayBuffer[TaskDescription]]) : Boolean = {}</code> 给一个 TaskSet 分配资源，具体逻辑如下：</p>
<p>枚举每个 executor，如果当前 executor 的可用 cpu 大于单个 task 需要的 cpu<br>则调用 tasksetManager.resourceOffer() 进行具体的 task 分配，并且更新相应的信息<br>包括 taskIdToTaskSetManager，taskIdToExecutorId, executorIdToRunningTaskIds，executorsByHost 等。然后直接返回</p>
<p><code>cancelTasks(stageId: Int, interruptThread: Boolean): Unit</code> 取消某个 stage 的所有 task。分两种情况：</p>
<ul>
<li>task set manager 已经创建，并且有部分 task 已经被调度，这种情况下，给对于的 executor 发送 kill 指令，然后终止整个 stage</li>
<li>task set manager 已经创建，但是还没有 task 被调度，直接终止整个 stage 即可</li>
</ul>
<p><code>taskSetFinished(manager: TaskSetManager): Unit</code><br>整个 task set manager 的 task 都执行完成了，主要做一些清理公族藕，然后将 task set manager 删除</p>
<p><code>statusUpdate(tid: Long, state: TaskState, serializedData: ByteBuffer)</code> 进行 task 的状态更新</p>
<ul>
<li>如果 task 的状态为 LOST</li>
<li>这个状态仅仅在 Mesos 中使用，每个 executor 对于一个 task，所以将这个 executor 标记为 Failure</li>
<li>如果 task 的状态为 {FINISHED, FAILED, KILLED, LOST} 中的任何一种</li>
<li>首先清理 taskset scheduler 的一些状态信息（比如需要跟踪的 task 等），并根据状态信息（FINISHED 是一类，其他三种为一类）更新相应的状态信息<br>如果检测到有失败的 executor（只在 mesos 模式下会产生），会给 dagscheduler 发送 executorLost 信号，并且调用 backend 的 makeOffer 重新调度 task</li>
</ul>
<p><code>executorHeartbeatReceived(execId: String, taskMetrics: Array[(Long, TaskMetrics)], blockManagerId: BlockManagerId): Boolean) = {}</code><br>更新正在运行的 task 的状态， 并且充当 master 和 BlockManager 的心跳，如果 master 认识当前 blockManager，则返回 true，否则返回 false – 表示 blockManager 需要重新注册</p>
<ul>
<li>taskMetrics: Array[(Long, TaskMetrics)]  //taskId -&gt; TaskMetrics</li>
<li>根据每个 taskId 获取对于的信息（taskId, taskSetMgr.stageId, taskSetMgr.taskSet.stageAttemptId, metrics) 发送给 dagScheduler, 其中 metric 为 taskMetrics 的第二维信息</li>
</ul>
<p><code>handleTaskGettingResult() &amp; handleSuccessfulTask()</code> 直接调用对应的 taskSetManager 对应方法</p>
<p><code>handleFailedTask(taskSetManager: TaskSetManager, tid: Long, taskState: TaskState, reason: TaskEndReason): Unit = synchronized {}</code><br>如果对应的 taskSetManger 不是僵尸状态，而且 taskState 不为 KILLED，就重新申请资源</p>
<p><code>executorLost(executorId: String, reason: ExecutorLossReason): Unit = {}</code></p>
<ul>
<li>如果 <code>executorIdToRunningTasksIds</code> 包含当前上报的 executor（失败的 executor 有 task 正在运行）, 则将当前 executor 从所有的数据结构中进行删除，然后通知 <code>DAGScheduler</code> 并且重新申请资源</li>
<li>否则只进行一些数据结构的处理（包括从所有的数据结构中进行删除，然后打印 logger 等）</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文属于自己看源码后的记录&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;与不同的后端调度器一起，进行 task 的调度（task 是 DAGScheduler 中划分的 Stage 中的具体任务），后端调度器包括 &lt;code&gt;LocalBackend&lt;/code&gt;， &lt;code&gt;SparkDeploySchedulerBackend&lt;/code&gt;，&lt;code&gt;MesosSchedulerBackend&lt;/code&gt;，&lt;code&gt;YarnClientSchedulerBackend&lt;/code&gt;， &lt;code&gt;YarnClusterSchedulerBackend&lt;/code&gt;，&lt;code&gt;SimrSchedulerBackend&lt;/code&gt;等&lt;/p&gt;
&lt;p&gt;整个 TaskSchedulerImpl 比较简单，复杂的地方在于和各种 后端调度器联合，以及具体 &lt;code&gt;TasksetManager&lt;/code&gt; 进行联合&lt;/p&gt;
    
    </summary>
    
    
      <category term="Spark" scheme="http://yoursite.com/tags/Spark/"/>
    
      <category term="Task" scheme="http://yoursite.com/tags/Task/"/>
    
      <category term="Scheduler" scheme="http://yoursite.com/tags/Scheduler/"/>
    
      <category term="TaskScheduler" scheme="http://yoursite.com/tags/TaskScheduler/"/>
    
      <category term="Source_Code" scheme="http://yoursite.com/tags/Source-Code/"/>
    
      <category term="Code" scheme="http://yoursite.com/tags/Code/"/>
    
  </entry>
  
  <entry>
    <title>git inside</title>
    <link href="http://yoursite.com/2017/11/20/git-inside/"/>
    <id>http://yoursite.com/2017/11/20/git-inside/</id>
    <published>2017-11-20T13:17:59.000Z</published>
    <updated>2017-11-20T14:04:38.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p><a href="https://git-scm.com/" target="_blank" rel="external">Git</a>是一个分布式的版本控制系统，能够完成你能想到的关于版本相关的所有事情，但是 Git 却不是那么好上手，也就是所谓的入门门槛有点高。</p>
</blockquote>
<h1 id="本文会讲什么"><a href="#本文会讲什么" class="headerlink" title="本文会讲什么"></a>本文会讲什么</h1><p>本文会换一个角度讲述 Git 怎么做的，给大家提供一个另外的视角，这个视角主要设计 Git 的存储，这样给大家一个更深的认识，在平时想了解的时候，也能有合适的渠道进行。</p>
<a id="more"></a>
<h1 id="Git-是什么"><a href="#Git-是什么" class="headerlink" title="Git 是什么"></a>Git 是什么</h1><p>这个问题看上去想一句废话，因为文章开始就说了，Git 是一个分布式的版本控制系统。那么底层又是如何实现的呢？以及其他人是怎么看待 Git 的呢？</p>
<blockquote>
<p>In many ways you can just see git as a filesystem. –Linus </p>
</blockquote>
<p>从上面一句话来看，Linus 把 Git 当做一个文件系统来做的，而不是一个传统意义上的版本控制系统，恰好 Git 能够做版本控制的事情。</p>
<h1 id="Git-文件结构"><a href="#Git-文件结构" class="headerlink" title="Git 文件结构"></a>Git 文件结构</h1><p>每一个 Git 仓库中都有一个隐藏的文件夹，名字叫做 <code>.git</code> 这个文件夹包含了所有的文件内容，以及版本控制相关的信息，大致结构如下：</p>
<img src="/2017/11/20/git-inside/file_tree.png" alt="file_tree.png" title="">
<p>说几个大家关注的，有兴趣的可以自己打开文件看看</p>
<ol>
<li>HEAD 指向当前分支的最后一个 commit</li>
<li>config 表示一些本项目的配置（如果没有会使用全局的配置），最明显的是 user.name 和 user.email</li>
<li>hooks 表示一些钩子函数</li>
<li>objects, 这里面保存的是所有的数据（也就是实实在在的内容），会是主要的内容</li>
<li>refs 表示一些引用，包括分支的，远程分支的，tag 的，每一个都具体指向一个 commit</li>
</ol>
<h1 id="几个概念"><a href="#几个概念" class="headerlink" title="几个概念"></a>几个概念</h1><ol>
<li>blob :上面的 objects 中的具体文件内容（保存的是文件内容，如果多个文件的内容一样，则只保存一份）</li>
<li>tree : tree 包含 tree 或者 blob</li>
<li>commit : commit 指向 tree 的根节点</li>
<li>tag : 指向一个具体的 commit</li>
<li>parent : 表示该 commit 从哪个 commit 演变而来</li>
<li>branch : 一条 commit 的链</li>
<li>HEAD : branc 的最新 commit</li>
</ol>
<h1 id="一个例子"><a href="#一个例子" class="headerlink" title="一个例子"></a>一个例子</h1><p>假设我们有一个项目，如下图所示：</p>
<img src="/2017/11/20/git-inside/project.png" alt="project.png" title="">
<p>那么我们初始化后的 git 示意图应该是这样的</p>
<img src="/2017/11/20/git-inside/first_version.png" alt="first_version.png" title="">
<p>接着我们修改 c.txt，那么我们会得到下面的示意图</p>
<img src="/2017/11/20/git-inside/second_version.png" alt="second_version.png" title="">
<p>再接着我们修改 a.txt，我们就会得到下面的示意图</p>
<img src="/2017/11/20/git-inside/third_version.png" alt="third_version.png" title="">
<p>上面的图中灰色表示非当前版本，亮色表示当前版本，三角形代表 commit，树形表示 tree，六边形表示 blob。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文介绍了 Git 的一些基本概念，以及一个简单的示例，当然这些还远远不够了解 Git 本身的，但是我认为这些是 Git 中最基本，也是最核心的一些东西了，其他的命令大概都能够从这几个命令中得到。</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://git-scm.com/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Git&lt;/a&gt;是一个分布式的版本控制系统，能够完成你能想到的关于版本相关的所有事情，但是 Git 却不是那么好上手，也就是所谓的入门门槛有点高。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;本文会讲什么&quot;&gt;&lt;a href=&quot;#本文会讲什么&quot; class=&quot;headerlink&quot; title=&quot;本文会讲什么&quot;&gt;&lt;/a&gt;本文会讲什么&lt;/h1&gt;&lt;p&gt;本文会换一个角度讲述 Git 怎么做的，给大家提供一个另外的视角，这个视角主要设计 Git 的存储，这样给大家一个更深的认识，在平时想了解的时候，也能有合适的渠道进行。&lt;/p&gt;
    
    </summary>
    
    
      <category term="git" scheme="http://yoursite.com/tags/git/"/>
    
      <category term="file-system" scheme="http://yoursite.com/tags/file-system/"/>
    
      <category term="inside" scheme="http://yoursite.com/tags/inside/"/>
    
  </entry>
  
  <entry>
    <title>django-configuration in action</title>
    <link href="http://yoursite.com/2017/11/09/django-configuration-in-action/"/>
    <id>http://yoursite.com/2017/11/09/django-configuration-in-action/</id>
    <published>2017-11-09T14:34:22.000Z</published>
    <updated>2017-11-10T06:36:03.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Django-中统一配置的做法"><a href="#Django-中统一配置的做法" class="headerlink" title="Django 中统一配置的做法"></a>Django 中统一配置的做法</h1><blockquote>
<p>本文主要描述如何在 django 中统一 settings 文件</p>
</blockquote>
<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>django 服务会有多个环境，比如 开发环境、测试环境以及线上环境等。现在大部分使用的方案是针对每一种环境使用一个 settings 文件，然后在不同的环境中使用不同的 settings 文件。这样的设计我认为有至少两个问题:</p>
<ol>
<li>很多公用的配置不太好公用</li>
<li>文件数会很多，项目中管理会比较麻烦</li>
</ol>
<a id="more"></a>
<h1 id="新方案"><a href="#新方案" class="headerlink" title="新方案"></a>新方案</h1><p>针对 settings 文件，在 django 项目中可以使用 django-configuration module 进行管理，从而解决上面的两个问题。</p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><ol>
<li>使用 <code>pip install django-configurations</code> 安装 django-configuration module </li>
<li>修改 settings.py manager.py 以及 wsgi.py 三个文件即可</li>
<li>建立一个配置统一的文件，用于管理不同的配置项</li>
</ol>
<h3 id="具体-Demo"><a href="#具体-Demo" class="headerlink" title="具体 Demo"></a>具体 Demo</h3><p>settings.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div></pre></td><td class="code"><pre><div class="line">from configurations import Configuration</div><div class="line"># Build paths inside the project like this: os.path.join(BASE_DIR, ...)</div><div class="line">import os</div><div class="line"></div><div class="line"></div><div class="line">class Common(Configuration):</div><div class="line">    BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))</div><div class="line"></div><div class="line">   # Quick-start development settings - unsuitable for production</div><div class="line">    # See https://docs.djangoproject.com/en/1.8/howto/deployment/checklist/</div><div class="line"></div><div class="line">    # SECURITY WARNING: keep the secret key used in production secret!</div><div class="line">    SECRET_KEY = &apos;secret_key&apos;</div><div class="line"></div><div class="line">    # SECURITY WARNING: don&apos;t run with debug turned on in production!</div><div class="line">    DEBUG = True</div><div class="line"></div><div class="line">    ALLOWED_HOSTS = []</div><div class="line"></div><div class="line">   # Application definition</div><div class="line"></div><div class="line">    INSTALLED_APPS = (</div><div class="line">		&apos;django.contrib.admin&apos;,</div><div class="line">		&apos;django.contrib.auth&apos;,</div><div class="line">		&apos;django.contrib.contenttypes&apos;,</div><div class="line">		&apos;django.contrib.sessions&apos;,</div><div class="line">		&apos;django.contrib.messages&apos;,</div><div class="line">		&apos;django.contrib.staticfiles&apos;,</div><div class="line">																				    )</div><div class="line"></div><div class="line">		MIDDLEWARE_CLASSES = (</div><div class="line">			&apos;django.contrib.sessions.middleware.SessionMiddleware&apos;,</div><div class="line">			&apos;django.middleware.common.CommonMiddleware&apos;,</div><div class="line">			&apos;django.middleware.csrf.CsrfViewMiddleware&apos;,</div><div class="line">			&apos;django.contrib.auth.middleware.AuthenticationMiddleware&apos;,</div><div class="line">			&apos;django.contrib.auth.middleware.SessionAuthenticationMiddleware&apos;,</div><div class="line">			&apos;django.contrib.messages.middleware.MessageMiddleware&apos;,</div><div class="line">			&apos;django.middleware.clickjacking.XFrameOptionsMiddleware&apos;,</div><div class="line">			&apos;django.middleware.security.SecurityMiddleware&apos;,</div><div class="line">			)</div><div class="line"></div><div class="line">		ROOT_URLCONF = &apos;leopard.urls&apos;</div><div class="line"></div><div class="line">		TEMPLATES = [</div><div class="line">			&#123;</div><div class="line">				&apos;BACKEND&apos;: &apos;django.template.backends.django.DjangoTemplates&apos;,</div><div class="line">				&apos;DIRS&apos;: [],</div><div class="line">				&apos;APP_DIRS&apos;: True,</div><div class="line">				&apos;OPTIONS&apos;: &#123;</div><div class="line">					&apos;context_processors&apos;: [</div><div class="line">					&apos;django.template.context_processors.debug&apos;,</div><div class="line">					&apos;django.template.context_processors.request&apos;,</div><div class="line">					&apos;django.contrib.auth.context_processors.auth&apos;,</div><div class="line">					&apos;django.contrib.messages.context_processors.messages&apos;,</div><div class="line">					],</div><div class="line">				&#125;,</div><div class="line">			&#125;,</div><div class="line">		]</div><div class="line"></div><div class="line">		WSGI_APPLICATION = &apos;leopard.wsgi.application&apos;</div><div class="line"></div><div class="line">		# Internationalization</div><div class="line">		# https://docs.djangoproject.com/en/1.8/topics/i18n/</div><div class="line"></div><div class="line">		LANGUAGE_CODE = &apos;en-us&apos;</div><div class="line"></div><div class="line">		TIME_ZONE = &apos;UTC&apos;</div><div class="line"></div><div class="line">		USE_I18N = True</div><div class="line"></div><div class="line">		USE_L10N = True</div><div class="line"></div><div class="line">		USE_TZ = True</div><div class="line"></div><div class="line">		# Static files (CSS, JavaScript, Images)</div><div class="line">		# https://docs.djangoproject.com/en/1.8/howto/static-files/</div><div class="line"></div><div class="line">		STATIC_URL = &apos;/static/&apos;</div><div class="line"></div><div class="line">		STATE_REDIS_DB = 6</div><div class="line"></div><div class="line">		LOGGING = &#123;</div><div class="line">			&apos;version&apos;: 1,</div><div class="line">			&apos;disable_existing_loggers&apos;: False,</div><div class="line">			&apos;formatters&apos;: &#123;</div><div class="line">				&apos;verbose&apos;: &#123;</div><div class="line">					&apos;format&apos;: &apos;%(levelname)s %(asctime)s %(module)s %(funcName)s %(message)s&apos;</div><div class="line">				&#125;,</div><div class="line">			&#125;,</div><div class="line">			&apos;handlers&apos;: &#123;</div><div class="line">				&apos;mail_admins&apos;: &#123;</div><div class="line">					&apos;level&apos;: &apos;ERROR&apos;,</div><div class="line">					&apos;class&apos;: &apos;django.utils.log.AdminEmailHandler&apos;</div><div class="line">				&#125;,</div><div class="line">				&apos;console&apos;: &#123;</div><div class="line">					&apos;level&apos;: &apos;DEBUG&apos;,</div><div class="line">					&apos;class&apos;: &apos;logging.StreamHandler&apos;</div><div class="line">					&#125;,</div><div class="line">				&apos;realtimejob_file&apos;: &#123;</div><div class="line">					&apos;level&apos;: &apos;INFO&apos;,</div><div class="line">					&apos;class&apos;: &apos;logging.handlers.RotatingFileHandler&apos;,</div><div class="line">					&apos;filename&apos;: &apos;filename&apos;,</div><div class="line">					&apos;formatter&apos;: &apos;verbose&apos;</div><div class="line">					&#125;,</div><div class="line">				&#125;,</div><div class="line">			&apos;loggers&apos;: &#123;</div><div class="line">				&apos;django.request&apos;: &#123;</div><div class="line">					&apos;handlers&apos;: [&apos;mail_admins&apos;],</div><div class="line">					&apos;level&apos;: &apos;ERROR&apos;,</div><div class="line">					&apos;propagate&apos;: True,</div><div class="line">				&#125;,</div><div class="line">				&apos;django_crontab.crontab&apos;: &#123;</div><div class="line">					&apos;handlers&apos;: [&apos;console&apos;],</div><div class="line">						&apos;level&apos;: &apos;DEBUG&apos;,</div><div class="line">						&apos;propagate&apos;: True,</div><div class="line">						&#125;,</div><div class="line">					&apos;realtimejob&apos;: &#123;</div><div class="line">						&apos;handlers&apos;: [&apos;realtimejob_file&apos;],</div><div class="line">						&apos;level&apos;: &apos;INFO&apos;,</div><div class="line">						&apos;propagate&apos;: False</div><div class="line">						&#125;,</div><div class="line">			&#125;,</div><div class="line">		&#125;</div><div class="line"></div><div class="line"></div><div class="line">class Dev(Common):</div><div class="line">	  DEBUG = True</div><div class="line">	  DATABASES = &#123;</div><div class="line">		&apos;default&apos;: &#123;</div><div class="line">			&apos;ENGINE&apos;: &apos;django.db.backends.mysql&apos;,</div><div class="line">			&apos;NAME&apos;: &apos;name&apos;,</div><div class="line">			&apos;USER&apos;: &apos;user&apos;,</div><div class="line">			&apos;PASSWORD&apos;: &apos;password&apos;,</div><div class="line">			&apos;HOST&apos;: &apos;host&apos;,</div><div class="line">			&apos;PORT&apos;: &apos;port&apos;,</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">    STATE_REDIS_HOST = &apos;127.0.0.1&apos;</div><div class="line">	STATE_REDIS_PORT = 6379</div><div class="line">	STATE_REDIS_SOCKET_TIMEOUT = 1000</div><div class="line"></div><div class="line"></div><div class="line">class Prod(Common):</div><div class="line">	# Database</div><div class="line">	# https://docs.djangoproject.com/en/1.8/ref/settings/#databases</div><div class="line">	DATABASES = &#123;</div><div class="line">		&apos;default&apos;: &#123;</div><div class="line">			&apos;ENGINE&apos;: &apos;django.db.backends.mysql&apos;,</div><div class="line">			&apos;NAME&apos;: &apos;name&apos;,</div><div class="line">			&apos;USER&apos;: &apos;user&apos;,</div><div class="line">			&apos;PASSWORD&apos;: &apos;password&apos;,</div><div class="line">			&apos;HOST&apos;: &apos;host&apos;,</div><div class="line">			&apos;PORT&apos;: &apos;port&apos;,</div><div class="line">		&#125;</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<p>manager.py<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">if __name__ == &quot;__main__&quot;:</div><div class="line">    os.environ.setdefault(&quot;DJANGO_SETTINGS_MODULE&quot;, &quot;settings&quot;)</div><div class="line">	os.environ.setdefault(&apos;DJANGO_CONFIGURATION&apos;, &apos;Dev&apos;)  #设置默认的环境</div><div class="line"></div><div class="line">	from configurations.management import execute_from_command_line # 引入需要的包</div><div class="line"></div><div class="line">	execute_from_command_line(sys.argv) # 使用 django-configurations 来启动</div></pre></td></tr></table></figure></p>
<p>wsgi.py 修改同上所示<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">import os</div><div class="line">from configurations.wsgi import get_wsgi_application # 引入相关的包</div><div class="line"></div><div class="line">os.environ.setdefault(&quot;DJANGO_SETTINGS_MODULE&quot;, &quot;settings&quot;)</div><div class="line">os.environ.setdefault(&apos;DJANGO_CONFIGURATION&apos;, &apos;Dev&apos;)</div><div class="line"></div><div class="line">application = get_wsgi_application() # 使用 django-configurations 中的如括进行启动</div></pre></td></tr></table></figure></p>
<p>对于希望针对不同的环境引入不同的变量值（变量名字一样），可以新建一个 commonsettings.py 文件。demo 如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">import settings</div><div class="line">import os</div><div class="line"></div><div class="line">DJANGO_CONFIGURATION_KEY = &apos;DJANGO_CONFIGURATION&apos;</div><div class="line">STATE_REDIS_HOST = getattr(settings, os.environ.get(DJANGO_CONFIGURATION_KEY, &apos;Dev&apos;)).STATE_REDIS_HOST</div><div class="line">STATE_REDIS_PORT = getattr(settings, os.environ.get(DJANGO_CONFIGURATION_KEY, &apos;Dev&apos;)).STATE_REDIS_PORT</div><div class="line">STATE_REDIS_DB = getattr(settings, os.environ.get(DJANGO_CONFIGURATION_KEY, &apos;Dev&apos;)).STATE_REDIS_DB</div><div class="line">STATE_REDIS_SOCKET_TIMEOUT = getattr(settings, os.environ.get(DJANGO_CONFIGURATION_KEY, &apos;Dev&apos;)).STATE_REDIS_SOCKET_TIMEOUT</div></pre></td></tr></table></figure>
<p>这样，所有的人从 commonsettings.py 进行引入即可，不需要关心使用的那个环境（commonsettings.py 中没有处理异常，希望对于异常情况提前抛出来）</p>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Django-中统一配置的做法&quot;&gt;&lt;a href=&quot;#Django-中统一配置的做法&quot; class=&quot;headerlink&quot; title=&quot;Django 中统一配置的做法&quot;&gt;&lt;/a&gt;Django 中统一配置的做法&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;本文主要描述如何在 django 中统一 settings 文件&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h1&gt;&lt;p&gt;django 服务会有多个环境，比如 开发环境、测试环境以及线上环境等。现在大部分使用的方案是针对每一种环境使用一个 settings 文件，然后在不同的环境中使用不同的 settings 文件。这样的设计我认为有至少两个问题:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;很多公用的配置不太好公用&lt;/li&gt;
&lt;li&gt;文件数会很多，项目中管理会比较麻烦&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="django" scheme="http://yoursite.com/tags/django/"/>
    
      <category term="settings" scheme="http://yoursite.com/tags/settings/"/>
    
      <category term="code-style" scheme="http://yoursite.com/tags/code-style/"/>
    
  </entry>
  
  <entry>
    <title>spark_dagscheduler</title>
    <link href="http://yoursite.com/2017/10/16/spark-dagscheduler/"/>
    <id>http://yoursite.com/2017/10/16/spark-dagscheduler/</id>
    <published>2017-10-16T12:01:20.000Z</published>
    <updated>2017-10-18T03:39:10.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>基于 spark 1.6</p>
</blockquote>
<p>面向 Stage 的调度器，负责计算每个 job 的 DAG，并将 DAG 图划分为不同的 stage，对哪些 RDD 以及相应输出进行记录，寻找一个运行相应 job 所需要的最小 stage。然后将 <code>stage</code> 以 <code>TaskSet</code> 的形式提交到下层的 <code>TaskScheduler</code> 进行具体的 task 调度。每个 <code>TaskSet</code> 包含整个可以独立运行的 task，这些 task 能够利用集群上已有的数据立即运行起来，如果集群上已有的数据已经不存在了，那么当前 task 就会失败。</p>
<p>Spark 的 stage 以 RDD 的 shuffle 为界进行划分。窄依赖的 RDD 操作会被穿起来放到一个 task 中，比如 <code>map()</code>, <code>filter()</code> 这样的操作。但是需要使用到 shuffle 依赖的操作，需要多个 stage（至少一个将中间文件写到特定的地方，另外一个从特定的地方进行读取）。每个 Stage，只会对其他 Stage 有 shuffle 依赖，在同一个 stage 中会进行很多计算。实际的将计算串起来的操作在 RDD.compute 中完成。</p>
<p><code>DAGScheduler</code> 同样会基于缓存状态决定 task 希望运行在那（preferred location），如果 shuffle 输出文件丢失造成的 Stage 失败，会重新被提交。在 <strong>Stage 内部</strong> 的不是由 shuffle 文件丢失造成的失败，由 <code>TaskScheduler</code> 来完成，<code>TaskScheduler</code> 会在取消整个 stage 前进行小部分重试。<br><a id="more"></a><br>下面的几个核心概念：</p>
<ul>
<li>Jobs (通过 <code>ActiveJob</code> 来表示）是提交给调度器中最上次工作单元。比如，用户触发一次 action，比如 <code>count()</code>，的时候，就会提交一个 Job。每个 Job 可能会包含多个 stage</li>
<li>Stages 是 Job 中产生中间结果的一系列 Task 的集合，同一个 Stage 的每个 Task 运行着同样的逻辑，只是处理同一个 RDD 的不同分区。Stage 以 Shuffle 为界（后面的 Stage 必须等前面的 Stage 运行完才能继续往下进行）。现在有两种 Stage：<code>ResultStage</code>，Job 的最终执行 action 的 Stage，<code>ShuffleMapState</code> 产生中间文件的 shuffle。如果多 Job 公用同一个 RDD 的话，Stage 可能会在多个 Job 间共享。</li>
<li>Tasks 是组小的独立工作单元，每个 Task 会被独立的分发到具体的机器上运行</li>
<li>Cache tracking: <code>DAGScheduler</code> 会记录哪些 RDD 以及被缓存过，从而避免重复计算，也会记录哪些 Stage 已经生成过输出文件从而避免重复操作</li>
<li>Preferred locations: <code>DAGScheduler</code> 会根据计算所依赖的 RDD 数据、缓存的地址以及 shuffle 输出结果对 Task 进行调度</li>
<li>Cleanup: 如果上游依赖的 Job 处理完成后，下游的数据会被清理掉，防止长驻服务的内存泄漏</li>
</ul>
<p>为了能够从错误中进行恢复，同一个 Stage 可能会被运行多次，每一次就是一个 “attempts”。如果 <code>TaskScheduler</code> 汇报某个 task 失败的原因是因为依赖的前一个 Stage 的输出文件已经不见了，那么 <code>DAGScheduler</code> 会对前一个 Stage 重新进行提交。这通过 <code>CompletionEveent</code> 以及 <code>FetchFailed</code> 或者 <code>ExecutorLost</code> 事件来完成。<code>DAGScheduler</code> 会等待一小段时间来判断是否还有其他 task 需要重试，然后将所有失败的 task 进行重试。在这一过程中，我们需要重新对之前清理过的 Stage 进行计算。由于之前的 “attempt” 可能还在运行，所以需要特别注意</p>
<img src="/2017/10/16/spark-dagscheduler/procesure.jpg" alt="procesure.jpg" title="">
<p>上面的图是 DAGScheduler.scala 的主题脉络，当然还包括其他诸如 <code>taskStarted</code>, <code>taskGettingResult</code>, <code>taskEnded</code>, <code>executorZHeatbeatReceived</code>, <code>executorLost</code>, <code>executorAdded</code>, <code>taskSetFailed</code>, 等函数</p>
<blockquote>
<p>其中带箭头的虚线为消息调用；带箭头的实线为直接调用，无箭头的实线表示方法申明和内部的主要实现（比如 <code>getShuffleMapStage</code> 中包括了 <code>getAncestorShuffleDependencies</code> 和 <code>newOrUsedShuffleStage</code>)</p>
</blockquote>
<h4 id="上面是注释，下面是代码解释"><a href="#上面是注释，下面是代码解释" class="headerlink" title="上面是注释，下面是代码解释"></a>上面是注释，下面是代码解释</h4><p>入口在 <code>runJob</code>，<code>runJob</code> 会调用 <code>submitJob(rdd, func, partitions, callSite, resultHandler, properties)</code> 返回一个 waiter，等待处理完成</p>
<p><code>submitJob</code> 核心代码如下，主要生成一个 waiter 对象，然后发送一个 <code>JobSubmitted</code> 信号</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">val func2 = func.asInstanceOf[(TaskContext, Iterator[_]) =&gt; _]</div><div class="line">val waiter = new JobWaiter(this, jobId, partitions.size, resultHandler)</div><div class="line">eventProcessLoop.post(JobSubmitted(jobId, rdd, func2, partitions.toArray, callSite, waiter, SerializationUtils.clone(properties)))</div></pre></td></tr></table></figure>
<p><code>JobSubmitted</code> 方法会将整个 DAG 图进行 Stage 的划分，然后提交 finalStage（也就是 action 所在的 Stage），其中 <code>newResultStage</code> 会进行具体的 Stage 划分，</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">try &#123;</div><div class="line">	// New stage creation may throw an exception if, for example, jobs are run on a</div><div class="line">	// HadoopRDD whose underlying HDFS files have been deleted.</div><div class="line">	finalStage = newResultStage(finalRDD, func, partitions, jobId, callSite)</div><div class="line">&#125; catch &#123;</div><div class="line">	case e: Exception =&gt;</div><div class="line">		logWarning(&quot;Creating new stage failed due to exception - job: &quot; + jobId, e)</div><div class="line">		listener.jobFailed(e)</div><div class="line">		return</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><code>submitJob</code> 返回的 <code>JobWaiter</code>，JobWaiter 用于控制 Job，以及 Job 结束后进行相应的状态更新</p>
<p><code>newResultStage</code> 会将所有的 Stage 划分出来（通过 <code>getParentStagesAndId</code> 函数，<code>getParentStages</code> 进行具体的 Stage 划分），其中 <code>getParentStages</code> 进行 BFS 进行查找（这个地方 BFS 和 DFS 有什么区别？）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">private def getParentStages(rdd: RDD[_], firstJobId: Int): List[Stage] = &#123;</div><div class="line">	val parents = new HashSet[Stage]</div><div class="line">	val visited = new HashSet[RDD[_]]</div><div class="line">	// We are manually maintaining a stack here to prevent StackOverflowError</div><div class="line">	// caused by recursively visiting</div><div class="line">	val waitingForVisit = new Stack[RDD[_]]</div><div class="line">	def visit(r: RDD[_]) &#123;</div><div class="line">		if (!visited(r)) &#123;</div><div class="line">			visited += r</div><div class="line">			// Kind of ugly: need to register RDDs with the cache here since</div><div class="line">			// we can&apos;t do it in its constructor because # of partitions is unknown</div><div class="line">			for (dep &lt;- r.dependencies) &#123;</div><div class="line">				dep match &#123;</div><div class="line">					case shufDep: ShuffleDependency[_, _, _] =&gt;</div><div class="line">						parents += getShuffleMapStage(shufDep, firstJobId)</div><div class="line">					case _ =&gt;</div><div class="line">						waitingForVisit.push(dep.rdd)</div><div class="line">				&#125;</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">	waitingForVisit.push(rdd)</div><div class="line">	while (waitingForVisit.nonEmpty) &#123;</div><div class="line">		visit(waitingForVisit.pop())</div><div class="line">	&#125;</div><div class="line">	parents.toList</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>getParentStages 返回某个 RDD 的所有依赖的 stage（直接和间接的），stage 以 getShuffleMapStage() 返回为准</p>
<p><code>getShuffleMapStage</code> 首先从 shuffleToMapStage(shuffleid 到 stage 的 map 结构）中查找，没有找到就以 shuffleDep.rdd 为起始点建立一个依赖关系，并且将整条依赖链上的东西都建立起来</p>
<p><code>getAncestorShuffleDependencies</code> 会以一个 RDD 为起点，找到 RDD 直接&amp;间接 依赖的所有 shuffleDependency</p>
<p><code>getAncestorShuffleDependencies</code> 和  <code>getParentStages</code> 类似但又不一样，前者在搜索的时候，每次都需要把 rdd.dep 入队，而后者只需要将 narrowdependency 的入队。还有为什么两个函数一个结果保存为 Set，一个是 Stack？</p>
<p><code>newShuffleMapStage</code> 中会更新 job 和 stage，以及 stage 和 job 的关系，每个 stage 属于哪些 job，每个 job 包含哪些 stage</p>
<p>整个类中有一个变量 <code>mapOutputTracker :MapOutputTracker</code> 用于记录 shuffle 的结果以及位置</p>
<p><code>MapOutputTracker</code> 记录 shuffleMapStage 的 output location  ，为啥要两个 status map（一个 MapStatus，一个 CachedSerializedStatus，后者是前者的序列化之后的结果）,这些 Map 都是带 ttl 的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">/* org.apache.spark.MapOutputTracker.scala</div><div class="line">* Class that keeps track of the location of the map output of</div><div class="line">* a stage. This is abstract because different versions of MapOutputTracker</div><div class="line">* (driver and executor) use different HashMap to store its metadata.</div></pre></td></tr></table></figure>
<p>Driver 使用 MapOutputTrackerMaster 跟踪 output location，只有所有 partition 的 output location 都就绪了，整个被依赖的 RDD 才是就绪的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">* MapOutputTracker for the driver. This uses TimeStampedHashMap to keep track of map</div><div class="line">* output information, which allows old output information based on a TTL.</div></pre></td></tr></table></figure>
<p>Executor 则使用 <code>MapOutputTrackerWorker</code> 从 Driver 获取 map output 的相应信息</p>
<p><code>newOrUsedShuffleStage</code> 函数中首先查找该 shuffleMapStage 是否注册过 MapOutputTracker，如果注册过就直接获取，如果没有注册过就进行注册。<br>MapOutputTracker 有两个 Map 结构，一个是原始的 partition location，一个是序列化之后的（用于加速？），这两个 map 包含过期清理策略，用于节省空间</p>
<p>定期清理的 Meta 信息包括如下几种：</p>
<ul>
<li>MAP_OUTPUT_TRACKER, </li>
<li>SPARK_CONTEXT, </li>
<li>HTTP_BROADCAST, </li>
<li>BLOCK_MANAGER,</li>
<li>SHUFFLE_BLOCK_MANAGER, </li>
<li>BROADCAST_VARS</li>
</ul>
<h3 id="上面是一条链路上的相关函数，下面包括一些其他的处理"><a href="#上面是一条链路上的相关函数，下面包括一些其他的处理" class="headerlink" title="上面是一条链路上的相关函数，下面包括一些其他的处理"></a>上面是一条链路上的相关函数，下面包括一些其他的处理</h3><h4 id="handleTaskCompletion-处理-Task-comple-的信息（不分成功和失败）"><a href="#handleTaskCompletion-处理-Task-comple-的信息（不分成功和失败）" class="headerlink" title="handleTaskCompletion 处理 Task comple 的信息（不分成功和失败）"></a><code>handleTaskCompletion</code> 处理 Task comple 的信息（不分成功和失败）</h4><p>task complete 会分几种信息：</p>
<ul>
<li><p>Success：<br>首先将 task 从 pendingTask 中去掉<br>  task 分为两种：</p>
<ul>
<li>ResultTask：<br>将 job 对应的当前 task 标记为 true（如果没有标记过的话），如果整个 job 都处理完成，就将 stage 标记为完成</li>
<li><p>ShuffleMapTask：<br>更新 outputLocation （当前 shuffleMapTask 的输出）<br>如果 ShuffleMapTask 的所有 partition 都处理完成，就将当前的 stage 标记为完成<br>这里为了防止是重复进行计算（之前失败过），需要重新进行 outputLocation 的注册（主要是增加 epoch）<br>如果当前当前的 Stage.isAvailable 为 true 就通知所有依赖该 stage 的 stage 可以继续工作了。否则重新提交失败的 task（注意这里available 和上面的 finish 不一样，available 是以 output 是否能够获取到为准）</p>
</li>
<li><p>Resubmitted：<br>将 task 加到 pendingTask 中即可，等待后续的调度</p>
</li>
<li><p>FetchFailed：<br>首先判断失败 task 的 attemp 是否是当前的 attemp，不是就忽略，然后判断当前 stage 是否正在运行，如果不是，忽略。<br>否则将当前的 stage 标记为 finish，然后将进行 mapStage.removeOutputLoc 以及 mapOutputTracker.unregisterMapOutput。<br>如果同一个 executor 上的 fetchFailed 很多（这个有调用方判断），则将所在的 executor 标记为 Failed</p>
</li>
<li><p>其他信息，直接忽略</p>
</li>
</ul>
</li>
</ul>
<h4 id="ExecutorLost"><a href="#ExecutorLost" class="headerlink" title="ExecutorLost"></a>ExecutorLost</h4><p>如果上报的 executor 之前没有上报过（会有一个 Map 记录所有上报过的 executor），或者之前上报的该 executor 对应的 epoch 小于 currentepoch， 则需要进行处理</p>
<p>首先从 blockManagerMaster 中将当前 executor 删除</p>
<p>如果没有启动 externalShuffleService（开启 <code>DynamicAllocation</code> 后需要开启，不然会出问题），或者 fetchFailed（由调用方设置），则进行下面的操作：</p>
<p><code>if (!env.blockManager.externalShuffleServiceEnabled || fetchFailed)</code></p>
<p>所该 executor 上所有的输出都进行标记删除，并且增加 <code>mapOutputTracker</code> 的 epoch</p>
<h4 id="ExecutorAdded"><a href="#ExecutorAdded" class="headerlink" title="ExecutorAdded"></a>ExecutorAdded</h4><p>  如果当前添加的 executor 是马上需要回收的，那么就从即将回收的 map 中删除，防止回收，否则不需要操作</p>
<h4 id="StageCancellation"><a href="#StageCancellation" class="headerlink" title="StageCancellation"></a>StageCancellation</h4><p>  如果有正在运行的 job 依赖当前 stage，则将所有的 job 标记为 cancel</p>
<h4 id="JobCancellation"><a href="#JobCancellation" class="headerlink" title="JobCancellation"></a>JobCancellation</h4><p>  将该 job 以及只由该 job 依赖的 stage 都标记为 failed</p>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><p>  <code>spark.stage.maxConsecutiveAttempts</code> 表示一个 stage 尝试多少次之后会被标记为失败</p>
<p>  SparkContext 中会根据模式生成和注册相应的 backend 以及 taskscheduler</p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><ol>
<li>如果一个 Stage 有多个 RDD，那么这些 RDD 是在同一个 TaskSet 中吗</li>
<li>如何模拟 <code>r2 = r0.reduceByKey; r3 = r1.reduceByKey; r4 = r2.map(xx); r5 = r4.union(r3); r6 = r5.map; r7 = r6.reduceByKey</code> 的 Stage 划分和生成（会有多少个 Stage，每个 Stage 分别包含哪些 RDD，以及整个 DAG 怎么整合起来的）</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;基于 spark 1.6&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;面向 Stage 的调度器，负责计算每个 job 的 DAG，并将 DAG 图划分为不同的 stage，对哪些 RDD 以及相应输出进行记录，寻找一个运行相应 job 所需要的最小 stage。然后将 &lt;code&gt;stage&lt;/code&gt; 以 &lt;code&gt;TaskSet&lt;/code&gt; 的形式提交到下层的 &lt;code&gt;TaskScheduler&lt;/code&gt; 进行具体的 task 调度。每个 &lt;code&gt;TaskSet&lt;/code&gt; 包含整个可以独立运行的 task，这些 task 能够利用集群上已有的数据立即运行起来，如果集群上已有的数据已经不存在了，那么当前 task 就会失败。&lt;/p&gt;
&lt;p&gt;Spark 的 stage 以 RDD 的 shuffle 为界进行划分。窄依赖的 RDD 操作会被穿起来放到一个 task 中，比如 &lt;code&gt;map()&lt;/code&gt;, &lt;code&gt;filter()&lt;/code&gt; 这样的操作。但是需要使用到 shuffle 依赖的操作，需要多个 stage（至少一个将中间文件写到特定的地方，另外一个从特定的地方进行读取）。每个 Stage，只会对其他 Stage 有 shuffle 依赖，在同一个 stage 中会进行很多计算。实际的将计算串起来的操作在 RDD.compute 中完成。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;DAGScheduler&lt;/code&gt; 同样会基于缓存状态决定 task 希望运行在那（preferred location），如果 shuffle 输出文件丢失造成的 Stage 失败，会重新被提交。在 &lt;strong&gt;Stage 内部&lt;/strong&gt; 的不是由 shuffle 文件丢失造成的失败，由 &lt;code&gt;TaskScheduler&lt;/code&gt; 来完成，&lt;code&gt;TaskScheduler&lt;/code&gt; 会在取消整个 stage 前进行小部分重试。&lt;br&gt;
    
    </summary>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="source_code" scheme="http://yoursite.com/tags/source-code/"/>
    
      <category term="dagscheduler" scheme="http://yoursite.com/tags/dagscheduler/"/>
    
  </entry>
  
  <entry>
    <title>GC 标记-清除算法</title>
    <link href="http://yoursite.com/2017/09/17/GC-%E6%A0%87%E8%AE%B0-%E6%B8%85%E9%99%A4%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2017/09/17/GC-标记-清除算法/</id>
    <published>2017-09-17T00:51:40.000Z</published>
    <updated>2017-09-17T12:36:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>GC 的世界中有三种基本的算法，分别是：</p>
<ul>
<li>标记清除</li>
<li>引用计数</li>
<li>GC 复制</li>
</ul>
<p>其他的 GC 算法都是在这三种算法上进行修改，优化得来。本文将要介绍的是 标记-清除 算法。</p>
<a id="more"></a>
<h1 id="标记清除算法介绍"><a href="#标记清除算法介绍" class="headerlink" title="标记清除算法介绍"></a>标记清除算法介绍</h1><p>像字面意思一样，标记-清除算法分为两步：标记和清除。其中标记和清除使用伪代码分别可以写出来如下：</p>
<p>标记</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">mark_phase() &#123;</div><div class="line">    for (r: $roots)</div><div class="line">       mark(*r)</div><div class="line">&#125;</div><div class="line"></div><div class="line">mark(obj)&#123;</div><div class="line">    if(obj.mark == FALSE)</div><div class="line">        obj.mark == TRUE</div><div class="line">        for(child: children(obj))</div><div class="line">            mark(*child)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>清除</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">sweep_phase() &#123;</div><div class="line">    sweeping = $head_start    //从堆头开始清除</div><div class="line">    while (sweeping &lt; $head_end)</div><div class="line">        if (sweeping.mark == TRUE) //如果当前对象是活跃对象</div><div class="line">            sweeping.mark = FALSE</div><div class="line">        else  //如果当前对象是可清除对象，则将其加入到 free_list 中</div><div class="line">            sweeping.next = $free_list   </div><div class="line">            $free_list = sweep</div><div class="line">        sweeping += sweeping.size</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>对于标记清除算法，最基本的，将某个对象是否已经被标记，以及对象的大小等等这些信息都保存在头部，从上面的代码中，我们可以知道头部至少有三个域：标记位(名为 mark)、对象大小（名为 size）以及下一个对象的头部地址（名为 next）– 有 size 和 next 两个域，是因为下一个对象不一定在物理上是连续的。如下图所示</p>
<img src="/2017/09/17/GC-标记-清除算法/heap.png" alt="heap.png" title="">
<p>其中淡蓝色的表示还在使用的空间，白色的表示空闲空间。</p>
<h2 id="标记算法的选取"><a href="#标记算法的选取" class="headerlink" title="标记算法的选取"></a>标记算法的选取</h2><p>在标记部分，我们从 root 节点触发，然后逐一标记哪个节点不再使用，哪些节点还需要在继续存活。在上述代码中，我们给出的是 DFS 搜素算法，那么对于这一块我们可以比较 DFS 和 BFS 的区别，大致可以用下图表示：</p>
<img src="/2017/09/17/GC-标记-清除算法/BFS_DFS.jpeg" alt="BFS_DFS.jpeg" title=""> 
<p>主要区别在于：<strong>DFS 需要保存的内存使用量更低</strong></p>
<h2 id="分配内存"><a href="#分配内存" class="headerlink" title="分配内存"></a>分配内存</h2><p>所有的标记、清除等都是为了分配内存服务的，如果我们不需要再分配内存的话，那么就可以不进行前面哪些活动了。下面说说如何分配内存的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">new_obj(size) &#123;</div><div class="line">    chunk = pickup_chunk(size, $free_list) // 从上面的空闲列表中找一个合适的内存块</div><div class="line">    if (chunk != NULL)</div><div class="line">        return chunk</div><div class="line">    else</div><div class="line">        allocation_fail() //内存不够</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上面的代码表示整个分配内存的过程，其中 <code>pickup_chunk</code> 表示从空闲列表中找出一个合适的内存块，返回给申请者。</p>
<p>对于找出一个 <strong>合适</strong> 的内存块，我们已知的至少有三种方法：</p>
<ul>
<li>First-fit 返回最先找到的一个内存块</li>
<li>Best-fit  找到一个大于需要内存的最小内存块</li>
<li>Worst-fit 找到一个大于需要内存的最大内存块</li>
</ul>
<p>上面三种每一种都有不同的应用场景，也各有优劣。</p>
<h2 id="合并"><a href="#合并" class="headerlink" title="合并"></a>合并</h2><p>对于清除阶段，我们会遍历堆中所有的内存区域，将不需要的加入到 <code>free_list</code> 中，对于连续出现的两个内存块，我们并没有进行任何操作，那么下一次我们申请内存的时候，可能会由于没有足够大的内存块而失败。比如我们有两个大小分别为 3，4 的内存块，然后，我们需要申请一个内存大小为 5 的内存块，在之前的算法中是会失败的。这就牵涉到清除阶段的合并了，将连续的内存块进行合并形成大的内存块。上面的清除阶段算法改成如下所示</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">sweep_phase() &#123;</div><div class="line">    sweeping = $head_start    //从堆头开始清除</div><div class="line">    while (sweeping &lt; $head_end)</div><div class="line">        if (sweeping.mark == TRUE) //如果当前对象是活跃对象</div><div class="line">            sweeping.mark = FALSE</div><div class="line">        else  //如果当前对象是可清除对象，则将其加入到 free_list 中</div><div class="line">            if(sweeping == $free_list + $free_list.size) //邻接块的合并</div><div class="line">                $free_list.size + sweeping.size</div><div class="line">            else</div><div class="line">                sweeping.next = $free_list</div><div class="line">                $free_list = sweep</div><div class="line">        sweeping += sweeping.size</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h1><p>基本算法基本描述完成，接下来可以看看这种算法的优缺点分别是什么，以及是否有办法进行优化</p>
<p>优点：</p>
<ol>
<li>实现简单。算法简单，能够明显知道是否有问题，而且更容易和其他算法进行结合</li>
<li>与保守式 GC 算法兼容。在 保守式 GC 算法中，对象是不能被移动的，刚好 GC 标记-清除算法不需要移动对象。</li>
</ol>
<p>缺点：</p>
<ol>
<li>碎片化。在标记-清除的过程中，会不断的产生碎片，虽然在清除阶段有合并过程，但还是不够。</li>
<li>分配速度。由于在 标记-清除 算法 中分块不是连续的，因此每次分配都需要遍历空闲列表，找到足够大的分块，最坏的情况需要每次都遍历整个空闲列表。</li>
<li>与写时复制技术不兼容。标记-清除算法中的标记阶段，会修改对象的头部，因此和写时复制技术不兼容，可能导致很多不必要的复制。</li>
</ol>
<h1 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h1><p>既然我们了解到了该算法的优缺点，那么在此基础上如何进行改进呢？</p>
<p>针对上面的缺点分别有如下几点改进</p>
<h2 id="多个空闲链表"><a href="#多个空闲链表" class="headerlink" title="多个空闲链表"></a>多个空闲链表</h2><p>简单的说，就是将原来一个空闲链表变成现在的多个空闲链表，加快分配的速度。<br>我们之前分配内存的时候，需要查询整个空闲链表来寻找一个合适的内存块，现在我们将不同大小的内存块挂在不同的链表下面，这样我们就能够直接去合适的链表中查找了。如下图所示</p>
<img src="/2017/09/17/GC-标记-清除算法/multilink.jpeg" alt="multilink.jpeg" title="">
<p>我们有用于 2 个字的空闲链表，有用于 3 个字的空闲链表。这样当需要分配 3 个字的空间时，我们直接去对应的链表查找即可。</p>
<p>这里有一个注意的点，需要保持多少个链表呢？一般来说，根据经验，一般会将小于某个阈值的分别生成一个链表，大于该阈值的所有内存块放到一个链表中。比如所有小于 100 字的都有一个单独的链表，而所有大于等于 100 字的内存块都挂在 100 字这个链表下。</p>
<h2 id="BiBOP-法"><a href="#BiBOP-法" class="headerlink" title="BiBOP 法"></a>BiBOP 法</h2><p>另外一种优化方法，就是将大小相近的内存块放到一起，如下图所示</p>
<img src="/2017/09/17/GC-标记-清除算法/bibop.jpeg" alt="bibop.jpeg" title="">
<p>这样的形式，我们也知道去哪个地方查找需要的内存块，但是这个方法有一点不好，就是会形成很多内存碎片，比如我们分配的很多大小为 2 个字的空间，但是所有的申请中最小的都是 3 个字，这个时候这些 2 个字的空间都是浪费的</p>
<h2 id="位图标记"><a href="#位图标记" class="headerlink" title="位图标记"></a>位图标记</h2><p>位图标记法主要改善的是“和 copy-on-write 技术不兼容”，将标记位从头部抽离出来了。如下图所示</p>
<img src="/2017/09/17/GC-标记-清除算法/bitmap.jpeg" alt="bitmap.jpeg" title="">
<p>这样标记的时候就不会修改头部位置了。伪代码大致如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">mark(obj) &#123;</div><div class="line">    obj_num = (obj - $heap_start) / WORD_LENGTH</div><div class="line">    index = obj_num / WORD_LENGTH</div><div class="line">    offset = obj_num % WORD_LENGTH</div><div class="line">    </div><div class="line">    if (($bitmap_tbl[index] &amp; (1 &lt;&lt; offset)) == 0) // 未被标记</div><div class="line">        $bitmap_tbl[index] |= (1 &lt;&lt; offset)</div><div class="line">        for (child: children(obj))</div><div class="line">            mark(*child)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<blockquote>
<p>在参考条目[1] 中有描述这个算法的第二个优势：清除操作更高效。描述如下：以往的清除操作都必须遍历整个堆，把非活动对象连接到空闲链表，同时取消活动对象的标志位。</p>
</blockquote>
<p>我的理解现在还是需要遍历整个堆，而且需要取消标志位，只是现在标志位变成了连续的，这样处理起来会高效一点。</p>
<p>另外，如果有多个堆的情况下，就需要多个 bitmap 了</p>
<h2 id="延迟清除法"><a href="#延迟清除法" class="headerlink" title="延迟清除法"></a>延迟清除法</h2><p>在清除操作中，我们需要遍历整个堆，也就是处理时间和堆的大小成正比。在清除阶段，内存空间不能访问，这就牵涉到最大暂停时间。而延迟清除法（lazy sweep）则可以缩短清除操作导致的最大暂停时间。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">new_obj(size) &#123;</div><div class="line">    chunk = lazy_sweep(size) //先通过延迟清除法，查找是否以满足条件的内存块</div><div class="line">    if (chunk != NULL)</div><div class="line">        return chunk</div><div class="line"></div><div class="line">    mark_phase() //标记阶段</div><div class="line">    </div><div class="line">    chunk = lazy_sweep(size) //再次通过延迟清除法，寻找一个满足条件的内存块</div><div class="line">    if (chunk != NULL)</div><div class="line">        return chunk</div><div class="line">    </div><div class="line">    allocation_fail()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>至于 <code>lazy_sweep</code> 函数如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">lazy_sweep(size) &#123;</div><div class="line">    while($sweeping &lt; $headp_end)</div><div class="line">        if ($sweeping.mark == TRUE) //TRUE 表示活动对象</div><div class="line">            $sweeping.mark = FALSE</div><div class="line">        else if ($sweeping.size &gt;= size)</div><div class="line">            chunk = $sweeping</div><div class="line">            $sweeping += $sweeping.size</div><div class="line">            return chunk</div><div class="line">        $sweeping += $sweeping.size</div><div class="line">    $sweeping = $head_start</div><div class="line">    return NULL</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>注意，其中的 <code>sweeping</code> 时候全局变量，因此遍历的起始位置是上次结束的地方。</p>
<p>延迟清除法有一个缺点就是。如果空闲内存块和活动对象块基本分成两个大的部分的话（如下图所示），那么对于某次清除活动对象周围的空间时，就会增加暂停时间。</p>
<img src="/2017/09/17/GC-标记-清除算法/lazy-sweep.png" alt="lazy-sweep.png" title="">
<p>上图中淡蓝色的表示活动对象，白色的表示空闲对象，当清除到活动对象附近的时候，就会增加本次的最大暂停时间。</p>
<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol>
<li>《垃圾回收的算法与实现》第二章</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;GC 的世界中有三种基本的算法，分别是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;标记清除&lt;/li&gt;
&lt;li&gt;引用计数&lt;/li&gt;
&lt;li&gt;GC 复制&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其他的 GC 算法都是在这三种算法上进行修改，优化得来。本文将要介绍的是 标记-清除 算法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="GC" scheme="http://yoursite.com/tags/GC/"/>
    
      <category term="java" scheme="http://yoursite.com/tags/java/"/>
    
      <category term="标记-清除" scheme="http://yoursite.com/tags/%E6%A0%87%E8%AE%B0-%E6%B8%85%E9%99%A4/"/>
    
      <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="总结" scheme="http://yoursite.com/tags/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>tmux 简单使用指南</title>
    <link href="http://yoursite.com/2017/07/14/tmux-%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"/>
    <id>http://yoursite.com/2017/07/14/tmux-简单使用指南/</id>
    <published>2017-07-14T06:21:37.000Z</published>
    <updated>2017-07-14T09:37:38.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Tmux-的简单使用说明"><a href="#Tmux-的简单使用说明" class="headerlink" title="Tmux 的简单使用说明"></a>Tmux 的简单使用说明</h1><blockquote>
<p>工欲善其事，必先利其器</p>
</blockquote>
<p>Tmux 是一个多窗口管理程序。可以让用户在一个地方管理多个终端。而不需要在不同的终端间来回切换。</p>
<h2 id="在-Mac-下如何安装"><a href="#在-Mac-下如何安装" class="headerlink" title="在 Mac 下如何安装"></a>在 Mac 下如何安装</h2><p>直接使用 <code>brew install tmux</code> 就可以了，如果没有 brew，则需要先安装 brew，然后再执行上述命令。</p>
<h2 id="简单使用流程"><a href="#简单使用流程" class="headerlink" title="简单使用流程"></a>简单使用流程</h2><p>首先，需要了解 tmux 中的几个概念。session，window 以及 pane。这几者的关系如下，tmux 中可以起多个 session，每个 session 可以启动多个 window，然后每个 window 可以启动多个 pane。</p>
<p>这里给一个基本的流程</p>
<ol>
<li><p>启动 tmux（默认会启动一个 session）<br>使用 <code>tmux</code> 启动 tmux，使用 <code>exit</code> 退出 tmux，session 的命名默认是从 0 开始，一直往上加</p>
</li>
<li><p>在 session 中启动一个 window<br><code>PREFIX c</code> 会在当前 session 中创建一个 window, 其中 <code>PREFIX</code> 表示 tmux 中的命令前缀符（该条命令表示，先按下 PREFIX，然后按下 c)，</p>
</li>
<li><p>在启动的 window 中创建一个 pane<br><code>PREFIX %</code> 竖直方向切分一个 window，<code>PREFIX &quot;</code> 横向切分一个 window。这样就能够在 window 中创建 pane 了。基本的这些就够了。</p>
</li>
<li><p>如何在 session，window，pane 中进行移动<br>能够创建 session，window，pane 了，接下来就是如何在 session，window，pane 间进行移动了。<br><code>PREFIX s</code> 会列出所有 session，然后进行具体的选择（可以上下移动光标，然后按 ENTRER 确定）<br><code>PREFIX w</code> 可以列出所有的 window，然后进行具体的筛选<br><code>PREFIX n</code> 可以切换到下一个 window<br><code>PREFIX p</code> 可以切换到上一个 window<br><code>PREFIX &amp;</code> 可以关闭当前 window<br><code>PREFIX o</code> 可以在 pane 之间进行跳转<br><code>tmux ls</code> 会列出当前所有的 session（在非 tmux 环境下）</p>
</li>
</ol>
<h2 id="自定义-tmux"><a href="#自定义-tmux" class="headerlink" title="自定义 tmux"></a>自定义 tmux</h2><p>tmux 的配置文件可以保存在两个地方</p>
<ol>
<li>/etc/tmux.conf</li>
<li>~/.tmux.conf</li>
</ol>
<p>其中 2 的优先级会更高，1 的影响面更广</p>
<h2 id="接下来做什么"><a href="#接下来做什么" class="headerlink" title="接下来做什么"></a>接下来做什么</h2><p>上面的仅仅是一个入门文档，也就是最少基本知识，接下来就是多实践。推荐一本小书《tmux productive mouse-free development》</p>
<img src="/2017/07/14/tmux-简单使用指南/tmux_pic.png" alt="tmux_pic.png" title="">
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Tmux-的简单使用说明&quot;&gt;&lt;a href=&quot;#Tmux-的简单使用说明&quot; class=&quot;headerlink&quot; title=&quot;Tmux 的简单使用说明&quot;&gt;&lt;/a&gt;Tmux 的简单使用说明&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;工欲善其事，必先利其器&lt;/p&gt;
&lt;
    
    </summary>
    
    
      <category term="tmux, tools" scheme="http://yoursite.com/tags/tmux-tools/"/>
    
  </entry>
  
  <entry>
    <title>风险不仅仅是事件发生的概率</title>
    <link href="http://yoursite.com/2017/06/20/%E9%A3%8E%E9%99%A9%E4%B8%8D%E4%BB%85%E4%BB%85%E6%98%AF%E4%BA%8B%E4%BB%B6%E5%8F%91%E7%94%9F%E7%9A%84%E6%A6%82%E7%8E%87/"/>
    <id>http://yoursite.com/2017/06/20/风险不仅仅是事件发生的概率/</id>
    <published>2017-06-20T14:05:59.000Z</published>
    <updated>2017-06-20T14:57:02.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="风险不仅仅是事件发生的概率"><a href="#风险不仅仅是事件发生的概率" class="headerlink" title="风险不仅仅是事件发生的概率"></a>风险不仅仅是事件发生的概率</h1><blockquote>
<p>风险可以定义为 = 事件结果对你的影响 * 事件发生的概率</p>
</blockquote>
<p>风险在生活中处处存在，可能我们会想冒个险没啥关系，反正发生的概率小，而且在某些时候会有高收益/回报伴随这风险，这个时候就更有诱惑力了，总有人希望通过冒险得到高回报，但这恰恰是不可取的，是非常危险的。</p>
<a id="more"></a>
<h2 id="到底能不能闯红灯"><a href="#到底能不能闯红灯" class="headerlink" title="到底能不能闯红灯"></a>到底能不能闯红灯</h2><blockquote>
<p>如果 遇到/见到过一次严重的交通事故，产生过后怕的感觉，那么就再也不会在这件事上冒险了</p>
</blockquote>
<pre><code>当你你急着过马路，但是现在是红灯，你会选择等待吗？如果这个时候旁边有人正在闯红灯，你还会等待吗？
</code></pre><p>上面的问题只能自己回答自己，回答很容易，但是实践起来就不那么容易了。</p>
<p>闯红灯是生活中很常见的事情，但是却隐藏了非常大的安全隐患。因为一旦发生交通事故，对自己来说将是不能承担的后果。</p>
<p>有人会问：那绿灯的时候就不会发生交通事故了吗？绿灯照样可能发生交通事故啊。</p>
<p>是的，这句话没问题，绿灯同样可能发生交通事故，但是有两点：</p>
<ol>
<li>闯红灯发生交通事故的责任怎么算</li>
<li>闯红灯和不闯红灯发生交通事故的概率比</li>
</ol>
<p>不闯红灯，并不能完全避免交通事故的发生 – 意外并不能完全避免，只能尽量降低其发生的可能性 – 只是将可能性降低而已，因为这事的后果是不能承担的。</p>
<h2 id="投资要不要冒险"><a href="#投资要不要冒险" class="headerlink" title="投资要不要冒险"></a>投资要不要冒险</h2><p>不同的投资标的，风险和回报率是不一样的。每个人能承受的风险能力也是不一样的。</p>
<pre><code>投资一百万，到底算不算冒险？
</code></pre><p>这是一个因人而异的问题，只有自己能回答，因为每个人的资金大小不一样，所以一百万对每个人的意义是不一样的。</p>
<p>大家都想挣钱，甚至挣快钱，这都没有问题，不过要考虑了解两点：</p>
<ol>
<li>高回报率可能伴随这高风险，这个风险是自己能承受的吗？</li>
<li>快速的挣到一大笔钱后，自己有足够的心里能力可以承受吗？ – 这一点是很重要，很重要，很重要，但是很难用语言描述清楚。短时间内得到或者失去一大笔钱对一个人的心里都会造成很大的影响。</li>
</ol>
<p>还有就是，投资基本做不到百分之一百的有把握，所以不要 all in。all in 只要失败一次就全跪了。</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ol>
<li>风险真的是风险吗？是自己真正思考过后的风险，还是听别人说的风险？</li>
<li>知道有风险的存在，可以怎么利用风险吗？</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;风险不仅仅是事件发生的概率&quot;&gt;&lt;a href=&quot;#风险不仅仅是事件发生的概率&quot; class=&quot;headerlink&quot; title=&quot;风险不仅仅是事件发生的概率&quot;&gt;&lt;/a&gt;风险不仅仅是事件发生的概率&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;风险可以定义为 = 事件结果对你的影响 * 事件发生的概率&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;风险在生活中处处存在，可能我们会想冒个险没啥关系，反正发生的概率小，而且在某些时候会有高收益/回报伴随这风险，这个时候就更有诱惑力了，总有人希望通过冒险得到高回报，但这恰恰是不可取的，是非常危险的。&lt;/p&gt;
    
    </summary>
    
      <category term="想清楚" scheme="http://yoursite.com/categories/%E6%83%B3%E6%B8%85%E6%A5%9A/"/>
    
    
      <category term="风险" scheme="http://yoursite.com/tags/%E9%A3%8E%E9%99%A9/"/>
    
      <category term="概率" scheme="http://yoursite.com/tags/%E6%A6%82%E7%8E%87/"/>
    
  </entry>
  
  <entry>
    <title>Streaming 程序调用 Producer.close hang 住问题追查复盘</title>
    <link href="http://yoursite.com/2017/06/03/Streaming-%E7%A8%8B%E5%BA%8F%E8%B0%83%E7%94%A8-Producer-close-hang-%E4%BD%8F%E9%97%AE%E9%A2%98%E8%BF%BD%E6%9F%A5%E5%A4%8D%E7%9B%98/"/>
    <id>http://yoursite.com/2017/06/03/Streaming-程序调用-Producer-close-hang-住问题追查复盘/</id>
    <published>2017-06-03T02:44:55.000Z</published>
    <updated>2017-06-03T03:43:27.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文作为一个问题追查过程的复盘记录，主要希望找出自己在解决问题中可以优化改进的地方。以后遇到问题，能够快速的进行定位，解决。</p>
<a id="more"></a>
<h2 id="问题现象"><a href="#问题现象" class="headerlink" title="问题现象"></a>问题现象</h2><p>一个 Spark Streaming 作业从 Kafka 消费数据，写往 ES，在 Spark Streaming 作业中会采集一些 metric 指标发往一个特定的 topic A。每次往 A 发送完数据后会调用 <code>producer.close()</code> 方法，看到的现象为：作业启动一段时间之后 hang 住，类似下图</p>
<img src="/2017/06/03/Streaming-程序调用-Producer-close-hang-住问题追查复盘/hang_job.jpg" alt="hang_job.jpg" title="">
<h2 id="排查问题的过程"><a href="#排查问题的过程" class="headerlink" title="排查问题的过程"></a>排查问题的过程</h2><ol>
<li>看到现象后，知道作业 hang 住了，希望能找到为什么 hang 住。找到该作业的 executor 地址（如下图所示）</li>
</ol>
<img src="/2017/06/03/Streaming-程序调用-Producer-close-hang-住问题追查复盘/executor.jpg" alt="executor.jpg" title="">
<p>然后登录到机器上，通过 lsof 查看对应的进程，再通过 jstack dump 出具体的线程栈信息。由于第一次解决线程 hang 住的问题，得到栈信息后，暂时无从下手，然后 google <code>jvm 线程 hang 住</code> 等关键词，检查死锁 – 发现没有。</p>
<p>发现线程有 <code>RUNNABLE</code>，<code>WAITING</code>，<code>TIMED_WAITING</code> 等状态，然后一个个查看这些状态分别代表啥意思。到这就不知道怎么继续了 – 中间在 Spark Streaming 微信群里请教各路大神，有人说遇到链接关不掉的情况 – 多次重复查看 jstack 出来的信息，发现有一个 WAITING 线程在等待锁，具体如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">java.lang.Thread.State: WAITING (on object monitor)</div><div class="line">        at java.lang.Object.wait(Native Method)</div><div class="line">        - waiting on &lt;0x00000000c52cfba0&gt; (a org.apache.kafka.common.utils.KafkaThread)</div><div class="line">        at java.lang.Thread.join(Thread.java:1281)</div><div class="line">        - locked &lt;0x00000000c52cfba0&gt; (a org.apache.kafka.common.utils.KafkaThread)</div><div class="line">        at java.lang.Thread.join(Thread.java:1355)</div><div class="line">        at org.apache.kafka.clients.producer.KafkaProducer.close(KafkaProducer.java:422)</div><div class="line">        at org.elasticsearch.hadoop.rest.KafkaProducer.close(DSLKafkaProducer.java:60)</div></pre></td></tr></table></figure>
<p>然后对照到代码，在 Producer.close() 中有一句代码如下 <code>ioThread.join()</code>，猜测是 ioThread 一直没有执行完毕导致的。</p>
<ol>
<li><p>注释掉 producer.close() 这一句代码之后，重新上线运行 Spark Streaming 作业，发现没有再次出现问题。大致确定问题出在 <code>producer.close()</code> 这里。但是不确定更深层次的问题是啥。期间猜测是由于 producer 发送数据的时候需要有 leader 确认（配置有关），然后将这个配置修改为无需 leader 确认立即返回，但是依然会导致作业 hang 住。然后阅读源码，发现 <code>producer.close()</code> 方法做了两件事：1）将还未发送出去的数据发送出去，2）等待正在发送的数据完成。暂时没有找到造成 <code>ioThread</code> 线程 hang 住的原因。暂时不知道具体 hang 住的地方在哪，至此暂时告一段落。</p>
</li>
<li><p>再次跟进该问题，尝试找出造成线程 hang 住的原因，尝试 jdb attach 到具体的线程。得到如下信息：</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&gt; thread 0x1</div><div class="line">kafka-producer-network-thread | producer-12[1] where</div><div class="line">	  [1] sun.nio.ch.EPollArrayWrapper.epollWait (native method)</div><div class="line">	  [2] sun.nio.ch.EPollArrayWrapper.poll (EPollArrayWrapper.java:269)</div><div class="line">	  [3] sun.nio.ch.EPollSelectorImpl.doSelect (EPollSelectorImpl.java:79)</div><div class="line">	  [4] sun.nio.ch.SelectorImpl.lockAndDoSelect (SelectorImpl.java:87)</div><div class="line">	  [5] sun.nio.ch.SelectorImpl.select (SelectorImpl.java:98)</div><div class="line">	  [6] org.apache.kafka.common.network.Selector.select (Selector.java:328)</div><div class="line">	  [7] org.apache.kafka.common.network.Selector.poll (Selector.java:218)</div><div class="line">	  [8] org.apache.kafka.clients.NetworkClient.poll (NetworkClient.java:192)</div><div class="line">	  [9] org.apache.kafka.clients.producer.internals.Sender.run (Sender.java:191)</div><div class="line">	  [10] org.apache.kafka.clients.producer.internals.Sender.run (Sender.java:135)</div><div class="line">	  [11] java.lang.Thread.run (Thread.java:745)</div></pre></td></tr></table></figure>
<p>到这里暂时不知道该怎么继续往下查了，知道在这里 hang 住了，但是暂时不知道怎么继续往下查，看着屏幕发呆，然后想着这个问题或许别人也遇到过，就从上面的 栈信息 中抽取一部分关键词进行 google，得到信息在 kafka 0.8.2.1 中 producer.close() 在某些情况下会 hang 住，详情参考 <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-19+-+Add+a+request+timeout+to+NetworkClient" target="_blank" rel="external">KIP-19</a>，在 kafka 0.9.0.0 中提供一个带超时的 close 方法进行修复。</p>
<h2 id="问题复盘"><a href="#问题复盘" class="headerlink" title="问题复盘"></a>问题复盘</h2><ol>
<li><p>在知道作业 hang 住的情况，又不了解相应调试的情况下，能否快速了解定位问题的方法，能否询问其他人快速的定位问题，或者如何通过搜索引擎快速的获取自己需要的知识。这里自己有个小私心 – 觉得这是测试的作业，想保留现场，通过自己的努力完全把问题解决，好提升自己的能力。另外自己如何在平时积累一些查问题的经验（这次发现官方文档真是个好东西）</p>
</li>
<li><p>通过微信群询问是一个方法，但是提问需要有技巧，要能够提炼出自己的问题，以及自己进行了哪些尝试，有什么思考，而不是做伸手党。</p>
</li>
<li>为什么到最后才想着 Google 相关信息，而不是在知道 producer.close() 导致作业 hang 住的时候就 Google 相关信息。</li>
<li>对 Java 排查问题的工具非常不熟练，在平时需要自己模拟各种 case 进行练手。jstack, jvisualvm, jdb 等都是第一次使用，这些工具需要在平时进行熟练。</li>
<li>对常见的库或通用的写法要有一定的了解，比如看到 <code>org.apache.kafka.common.network.Selector.poll</code> 是否能想到没有超时而导致一直 hang 住，这些平时需要积累（思考这个怎么积累？）</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文作为一个问题追查过程的复盘记录，主要希望找出自己在解决问题中可以优化改进的地方。以后遇到问题，能够快速的进行定位，解决。&lt;/p&gt;
    
    </summary>
    
      <category term="实时计算" scheme="http://yoursite.com/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
      <category term="复盘" scheme="http://yoursite.com/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/%E5%A4%8D%E7%9B%98/"/>
    
      <category term="problem_solve" scheme="http://yoursite.com/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/%E5%A4%8D%E7%9B%98/problem-solve/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="spark_streaming" scheme="http://yoursite.com/tags/spark-streaming/"/>
    
      <category term="kafka" scheme="http://yoursite.com/tags/kafka/"/>
    
      <category term="thinking" scheme="http://yoursite.com/tags/thinking/"/>
    
      <category term="problem_solve" scheme="http://yoursite.com/tags/problem-solve/"/>
    
  </entry>
  
  <entry>
    <title>如何在不重启 Spark Streaming 作业的情况下，增加消费的 topic</title>
    <link href="http://yoursite.com/2017/06/01/%E5%A6%82%E4%BD%95%E5%9C%A8%E4%B8%8D%E9%87%8D%E5%90%AF-Spark-Streaming-%E4%BD%9C%E4%B8%9A%E7%9A%84%E6%83%85%E5%86%B5%E4%B8%8B%EF%BC%8C%E5%A2%9E%E5%8A%A0%E6%B6%88%E8%B4%B9%E7%9A%84-topic/"/>
    <id>http://yoursite.com/2017/06/01/如何在不重启-Spark-Streaming-作业的情况下，增加消费的-topic/</id>
    <published>2017-06-01T15:22:51.000Z</published>
    <updated>2017-06-03T03:37:01.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>本文所有和 kafka 相关操作都基于 Direct 消费模式</p>
</blockquote>
<p>在 Spark Streaming 作业中，每个作业会消费一个或多个 topic，但是这些 topic 需要在作业启动之前确定好，在作业运行中不能进行调整，之前<a href="https://klion26.github.io/2016/11/01/spark-streaming-topic-partition-change-auto-adaptive/" target="_blank" rel="external">修改了源码</a>做到了自适应 topic partition 扩容的情况，但是无法动态调整消费的 topic。现在需要在不重启作业的情况下，动态调整消费的 topic。</p>
<a id="more"></a>
<h2 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h2><p>回顾之前自适应 partition 调整的方案，落到源码层面最终以 partition 为最小消费单元，而不是 topic。因此动态的调整消费的 topic 在理论上就是可行的 – 假设作业已经消费的 topic 为 A，在自适应 partition 扩容的时候，我们是增加了 A 的某些 partition，那么我们同样可以增加 B topic 的 partition，其中 B topic 是作业之前没有消费的。</p>
<p>动态调整 partition 的方案中，只需要将现在消费的 parition 数不断的对齐现在 kafka 上相应 topic 的 partition 数目即可。动态调整作业消费的 topic 则需要有一个地方存储作业消费的 topic 数目，然后将这个信息周期性的同步给作业即可 – 可以理解前者使用 kafka 作为存储介质，保存了 topic 的 partition 数目。</p>
<p>本方案中，选择 zookeeper 作业作为存储 topic 的介质。希望动态调整 topic 的时候，修改 zookeeper 中对应路径下的节点即可。然后作业定时的访问 zookeeper 的特定路径同步需要消费的 topic 数目即可。示意图如下</p>
<p><img src="https://c1.staticflickr.com/5/4275/34202432474_eb409ae6a5.jpg" alt=""></p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>方案确定了，直接修改下上次自适应 partition 扩容的代码即可 – 本方案只实现了增加 topic 的功能，当前消费的 topic 不会被删除，如果需要的话可以自行修改源码满足这一点。</p>
<p>在 <code>DirectKafkaInputStream</code> 的 <code>compute</code> 函数开始处添加如下逻辑</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">val topic = currentOffsets.head._1.topic</div><div class="line">var addedTopic : Set[String] = Set()</div><div class="line">val topics = getTopicsForJob()</div><div class="line">for (i &lt;- topics) &#123;</div><div class="line">      if (!i.equals(topic)) &#123;</div><div class="line">            addedTopic = addedTopic + i</div><div class="line">       &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">if (addedTopic.nonEmpty) &#123;</div><div class="line">     val topicLeaders = MTKafkaUtils.getPartitions(kafkaParams, addedTopic)</div><div class="line">     val largestOffset = MTKafkaUtils.getLeaderOffsets(kafkaParams, topicLeaders, OffsetRequest.LatestTime)</div><div class="line"></div><div class="line">     currentOffsets = currentOffsets ++ largestOffset</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>然后增加一个获取所有 topic 的函数，下面 Constants 包中使用了一些常量，自行替换即可</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">private def getTopicsForJob() : List[String] = &#123;</div><div class="line">        val jobName = SparkEnv.get.conf.get(Constants.JOB_PREFIXED_NAME_KEY)  //这个是提交 job 时添加的一个参数，用于区分每个作业，会当作 zk 中路径的一级</div><div class="line">        val zkHostPort: String =  &quot;xxxxxxxxx&quot;</div><div class="line">        val zkClient = new ZkClient(zkHostPort, Constants.DEFAULT_SESSION_TIMEOUT, Constants.DEFAULT_CONNECTION_TIMEOUT, new ZkOffsetSerializer) //ZkOffsetSerializer 自己实现了一个简单的序列化，反序列化类，就用了 String.getBytes 和 new String()</div><div class="line"></div><div class="line">        if (!zkClient.exists(s&quot;$&#123;Constants.ROOT_PATH_OF_MULTI_TOPIC_PER_JOB&#125;/$&#123;jobName&#125;&quot;)) &#123;</div><div class="line">									            ZkUtils.updatePersistentPath(zkClient, s&quot;$&#123;Constants.ROOT_PATH_OF_MULTI_TOPIC_PER_JOB&#125;/$&#123;jobName&#125;&quot;, s&quot;$&#123;jobName&#125;&quot;);</div><div class="line">														        &#125;</div><div class="line"></div><div class="line">        zkClient.getChildren(s&quot;$&#123;Constants.ROOT_PATH_OF_MULTI_TOPIC_PER_JOB&#125;/$&#123;jobName&#125;&quot;).asScala.toList</div><div class="line">&#125;</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;本文所有和 kafka 相关操作都基于 Direct 消费模式&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在 Spark Streaming 作业中，每个作业会消费一个或多个 topic，但是这些 topic 需要在作业启动之前确定好，在作业运行中不能进行调整，之前&lt;a href=&quot;https://klion26.github.io/2016/11/01/spark-streaming-topic-partition-change-auto-adaptive/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;修改了源码&lt;/a&gt;做到了自适应 topic partition 扩容的情况，但是无法动态调整消费的 topic。现在需要在不重启作业的情况下，动态调整消费的 topic。&lt;/p&gt;
    
    </summary>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="spark_streaming" scheme="http://yoursite.com/tags/spark-streaming/"/>
    
      <category term="kafka" scheme="http://yoursite.com/tags/kafka/"/>
    
      <category term="zookeeper" scheme="http://yoursite.com/tags/zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>从源码级别分析 metric-core 的抽样算法</title>
    <link href="http://yoursite.com/2017/05/29/%E4%BB%8E%E6%BA%90%E7%A0%81%E7%BA%A7%E5%88%AB%E5%88%86%E6%9E%90-metric-core-%E7%9A%84%E6%8A%BD%E6%A0%B7%E7%AE%97%E6%B3%95/"/>
    <id>http://yoursite.com/2017/05/29/从源码级别分析-metric-core-的抽样算法/</id>
    <published>2017-05-29T09:45:22.000Z</published>
    <updated>2017-06-03T03:37:50.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://metrics.dropwizard.io" target="_blank" rel="external">metric-core</a> 是一个 java metric 库，用于统计 JVM 层面以及 服务级别 的各种 metric 信息。其中 metric-core 是其核心模块，代码量不多，总共 44 个文件，5700 行左右代码（包括注释）。算是一个很小的开源项目了。由于 metric 在所有项目中都非常重要，因此选择通读该项目，本文分析 metrci-core 中的抽样算法。</p>
<a id="more"></a>
<h2 id="metric-core-中的抽样算法"><a href="#metric-core-中的抽样算法" class="headerlink" title="metric-core 中的抽样算法"></a>metric-core 中的抽样算法</h2><p>在 metric-core 中总共有四种抽样算法，分别是 <code>ExponentiallyDecayingReservoir</code>, <code>SlidingTimeWindowReservoir</code>, <code>SlidingWindowReservoir</code>, <code>UniformReservoir</code>，其中后面三个抽样算法比较常规，也通常能见到，第一个则出于一篇论文<code>Forward Decay: A Practical Time Decay Model for Streaming Systems</code>，本文会通过源码分析自己对于这种抽样算法的理解。本文暂时只分析后面三种抽样算法，对于第一种，我会单独用一篇文章进行分析。</p>
<h3 id="UniformReservoir-算法"><a href="#UniformReservoir-算法" class="headerlink" title="UniformReservoir 算法"></a>UniformReservoir 算法</h3><p>该算法来自于论文<code>Random Sampling with a Reservoir</code>，讲述了一种随机抽样的方法，主要思想是使用一个固定的“蓄水池”装满需要数量的样本，如果当前“蓄水池”未满，将接下来的样本直接放入“蓄水池”，如果“蓄水池”已满，则随机从”蓄水池“中挑选一个样本进行替换（也可能不进行替换），这样在理论上能够保证所有的样本以同样的概率被选中。代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">public void update(long value) &#123;</div><div class="line">	final long c = count.incrementAndGet();//获得当前”蓄水池“的大小</div><div class="line">	if (c &lt;= values.length()) &#123; //如果”蓄水池“未满，直接将当前样本放入</div><div class="line">		values.set((int) c - 1, value);</div><div class="line">	&#125; else &#123;</div><div class="line">		final long r = nextLong(c);//随机挑选一个数据（这个随机挑选的数可能在&quot;蓄水池”中，也可能不在“蓄水池”中</div><div class="line">		if (r &lt; values.length()) &#123;//如果随机挑选的样本，在”蓄水池“中，则进行替换</div><div class="line">			values.set((int) r, value);</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>为了能够更好的理解，先使用样例如下。假设现在总共来了 10 个数 [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]，而“蓄水池“大小为 3. 那么”蓄水池”的 <strong>一种可能</strong> 变化如下（说是一种可能的变化，因为这里面牵涉到概率）</p>
<ul>
<li>[1]</li>
<li>[1, 2]</li>
<li>[1, 2, 3]</li>
<li>[1, 2, 4]  # 当 4 来的时候，发现“蓄水池”已满，然后从中筛选一个进行替换掉，假设我们替换掉 3</li>
<li>[1, 5, 4] # 当 5 来的时候，发现“蓄水池”已满，然后从中筛选一个进行替换掉，假设这次我们替换掉 2</li>
<li>[1, 5, 4] # 当 6 来的时候，发现“蓄水池”已满，我们打算从之前的数字中筛选一个进行替换，这个时候假设我们得到的下标是 3 或者 4，发现下标为 3 和 4 的数字不在“蓄水池”中（“蓄水池”的最大下标为 2 – 从 0 开始），因此不进行替换，所以本次“蓄水池”不变</li>
<li>[7, 5, 4] # 当 7 来的时候，发现“蓄水池”已满，随机一个下标，我们得到 0,那么将 7 放置到下标为 0 的位置</li>
<li>[8, 5, 4] # 同上</li>
<li>[8, 5, 9] # 同上</li>
<li>[10, 5, 9] # 同上<h3 id="SlidingWindowReservoir-抽样算法"><a href="#SlidingWindowReservoir-抽样算法" class="headerlink" title="SlidingWindowReservoir 抽样算法"></a>SlidingWindowReservoir 抽样算法</h3><code>SlidingWindowReservoir</code> 抽样算法则以最近的 N 个样本作为整个数据集的子集，这样简单直接，对于数据波动不大，或者窗口大小 N 足够大的情况下，该算法会有较好的效果。代码如下：</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">public synchronized void update(long value) &#123;//加锁保证线程安全</div><div class="line">	        //每次替换掉最旧的数据，保证”蓄水池“中的数据是最近的 N 个样本</div><div class="line">	        measurements[(int) (count++ % measurements.length)] = value;</div><div class="line">			    &#125;</div></pre></td></tr></table></figure>
<h3 id="SlidingTimeWindowReservoir-抽样算法"><a href="#SlidingTimeWindowReservoir-抽样算法" class="headerlink" title="SlidingTimeWindowReservoir 抽样算法"></a>SlidingTimeWindowReservoir 抽样算法</h3><p>该算法是上面移动窗口算法的变种，保留的是最近 N 时间单位（支持 TimeUnit 的所有时间单位）内的数据，而不是最近的 N 个数据。代码如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">public void update(long value) &#123;</div><div class="line">//每 TRIM_THRESHOLD 次操作之后会进行一次 trim() 操作</div><div class="line">	if (count.incrementAndGet() % TRIM_THRESHOLD == 0) &#123;</div><div class="line">		trim();</div><div class="line">	&#125;</div><div class="line">			        //直接将该值加入到 ”蓄水池“ 中</div><div class="line">	measurements.put(getTick(), value);</div><div class="line">	&#125;</div><div class="line">//获得当前的时间</div><div class="line">private long getTick() &#123;</div><div class="line">	for (; ; ) &#123;</div><div class="line">		final long oldTick = lastTick.get();</div><div class="line">		final long tick = clock.getTick() * COLLISION_BUFFER;</div><div class="line">		// ensure the tick is strictly incrementing even if there are duplicate ticks</div><div class="line">		final long newTick = tick - oldTick &gt; 0 ? tick : oldTick + 1;</div><div class="line">		if (lastTick.compareAndSet(oldTick, newTick)) &#123;</div><div class="line">			return newTick;</div><div class="line">		&#125;</div><div class="line">	&#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">private void trim() &#123;</div><div class="line">	measurements.headMap(getTick() - window).clear();</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这三种算法中，第二种和第三种是大家都很容易想到的，实现起来也很简单，第一种进行简单推导也不难，也算是一种现成的算法“蓄水池抽样”。</p>
<h2 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h2><p>如果某个系统每天会有 N 个人请求（N 不确定），需要从这些人中等概率的抽出 K 个中奖者，那么应该怎么做呢？是否可以使用上面抽样算法中的一种呢？</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://metrics.dropwizard.io&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;metric-core&lt;/a&gt; 是一个 java metric 库，用于统计 JVM 层面以及 服务级别 的各种 metric 信息。其中 metric-core 是其核心模块，代码量不多，总共 44 个文件，5700 行左右代码（包括注释）。算是一个很小的开源项目了。由于 metric 在所有项目中都非常重要，因此选择通读该项目，本文分析 metrci-core 中的抽样算法。&lt;/p&gt;
    
    </summary>
    
    
      <category term="code" scheme="http://yoursite.com/tags/code/"/>
    
      <category term="java" scheme="http://yoursite.com/tags/java/"/>
    
      <category term="metric" scheme="http://yoursite.com/tags/metric/"/>
    
      <category term="reservior" scheme="http://yoursite.com/tags/reservior/"/>
    
      <category term="metric-core" scheme="http://yoursite.com/tags/metric-core/"/>
    
  </entry>
  
  <entry>
    <title>Streaming 中 Receiver 相关源码分析</title>
    <link href="http://yoursite.com/2017/05/19/Streaming-%E4%B8%AD-Receiver-%E7%9B%B8%E5%85%B3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2017/05/19/Streaming-中-Receiver-相关源码分析/</id>
    <published>2017-05-19T03:52:54.000Z</published>
    <updated>2017-06-03T03:58:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>本文基于 spark 1.6.2<br>本次的源码全来自 <code>org.apache.spark.streaming.receiver</code> 这个 package 下，包括 <code>BlockGenerator.scala</code>, <code>RateLimiter</code>, <code>ReceiverdBlock.scala</code>, <code>ReceivedBlockHandler.scala</code>, <code>Receiver.scala</code>, <code>ReceiverSupervisor.scala</code>, <code>ReceiverSupervisorImpl.scala</code></p>
<a id="more"></a>
<p>其中 <code>Receiver</code> 是所有接受数据的父类，主要定义一些接口，用户只需要继承 <code>Receiver</code>，然后实现其中的接口就行。</p>
<p><code>ReceiverSupervisor</code> 则是负责和协调 <code>Receiver</code> 和其他组件，定义了一些接口，然后 <code>ReceiverSupervisorImpl</code> 是 <code>ReceiverSupervisor</code> 的具体实现，主要实现了协调其他组件（包括 <code>ReceivedBlockHandler</code> 和 <code>BlockGenerator</code> <code>BlockGeneratorListener</code> 以及远端 RPC 服务等）和 <code>Receiver</code> 的逻辑。</p>
<p><code>BlockGenerator</code> 则主要负责接受 <code>Receiver</code> 接受到的数据，然后存储成 block（具体的有 <code>ReceivedBlockHandler</code> 负责），会起两个线程来做相应的事情，一个是定时的将接受到的数据生成 block，一个是将 block push 给 <code>ReceivedBlockHandler</code> 进行存储，具体的 block 管理则通过 Spark core 的 block 模块来进行管理。</p>
<p><code>ReceivedBlockHandler</code> 则负责将 block 保存到具体的地方，包括指定的 storageLevel 以及 write ahead log。</p>
<p>整个 Receiver 端的代码结构简化版如下所示，其中 Receiver 包含一个 ReceiverSupervisor 对象，ReceiverSupervisor 负责和 BlockGenerator 以及 ReceivedBlockHandler 交互。用户继承 Receiver，实现具体的接受数据的逻辑即可，对于数据接受之后，怎么处理，都通过 ReceiverSupervisor 中转给 BlockGenerator 来处理（BlockGenerator 会有一个定时器用于生成 block，还有一个单独的线程用于将生成的 block push 给 BlockManager）</p>
<img src="/2017/05/19/Streaming-中-Receiver-相关源码分析/Receiver_sample.png" alt="Receiver_sample.png" title="">
<p>整个 Receiver 端的详细代码结构图如下所示<br><img src="/2017/05/19/Streaming-中-Receiver-相关源码分析/Receiver.png" alt="Receiver.png" title=""></p>
<h1 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h1><ol>
<li>为什么需要将 <code>Receiver</code> 和 <code>ReceiverSupervisor</code> 进行分开呢，下面提供这两个类的函数对比图（其中第一列表示 <code>Receiver</code> 的所有函数；后面几列表示 <code>ReceiverSupervisor</code> 的所有函数，同一行的函数表示有相<img src="/2017/05/19/Streaming-中-Receiver-相关源码分析/Receiver_ReceiverSupervisor.png" alt="Receiver_ReceiverSupervisor.png" title="">
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文基于 spark 1.6.2&lt;br&gt;本次的源码全来自 &lt;code&gt;org.apache.spark.streaming.receiver&lt;/code&gt; 这个 package 下，包括 &lt;code&gt;BlockGenerator.scala&lt;/code&gt;, &lt;code&gt;RateLimiter&lt;/code&gt;, &lt;code&gt;ReceiverdBlock.scala&lt;/code&gt;, &lt;code&gt;ReceivedBlockHandler.scala&lt;/code&gt;, &lt;code&gt;Receiver.scala&lt;/code&gt;, &lt;code&gt;ReceiverSupervisor.scala&lt;/code&gt;, &lt;code&gt;ReceiverSupervisorImpl.scala&lt;/code&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="分布式系统" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="实时计算" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
      <category term="源码阅读" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="spark_streaming" scheme="http://yoursite.com/tags/spark-streaming/"/>
    
      <category term="receiver" scheme="http://yoursite.com/tags/receiver/"/>
    
      <category term="source_code" scheme="http://yoursite.com/tags/source-code/"/>
    
  </entry>
  
  <entry>
    <title>Python 代码实践小结</title>
    <link href="http://yoursite.com/2017/05/10/Python-%E4%BB%A3%E7%A0%81%E5%AE%9E%E8%B7%B5%E5%B0%8F%E7%BB%93/"/>
    <id>http://yoursite.com/2017/05/10/Python-代码实践小结/</id>
    <published>2017-05-10T03:59:21.000Z</published>
    <updated>2017-06-03T04:07:01.000Z</updated>
    
    <content type="html"><![CDATA[<p>最近写了较多的 Python 脚本，将最近自己写的脚本进行一个总结，其中有些是 Python 独有的，有些是所有程序设计中共有的：</p>
<ol>
<li>考虑使用 Logger（logger 怎么配置，需要输出哪些信息 – 可以反向考虑，自己看到这个 logger 的时候想了解什么信息）</li>
<li>传递的数据结构如何考虑（是否对调用方有先验知识的要求，比如返回一个 Tuple，则需要用户了解 tuple 中元素的顺序，这样情况是否应该进行封装；），数据结构定义清楚了，很多东西也就清楚了。</li>
<li>如何操作数据库（可以学习 sqlalchemy，包括 core 和 orm 两种 api）</li>
<li>异常如何处理（异常应该分开捕获 – 可以清楚的知道什么情况下导致的，异常之后应该打印日志说明出现什么问题，如果情况恶劣需要进行异常再次抛出或者报警）</li>
<li>所有获取资源的地方都应该做 check（a. 没有获取到会怎么办；b.获取到异常的怎么办）</li>
<li>所有操作资源的地方都应该检查是否操作成功</li>
<li>每个函数都应该简短，如果函数过长应该进行拆分（有个建议值，函数包含的行数应该在 20-30 行之间，具体按照这个规范做过一次之后就会发现这样真好）</li>
<li>使用 class 之后，考虑重构 <code>__str__</code> 函数，用户打印输出，如果对象放到 collection 中之后，需要实现 <code>__repr__</code> 函数，用于打印整个 collection 的时候，直观显示（如果不实现 <code>__str__</code>，会调用 <code>__repr__</code>)</li>
<li>如果有些资源会发生变化，可以单独抽取出来，做成函数，这样后续调用就可以不用改变了</li>
</ol>
<p>上述总结肯定有片面的地方，也有不全的地方，欢迎指出</p>
<a id="more"></a>
<p>附上一份 Python2.7 代码（将一些私有的东西进行了修改）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div></pre></td><td class="code"><pre><div class="line"># -*- coding:utf-8 -*-</div><div class="line"></div><div class="line">from sqlalchemy import create_engine</div><div class="line">import logging</div><div class="line">from logging.config import fileConfig</div><div class="line">import requests</div><div class="line">import Clinet # 私有的模块</div><div class="line"></div><div class="line">fileConfig(&quot;logging_config.ini&quot;)</div><div class="line">logger = logging.getLogger(&quot;killduplicatedjob&quot;)</div><div class="line"></div><div class="line">#配置可以单独放到一个模块中</div><div class="line">DB_USER = &quot;xxxxxxx&quot;</div><div class="line">DB_PASSWORD = &quot;xxxxxxxx&quot;</div><div class="line">DB_PORT = 111111</div><div class="line">DB_HOST_PORT = &quot;xxxxxxxxxx&quot;</div><div class="line">DB_DATA_BASE = &quot;xxxxxxxxxxx&quot;</div><div class="line"></div><div class="line">REST_API_URL = &quot;http://sample.com&quot;</div><div class="line"></div><div class="line">engine = create_engine(&quot;mysql://%s:%s@%s:%s/%s&quot; % (DB_USER, DB_PASSWORD, DB_HOST_PORT, DB_PORT, DB_DATA_BASE))</div><div class="line"></div><div class="line"></div><div class="line"># 这个 class 是为了在函数间传递时，不需要使用方了解属性的具体顺序而写的，也可以放到一个单独的模块中</div><div class="line">class DuplicatedJobs(object):</div><div class="line">    def __init__(self, app_id, app_name, user):</div><div class="line">        self.app_id = app_id</div><div class="line">        self.app_name = app_name</div><div class="line">        self.user = user</div><div class="line"></div><div class="line">     def __repr__(self):</div><div class="line">        return &apos;[appid:%s, app_name:%s, user:%s]&apos; % (self.app_id, self.app_name, self.user)</div><div class="line"></div><div class="line"></div><div class="line">	def find_duplicated_jobs():</div><div class="line">		logger.info(&quot;starting find duplicated jobs&quot;)</div><div class="line">		(running_apps, app_name_to_user) = get_all_running_jobs()</div><div class="line">		all_apps_on_yarn = get_apps_from_yarn_with_queue(get_resource_queue())</div><div class="line"></div><div class="line">		duplicated_jobs = []</div><div class="line">		for app in all_apps_on_yarn:</div><div class="line">			(app_id, app_name) = app</div><div class="line"></div><div class="line">			if app_id not in running_apps:</div><div class="line">				if not app_name.startswith(&quot;test&quot;):</div><div class="line">					logger.info(&quot;find a duplicated job, prefixed_name[%s] with appid[%s]&quot; % (app_name, app_id))</div><div class="line">					user = app_name_to_user[app_name]</div><div class="line">					duplicated_jobs.append(DuplicatedJobs(app_id, app_name, user))</div><div class="line">	            else:</div><div class="line">				    logger.info(&quot;Job[%s] is a test job, would not kill it&quot; % app_name)</div><div class="line"></div><div class="line">	logger.info(&quot;Find duplicated jobs [%s]&quot; % duplicated_jobs)</div><div class="line"></div><div class="line">    return duplicated_jobs</div><div class="line"></div><div class="line"></div><div class="line">def get_apps_from_yarn_with_queue(queue):</div><div class="line">	param = &#123;&quot;queue&quot;: queue&#125;</div><div class="line">	r = requests.get(REST_API_URL, params=param)</div><div class="line">	apps_on_yarn = []</div><div class="line">	try:</div><div class="line">		jobs = r.json().get(&quot;apps&quot;)</div><div class="line">		app_list = jobs.get(&quot;app&quot;, [])</div><div class="line">		for app in app_list:</div><div class="line">			app_id = app.get(&quot;id&quot;)</div><div class="line">			name = app.get(&quot;name&quot;)</div><div class="line">			apps_on_yarn.append((app_id, name))</div><div class="line"></div><div class="line">	 except Exception as e: #Exception 最好进行单独的分开，针对每一种 Exception 进行不同的处理</div><div class="line">		logger.error(&quot;Get apps from Yarn Error, message[%s]&quot; % e.message)</div><div class="line"></div><div class="line">	logger.info(&quot;Fetch all apps from Yarn [%s]&quot; % apps_on_yarn)</div><div class="line"></div><div class="line">	return apps_on_yarn</div><div class="line"></div><div class="line"></div><div class="line">def get_all_running_jobs():</div><div class="line">	job_infos = get_result_from_mysql(&quot;select * from xxxx where xx=yy&quot;)</div><div class="line"></div><div class="line">	app_ids = []</div><div class="line">	app_name_to_user = &#123;&#125;</div><div class="line">	for (topology_id, topology_name) in job_infos:</div><div class="line">		status_set = get_result_from_mysql(&quot;select * from xxxx where xx=yy&quot;)</div><div class="line">		application_id = status_set[0][0]</div><div class="line">		if &quot;&quot; != application_id:</div><div class="line">			configed_resource_queue = get_result_from_mysql(&quot;select * from xxxx where xx=yy&quot;)</div><div class="line">			app_ids.append(application_id)</div><div class="line">	        app_name_to_user[topology_name] = configed_resource_queue[0][0].split(&quot;.&quot;)[1]</div><div class="line"></div><div class="line">	logger.info(&quot;All running jobs appids[%s] topology_name2user[%s]&quot; % (app_ids, app_name_to_user))</div><div class="line">	return app_ids, app_name_to_user</div><div class="line"></div><div class="line"></div><div class="line">def kill_duplicated_jobs(duplicated_jobs):</div><div class="line">	for job in duplicated_jobs:</div><div class="line">	app_id = job.app_id</div><div class="line">	app_name = job.app_name</div><div class="line">	user = job.user</div><div class="line">	logger.info(&quot;try to kill job[%s] with appid[%s] for user[%s]&quot; % (app_name, app_id, user))</div><div class="line">	try:</div><div class="line">		Client.kill_job(app_id, user)</div><div class="line">		logger.info(&quot;Job[%s] with appid[%s] for user[%s] has been killed&quot; % (app_name, app_id, user))</div><div class="line">	except Exception as e:</div><div class="line">		logger.error(&quot;Can&apos;t kill job[%s] with appid[%s] for user[%s]&quot; % (app_name, app_id, user))</div><div class="line"></div><div class="line"></div><div class="line">def get_result_from_mysql(sql):</div><div class="line">	a = engine.execute(sql)</div><div class="line">	return a.fetchall() </div><div class="line"></div><div class="line"></div><div class="line"># 因为下面的资源可能发生变化，而且可能包含一些具体的逻辑，因此单独抽取出来，独立成一个函数</div><div class="line">def get_resource_queue():</div><div class="line">	return &quot;xxxxxxxxxxxxx&quot;</div><div class="line"></div><div class="line">if __name__ == &quot;__main__&quot;:</div><div class="line">	kill_duplicated_jobs(find_duplicated_jobs())</div></pre></td></tr></table></figure>
<p>其中 logger 配置文件如下（对于 Python 的 logger，官方文档写的非常好，建议读一次，并且实践一次）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">[loggers]</div><div class="line">keys=root, simpleLogger</div><div class="line"></div><div class="line">[handlers]</div><div class="line">keys=consoleHandler, logger_handler</div><div class="line"></div><div class="line">[formatters]</div><div class="line">keys=formatter</div><div class="line"></div><div class="line">[logger_root]</div><div class="line">level=WARN</div><div class="line">handlers=consoleHandler</div><div class="line"></div><div class="line">[logger_simpleLogger]</div><div class="line">level=INFO</div><div class="line">handlers=logger_handler</div><div class="line">propagate=0</div><div class="line">qualname=killduplicatedjob</div><div class="line"></div><div class="line">[handler_consoleHandler]</div><div class="line">class=StreamHandler</div><div class="line">level=WARN</div><div class="line">formatter=formatter</div><div class="line">args=(sys.stdout,)</div><div class="line"></div><div class="line">[handler_logger_handler]</div><div class="line">class=logging.handlers.RotatingFileHandler</div><div class="line">level=INFO</div><div class="line">formatter=formatter</div><div class="line">args=(&quot;kill_duplicated_streaming.log&quot;, &quot;a&quot;, 52428800, 3,)</div><div class="line"></div><div class="line">[formatter_formatter]</div><div class="line">format=%(asctime)s %(name)-12s %(levelname)-5s %(message)s</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近写了较多的 Python 脚本，将最近自己写的脚本进行一个总结，其中有些是 Python 独有的，有些是所有程序设计中共有的：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;考虑使用 Logger（logger 怎么配置，需要输出哪些信息 – 可以反向考虑，自己看到这个 logger 的时候想了解什么信息）&lt;/li&gt;
&lt;li&gt;传递的数据结构如何考虑（是否对调用方有先验知识的要求，比如返回一个 Tuple，则需要用户了解 tuple 中元素的顺序，这样情况是否应该进行封装；），数据结构定义清楚了，很多东西也就清楚了。&lt;/li&gt;
&lt;li&gt;如何操作数据库（可以学习 sqlalchemy，包括 core 和 orm 两种 api）&lt;/li&gt;
&lt;li&gt;异常如何处理（异常应该分开捕获 – 可以清楚的知道什么情况下导致的，异常之后应该打印日志说明出现什么问题，如果情况恶劣需要进行异常再次抛出或者报警）&lt;/li&gt;
&lt;li&gt;所有获取资源的地方都应该做 check（a. 没有获取到会怎么办；b.获取到异常的怎么办）&lt;/li&gt;
&lt;li&gt;所有操作资源的地方都应该检查是否操作成功&lt;/li&gt;
&lt;li&gt;每个函数都应该简短，如果函数过长应该进行拆分（有个建议值，函数包含的行数应该在 20-30 行之间，具体按照这个规范做过一次之后就会发现这样真好）&lt;/li&gt;
&lt;li&gt;使用 class 之后，考虑重构 &lt;code&gt;__str__&lt;/code&gt; 函数，用户打印输出，如果对象放到 collection 中之后，需要实现 &lt;code&gt;__repr__&lt;/code&gt; 函数，用于打印整个 collection 的时候，直观显示（如果不实现 &lt;code&gt;__str__&lt;/code&gt;，会调用 &lt;code&gt;__repr__&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;如果有些资源会发生变化，可以单独抽取出来，做成函数，这样后续调用就可以不用改变了&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上述总结肯定有片面的地方，也有不全的地方，欢迎指出&lt;/p&gt;
    
    </summary>
    
      <category term="语言学习" scheme="http://yoursite.com/categories/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="计算机基础" scheme="http://yoursite.com/categories/%E8%AF%AD%E8%A8%80%E5%AD%A6%E4%B9%A0/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/"/>
    
    
      <category term="python" scheme="http://yoursite.com/tags/python/"/>
    
      <category term="code" scheme="http://yoursite.com/tags/code/"/>
    
      <category term="logger" scheme="http://yoursite.com/tags/logger/"/>
    
  </entry>
  
  <entry>
    <title>从现在开始写作</title>
    <link href="http://yoursite.com/2017/04/15/%E4%BB%8E%E7%8E%B0%E5%9C%A8%E5%BC%80%E5%A7%8B%E5%86%99%E4%BD%9C/"/>
    <id>http://yoursite.com/2017/04/15/从现在开始写作/</id>
    <published>2017-04-15T04:07:32.000Z</published>
    <updated>2017-06-03T04:11:28.000Z</updated>
    
    <content type="html"><![CDATA[<p>这里的写作不特指写长篇大论的文章</p>
<h2 id="为什么要写作"><a href="#为什么要写作" class="headerlink" title="为什么要写作"></a>为什么要写作</h2><blockquote>
<p>写作是了帮助自己更好的思考，提高自己的效率。</p>
</blockquote>
<p>首先，每个人同一时刻能记住的东西有限，而做一件事情可能需要考虑的条件往往会比较多，将所有的情况写到一张纸上，就能在需要的时候看到自己需要的条件。相信每个人都有这样的体验：做数学题的时候，将所有的已知条件，或者自己推导出来的结论写到草稿纸上，往往能更快的解出这道题目。这也是写作帮助自己思考的一个直接。</p>
<p>其次，我遇到过这种情况，自己想着是这么回事，可是写下来的时候，发现无从下笔，根本写不出来，写出来之后，也很难向别人清楚、准确的表达自己的意思。这归根结底还是没有思考清楚导致的，而写作可以帮助我整理自己的思路。</p>
<p>然后，当过客服的人肯定有一个体会，客服非常耗时间，而这些客服的问题大部分是差不多的，一对一的客服是非常低效的，就算在群组里面进行客服，其实很多后来遇到同样问题的人也是不看群历史记录（或者不知道以前有人遇到过类似的问题）的。这种情况下，虽然文档不是最优解，但能够省出自己足够多的时间，实际上这是一件杠杆率非常高（产出投入比高）的事件。</p>
<a id="more"></a>
<h2 id="写什么"><a href="#写什么" class="headerlink" title="写什么"></a>写什么</h2><p>既然明确了写作的目的，那么写什么也就清楚了。</p>
<ol>
<li><p>写对自己有用的东西</p>
<ul>
<li>读书笔记。看完一本书之后，可以写一个读书笔记，或者用思维导图整理书中内容，整理的过程是帮助自己加深书中思想印象的过程，可以帮助自己将整本书串起来，有一个全局的了解，也知道自己哪些地方暂时不太清楚。</li>
<li>自己解决问题的总结。总结可以帮助自己复盘 – 有复盘才有改进的方向；备忘。我曾经在公司内网上记录了自己解决某个问题的过程与解决方案，后来有不少遇到同样问题的同事联系我，每个人的问题相似，但每个人的问题又都不一样，解决别人的问题，实际上也能扩大自己的知识储备。</li>
<li>写教程 – 教是最好的学。在自己写教程的过程中，实际上也是自己再教自己一次相应的知识，对掌握的知识了解更深，对掌握不够的知识，去学习了解。</li>
<li>自己对某件事的思考过程。将自己的思考过程写下来，能够更好的了解自己是怎么考虑问题的，在哪些地方可以进行改善，哪些地方是自己没有考虑到的，能够帮助自己更好的提高思考质量。</li>
</ul>
</li>
<li><p>写对别人有用的东西</p>
</li>
</ol>
<blockquote>
<p>解决问题的思路和方案，以及教程对别人也是有用的。其他对别人有用的东西，暂时还在思考中。</p>
</blockquote>
<h2 id="怎么写"><a href="#怎么写" class="headerlink" title="怎么写"></a>怎么写</h2><blockquote>
<p>用笔写</p>
</blockquote>
<p>为什么写，以及写什么都考虑清楚之后，怎么写就不再是个问题了。如果你习惯写在纸质的本子上，那么买一个自己喜欢的本子，一只自己喜欢的笔，直接写就好了；如果你习惯使用电脑或者手机写，那么新建一个文件夹，在里面写就好了。</p>
<p>写的东西并不一定要公开，对于暂时不成熟的东西，或者私密的东西，保存在一个只有自己能看的地方就好了；对于成熟的东西，或者可以公开的东西，公开发表就好了。公开发表有一个好处，得到更多的反馈，这些反馈是非常利于自己进步的。</p>
<p>另外，有一个很简单的小技巧。自己维护一个素材记录本（可以是纸质的本子，可以是电子产品 – 建议购买正版），里面记录自己觉得不错的句子，例子，表达方式等等，偶尔翻一翻，或者打算写作的时候进行一下搜索。</p>
<h2 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h2><ol>
<li>写的不好怎么办  <blockquote>
<p>没有人一生下来就会吃饭吗？也没有人长大之后不会吃饭</p>
</blockquote>
</li>
<li>不知道怎么写开头怎么办<blockquote>
<p>如果不写开头的话，知道怎么写吗？知道的话，那就不要写开头了，为什么一定要写开头呢</p>
</blockquote>
</li>
<li>要写多少才合适呢<blockquote>
<p>没有一个统一的结论写多少才合适，只要能将自己想表达的东西讲清楚就行。写作最终受益的是自己，这不是在考试，不要为了凑字数而写。</p>
</blockquote>
</li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这里的写作不特指写长篇大论的文章&lt;/p&gt;
&lt;h2 id=&quot;为什么要写作&quot;&gt;&lt;a href=&quot;#为什么要写作&quot; class=&quot;headerlink&quot; title=&quot;为什么要写作&quot;&gt;&lt;/a&gt;为什么要写作&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;写作是了帮助自己更好的思考，提高自己的效率。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;首先，每个人同一时刻能记住的东西有限，而做一件事情可能需要考虑的条件往往会比较多，将所有的情况写到一张纸上，就能在需要的时候看到自己需要的条件。相信每个人都有这样的体验：做数学题的时候，将所有的已知条件，或者自己推导出来的结论写到草稿纸上，往往能更快的解出这道题目。这也是写作帮助自己思考的一个直接。&lt;/p&gt;
&lt;p&gt;其次，我遇到过这种情况，自己想着是这么回事，可是写下来的时候，发现无从下笔，根本写不出来，写出来之后，也很难向别人清楚、准确的表达自己的意思。这归根结底还是没有思考清楚导致的，而写作可以帮助我整理自己的思路。&lt;/p&gt;
&lt;p&gt;然后，当过客服的人肯定有一个体会，客服非常耗时间，而这些客服的问题大部分是差不多的，一对一的客服是非常低效的，就算在群组里面进行客服，其实很多后来遇到同样问题的人也是不看群历史记录（或者不知道以前有人遇到过类似的问题）的。这种情况下，虽然文档不是最优解，但能够省出自己足够多的时间，实际上这是一件杠杆率非常高（产出投入比高）的事件。&lt;/p&gt;
    
    </summary>
    
      <category term="我的生活" scheme="http://yoursite.com/categories/%E6%88%91%E7%9A%84%E7%94%9F%E6%B4%BB/"/>
    
      <category term="想清楚" scheme="http://yoursite.com/categories/%E6%88%91%E7%9A%84%E7%94%9F%E6%B4%BB/%E6%83%B3%E6%B8%85%E6%A5%9A/"/>
    
    
      <category term="写作" scheme="http://yoursite.com/tags/%E5%86%99%E4%BD%9C/"/>
    
      <category term="成长" scheme="http://yoursite.com/tags/%E6%88%90%E9%95%BF/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming 统一在每分钟的 00 秒消费 Kafka 数据的问题解决</title>
    <link href="http://yoursite.com/2017/02/16/spark-streaming-consume-kafka-at-00-second-of-every-minute/"/>
    <id>http://yoursite.com/2017/02/16/spark-streaming-consume-kafka-at-00-second-of-every-minute/</id>
    <published>2017-02-16T15:44:31.000Z</published>
    <updated>2017-06-03T03:38:41.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="现象"><a href="#现象" class="headerlink" title="现象"></a>现象</h2><p>一批 Spark Streaming 会统一在每分钟的 00 秒开始消费 kafka 数据</p>
<a id="more"></a>
<h2 id="问题排查"><a href="#问题排查" class="headerlink" title="问题排查"></a>问题排查</h2><p>这一批作业的功能就是从 kafka 消费数据，进行转化后存储到外部可靠介质中。所有作业的 <code>batchDuration</code> 都设置为 60s。<br>我们追踪代码可以得到在 <code>JobGenerator</code> 中有一个变量 <code>timer : RecurringTimer</code>，改变量用于定时的启动 task 去消费数据。<br>从 <code>RecurringTimer#getStartTime</code> 我们可以得到作业第一个 batch 的启动时间，后续的 batch 启动时间则是在第一个 batch 的启动时间上加上 <code>batchDuration</code> 的整数倍。<br>第一个 batch 的起动时间实现如下：<br><code>(math.floor(clock.getTimeMillis().toDouble / period) + 1).toLong * period</code><br>其中 <code>clock.getTimeMillis()</code> 是当前时间，period 是<code>batchDuration</code> 的毫秒表示法。通过上述公式，我们可以知道作业的启动时间会对齐到 <code>batchDuration</code>，而我们把这一批作业的 <code>batchDuration</code> 都设置为 60s，因此都会在每分钟的 00 秒开始消费 kafka 数据。</p>
<h2 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h2><p>我们可以通过下面两种方式进行解决</p>
<ol>
<li>设置不同的 <code>batchDuration</code></li>
<li>改写 <code>RecurringTimer#getStartTime</code> 的逻辑，在上述对齐的时间基础上加上一个 [0, period) 范围内的随机数</li>
</ol>
<p>我们知道在上述两种解决方案中，第一种，不同作业还是会在某一时刻重合，而且这个重合的时间点不可控，可能是作业运行一小时后，可能是运行一天后，也可能是运行一周后。而第二种作业则是可控的，在作业启动时就决定了。因此这里我们采用第二种方案。</p>
<p>本文采用了一种新的排版方式，在进行实验，如果效果好的好，后续大部分内容都会以这种形式进行发布</p>
<div class="markdown-here-wrapper" style="font-size: 16px; line-height: 1.8em; letter-spacing: 0.1em;" data-md-url="http://www.klion26.com/wp-admin/post-new.php"></div>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;现象&quot;&gt;&lt;a href=&quot;#现象&quot; class=&quot;headerlink&quot; title=&quot;现象&quot;&gt;&lt;/a&gt;现象&lt;/h2&gt;&lt;p&gt;一批 Spark Streaming 会统一在每分钟的 00 秒开始消费 kafka 数据&lt;/p&gt;
    
    </summary>
    
      <category term="分布式系统" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="spark_streaming" scheme="http://yoursite.com/tags/spark-streaming/"/>
    
      <category term="kafka" scheme="http://yoursite.com/tags/kafka/"/>
    
      <category term="problem_solve" scheme="http://yoursite.com/tags/problem-solve/"/>
    
      <category term="batchDuration" scheme="http://yoursite.com/tags/batchDuration/"/>
    
      <category term="JobGenerator" scheme="http://yoursite.com/tags/JobGenerator/"/>
    
      <category term="RecurringTimer" scheme="http://yoursite.com/tags/RecurringTimer/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming 往 HDFS 追加 LZO 文件</title>
    <link href="http://yoursite.com/2017/01/15/spark-streaming-e5-be-80-hdfs-e8-bf-bd-e5-8a-a0-lzo-e6-96-87-e4-bb-b6/"/>
    <id>http://yoursite.com/2017/01/15/spark-streaming-e5-be-80-hdfs-e8-bf-bd-e5-8a-a0-lzo-e6-96-87-e4-bb-b6/</id>
    <published>2017-01-15T07:38:17.000Z</published>
    <updated>2017-06-03T03:39:14.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>将数据从 Kafka 同步到 Hive，并且目标格式希望是 lzo。我们通过 Spark Streaming 做这件事，将文件写成 lzo 格式，并且添加索引。</p>
<a id="more"></a>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>要实现将数据从 Kafka 同步到 Hive 的功能，我们通过将数据直接写到 HDFS 路径来解决，由于担心小文件太多的问题（一个 batch 一个文件的话，可能造成小文件太多，对 HDFS 造成非常大的压力），所以我们通过追加的方式写 HDFS 文件。</p>
<p>往 HDFS 追加写文件的方式，我们在前面一篇文章中描述了具体的方案。但是对于格式为 LZO 的文件，我们发现一个现象：通过 Hive 查询，只能查到第一个 batch 的数据（也就是说所有 append 的数据都不能被查询到）。这是因为 LZO 文件会在关闭的时候在文件末尾添加一个块结束标记符，导致解析的时候只能读取到块结束符之前的数据（Linux 自带的 lzop 文件可以解析包含块结束符的文件）。到这里我们有两个思路：</p>
<pre><code>1. 在 Hive 层面进行修改，将 Hive 使用的 InputFormat 重新实现，从而可以解析 multipart 的文件；
2. 通过某种方式将文件进行追加，但是文件的中间不会出现结束块的标记符。
</code></pre><p>由于第一种方式影响较大，实现起来周期较长，所以这里采用第二种方法。</p>
<p>我们考虑如何做到往 HDFS 写完数据之后，文件流不进行关闭，在我们需要关闭的时候再手动关闭。也就是说同一个 Executor 上的多 batch 公用同一个文件流。</p>
<p>查看<a href="http://spark.apache.org/docs/latest/streaming-programming-guide.html#design-patterns-for-using-foreachrdd" target="_blank" rel="external">官方文档</a>我们可以得到这是可以实现的，也就是文档中的 ConnectionPool 实现的方式，这可以做到在同一个 Executor 上执行的多个 batch 公用同一个文件流（个人觉得这里也可以从 JVM 的层面来考虑，就是利用了 static 变量的声明周期以及可访问范围）。</p>
<p>当我们手动关闭某个文件的时候，再考虑将这个文件 move 到特定的地方（Hive 表对应的 HDFS 路径），然后添加索引，大致框架就完成了。当然这也仅仅是一个框架，需要处理的细节问题还有很多。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;需求&quot;&gt;&lt;a href=&quot;#需求&quot; class=&quot;headerlink&quot; title=&quot;需求&quot;&gt;&lt;/a&gt;需求&lt;/h2&gt;&lt;p&gt;将数据从 Kafka 同步到 Hive，并且目标格式希望是 lzo。我们通过 Spark Streaming 做这件事，将文件写成 lzo 格式，并且添加索引。&lt;/p&gt;
    
    </summary>
    
      <category term="实时计算" scheme="http://yoursite.com/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
    
      <category term="spark_streaming" scheme="http://yoursite.com/tags/spark-streaming/"/>
    
      <category term="kafka" scheme="http://yoursite.com/tags/kafka/"/>
    
      <category term="append" scheme="http://yoursite.com/tags/append/"/>
    
      <category term="hdfs" scheme="http://yoursite.com/tags/hdfs/"/>
    
      <category term="hive" scheme="http://yoursite.com/tags/hive/"/>
    
      <category term="lzo" scheme="http://yoursite.com/tags/lzo/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming Ran out of messages before reaching ending offset 异常</title>
    <link href="http://yoursite.com/2016/12/16/spark-streaming-ran-out-of-messages-before-reaching-ending-offset/"/>
    <id>http://yoursite.com/2016/12/16/spark-streaming-ran-out-of-messages-before-reaching-ending-offset/</id>
    <published>2016-12-16T06:23:13.000Z</published>
    <updated>2017-06-03T03:39:33.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="现象"><a href="#现象" class="headerlink" title="现象"></a>现象</h2><p>Spark Streaming 处理数据过程中遇到 <code>Ran out of messages before reaching ending offset</code> 异常，导致程序一直 hang 住（因为我们希望接上次消费从而不丢失数据）</p>
<a id="more"></a>
<h2 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h2><p>通过异常栈信息，我们知道异常从 KafkaRDD.scala#211 行抛出，下面是相应代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">206 override def getNext(): R = &#123;</div><div class="line">	  if (iter == null || !iter.hasNext) &#123;</div><div class="line">208        iter = fetchBatch</div><div class="line">      &#125;</div><div class="line">210      if (!iter.hasNext) &#123;</div><div class="line">211        assert(requestOffset == part.untilOffset, errRanOutBeforeEnd(part))</div><div class="line">212        finished = true</div><div class="line">           null.asInstanceOf[R]</div><div class="line">		&#125; else &#123;</div><div class="line">		   	val item = iter.next()</div><div class="line">			if (item.offset &gt;= part.untilOffset) &#123;</div><div class="line">217 	        assert(item.offset == part.untilOffset, errOvershotEnd(item.offset, part))</div><div class="line">				finished = true</div><div class="line">				null.asInstanceOf[R]</div><div class="line">			&#125; else &#123;</div><div class="line">				requestOffset = item.nextOffset</div><div class="line">				messageHandler(new MessageAndMetadata(</div><div class="line">				part.topic, part.partition, item.message, item.offset, keyDecoder, valueDecoder))</div><div class="line">			&#125;</div><div class="line">		&#125;</div><div class="line">226    &#125;</div></pre></td></tr></table></figure>
<p>通过分析，我们知道这个地方是实际从 Kafka 读取数据的逻辑，首先会调用 <code>fetchBatch</code> 函数（208 行），然后进行逻辑判断，数据是否读取完毕，是否发生异常</p>
<p>其中 211 行的异常表示还未读取到 part.untilOffset 但是当前迭代器中没有数据了；217 行表示当前读取的数据如果超过了 part.untilOffset ，那么在这个时候退出当前 batch（offset 从 fromOffset 逐次加一增加的，正常的逻辑肯定会和 part.untilOffset 相等）</p>
<p>我们知道异常从 211 行抛出来的，也知道了异常的最直接原因，那么这个原因是什么造成的呢？</p>
<p>211 行的代码执行了，也就是 210 行的 if 语句未 true，这样的话，207 行的逻辑也应该为 true。这样的话 iter 就是 fetchBatch 返回的迭代器了。接下来我们看看 fetchBatch 的代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">188 private def fetchBatch: Iterator[MessageAndOffset] = &#123;</div><div class="line">189      val req = new FetchRequestBuilder()</div><div class="line">190         .addFetch(part.topic, part.partition, requestOffset, kc.config.fetchMessageMaxBytes)</div><div class="line">			.build()</div><div class="line">192      val resp = consumer.fetch(req)</div><div class="line">	   	 handleFetchErr(resp)</div><div class="line">		// kafka may return a batch that starts before the requested offset</div><div class="line">		 resp.messageSet(part.topic, part.partition)</div><div class="line">196       .iterator</div><div class="line">          .dropWhile(_.offset &lt; requestOffset)</div><div class="line">		&#125;</div></pre></td></tr></table></figure></p>
<p>我们发现 192 行会通过 consumer 从 kafka 获取数据，本次从哪获取数据，以及获取多少分别由 190 行的 <code>topic</code>, <code>partition</code> 和 <code>kc.config.fetchMessageMaxBytes</code> 指定。我们查看 <code>kc.config.fetchMessageMaxBytes</code>，发现默认使用的是 1M</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">ConsumerConfig.scala</div><div class="line">29 val FetchSize = 1024 * 1024</div><div class="line"></div><div class="line">114 val fetchMessageMaxBytes = props.getInt(&quot;fetch.message.max.bytes&quot;, FetchSize)</div></pre></td></tr></table></figure>
<p>从这里我们知道每次从 kafka 上最多获取 1M 的数据（这也是为什么需要在 <code>KafkaRDD.getNext</code> 函数的开头通过 <code>iter.hasNext()</code> 来判断是否需要调用 <code>fetchBatch</code> </p>
<p>然后看到 fetchBatch 函数对应的 196 行，获取迭代器作为返回值，查看相应代码，跳转到 <code>ByteBufferMessageSet.scala</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">139 override def iterator: Iterator[MessageAndOffset] = internalIterator()</div><div class="line">145 private def internalIterator(isShallow: Boolean = false): Iterator[MessageAndOffset] = &#123;</div><div class="line">		    new IteratorTemplate[MessageAndOffset] &#123;</div><div class="line">				        ......</div><div class="line">152      def makeNextOuter: MessageAndOffset = &#123;</div><div class="line">             // if there isn&apos;t at least an offset and size, we are done</div><div class="line">			if (topIter.remaining &lt; 12)</div><div class="line">				return allDone()</div><div class="line">			    val offset = topIter.getLong()</div><div class="line">			    val size = topIter.getInt()</div><div class="line">		        if(size &lt; Message.MinHeaderSize)</div><div class="line">			         throw new InvalidMessageException(&quot;Message found with corrupt size (&quot; + size + &quot;)&quot;)</div><div class="line">					</div><div class="line">160       // we have an incomplete message</div><div class="line">161       if(topIter.remaining &lt; size)</div><div class="line">162         return allDone()</div><div class="line">		....</div><div class="line">185     &#125;</div></pre></td></tr></table></figure>
<p>从 161 行我们可以看出，如果读取的消息是一条不完整的，那么本次不处理，默认本次消息读取完成。<br>上面所有的链条穿起来就抛出了我们文章开始的异常。</p>
<pre><code>1. 从 kafka 读取 1M 的数据（默认大小）
2. 发现读取的数据不完整（这个消息的大小大于 1M），所以本次读取的 迭代器 为空
3. 发现迭代器为空，但是当前的 offset 和 part.untilOffset 不想等，抛出异常
</code></pre><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>通过设置 kafkaParam 的参数 <code>fetch.message.max.bytes</code> 就行了，我们设置成 2M（大于一条数据的最大值即可），就能够运行成功了</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;现象&quot;&gt;&lt;a href=&quot;#现象&quot; class=&quot;headerlink&quot; title=&quot;现象&quot;&gt;&lt;/a&gt;现象&lt;/h2&gt;&lt;p&gt;Spark Streaming 处理数据过程中遇到 &lt;code&gt;Ran out of messages before reaching ending offset&lt;/code&gt; 异常，导致程序一直 hang 住（因为我们希望接上次消费从而不丢失数据）&lt;/p&gt;
    
    </summary>
    
      <category term="实时计算" scheme="http://yoursite.com/categories/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="kafka" scheme="http://yoursite.com/tags/kafka/"/>
    
      <category term="spark-streaming" scheme="http://yoursite.com/tags/spark-streaming/"/>
    
      <category term="exception" scheme="http://yoursite.com/tags/exception/"/>
    
      <category term="ran_out_of_messages" scheme="http://yoursite.com/tags/ran-out-of-messages/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming 从指定时间戳开始消费 kafka 数据</title>
    <link href="http://yoursite.com/2016/12/02/spark-streaming-consume-kafka-message-from-specify-timestamp/"/>
    <id>http://yoursite.com/2016/12/02/spark-streaming-consume-kafka-message-from-specify-timestamp/</id>
    <published>2016-12-02T11:27:49.000Z</published>
    <updated>2017-06-03T03:39:56.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>从指定时间戳（比如 2 小时）开始消费 Kafka 数据</p>
<a id="more"></a>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>我们知道通过 Kafka 的 API 可以得到指定时间戳对应数据所在的 segment 的起始 offset。那么就可以通过这个功能来粗略的实现需求。</p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>我们知道 <code>KafkaUitls.createDirectStream</code> 这个接口可以指定起始点的 offset，那么我们需要做的就变成如下三步：</p>
<ol>
<li>获取 <code>topic</code> 对应的 <code>TopicAndPartitions</code>，得到当前 topic 有多少 partition</li>
<li>从 Kafka 获取每个 partition 指定时间戳所在 segment 的起始 offset</li>
<li>将步骤 2 中的 offset 作为参数传入 <code>createDirectStream</code> 即可<br>通过查看源码，我们知道步骤 1 和步骤 2 中的功能在 <code>org.apache.spark.streaming.kafka.KafkaCluster</code> 中都已经有现成的函数了：<code>getPartitions</code> 和 <code>getLeaderOffsets</code>，分别表示获取指定 topic 的 partition 以及获取 partition 指定时间戳所在的 segment 的起始 offset，那么我们需要做的就是如何调用这两个函数实现我们的功能了。</li>
</ol>
<p>我们知道 <code>KafkaCluster</code> 的作用域是 <code>private[spark]</code> 所以我们需要在自己的代码中使用 <code>package org.apache.spark(.xxx ... .yyy)</code>(小括号中表示可以省略）来限定自己的代码，因此我们可以将步骤 1 和步骤 2 中的功能实现如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">package org.apache.spark.streaming.kafka</div><div class="line">......      //省略其他不相关的代码</div><div class="line"></div><div class="line">def getPartitions(kafkaParams: Map[String, String], topics: Set[String]): Either[Err, Set[TopicAndPartition]] = &#123;</div><div class="line">        val kc = new KafkaCluster(kafkaParams)</div><div class="line">        kc.getPartitions(topics)    //我们可以在这里处理错误，也可以将错误继续往上传递</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    def getLeaderOffsets(kafkaParams: Map[String, String], topicAndPartitions: Set[TopicAndPartition], before: Long) : Map[TopicAndPartition, Long]  = &#123;</div><div class="line">        val kc = new KafkaCluster(kafkaParams)</div><div class="line">        val leaderOffsets = kc.getLeaderOffsets(topicAndPartitions, before)</div><div class="line">        if (leaderOffsets.isLeft) &#123;  //在本函数内部处理错误，如果有错误抛出异常</div><div class="line">            throw new RuntimeException(s&quot;### Exception when MTKafkaUtils#getLeaderOffsets $&#123;leaderOffsets.left.get&#125; ###&quot;)</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        leaderOffsets.right.get.map &#123; case (k, v) =&gt; (k, v.offset)&#125;  //将 Map[TopicAndPartition, LeaderOffset] 转变为 Map[TopicAndPartition, Long]（Long 为对应 partition 的 offset，从 LeaderOffset 中获取）</div><div class="line">    &#125;</div></pre></td></tr></table></figure>
<p>步骤 3 直接传入参数即可，就可以从指定时间戳开始消费 Kafka 数据了</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;需求&quot;&gt;&lt;a href=&quot;#需求&quot; class=&quot;headerlink&quot; title=&quot;需求&quot;&gt;&lt;/a&gt;需求&lt;/h2&gt;&lt;p&gt;从指定时间戳（比如 2 小时）开始消费 Kafka 数据&lt;/p&gt;
    
    </summary>
    
      <category term="分布式系统" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="实时计算" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
    
      <category term="kafka" scheme="http://yoursite.com/tags/kafka/"/>
    
      <category term="spark-streaming" scheme="http://yoursite.com/tags/spark-streaming/"/>
    
      <category term="specify_timestamp" scheme="http://yoursite.com/tags/specify-timestamp/"/>
    
      <category term="timestamp" scheme="http://yoursite.com/tags/timestamp/"/>
    
  </entry>
  
  <entry>
    <title>Spark Streaming 往 HDFS 写文件，自定义文件名</title>
    <link href="http://yoursite.com/2016/11/26/spark-streaming-e5-be-80-hdfs-e5-86-99-e6-96-87-e4-bb-b6-ef-bc-8c-e8-87-aa-e5-ae-9a-e4-b9-89-e6-96-87-e4-bb-b6-e5-90-8d/"/>
    <id>http://yoursite.com/2016/11/26/spark-streaming-e5-be-80-hdfs-e5-86-99-e6-96-87-e4-bb-b6-ef-bc-8c-e8-87-aa-e5-ae-9a-e4-b9-89-e6-96-87-e4-bb-b6-e5-90-8d/</id>
    <published>2016-11-26T08:09:25.000Z</published>
    <updated>2017-06-03T03:40:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>将 kafka 上的数据实时同步到 HDFS，不能有太多小文件</p>
<a id="more"></a>
<h2 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h2><p>Spark Streaming 支持 RDD#saveAsTextFile，将数据以 <strong>纯文本</strong> 方式写到 HDFS，我们查看 RDD#saveAsTextFile 可以看到</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">RDD.rddToPairRDDFunctions(r)(nullWritableClassTag, textClassTag, null)</div><div class="line">      .saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path)</div></pre></td></tr></table></figure>
<p>从上面这句话我们可以知道，首先将 RDD 转化为 PariRDD，然后再调用 saveAsHadoopFile 函数进行实际的操作。上面的语句中 <code>r</code> 是原始 RDD，<code>nullWritableClassTag</code> 和 <code>textClassTag</code> 表示所写数据的类型，使用 <code>nullWritableClassTag</code> 是因为 HDFS 不会将这个数据进行实际写入（pariRDD 是 (K,V) 类型， 我们只需要写入 V），从效果上看就只写如后面的一个字段。<code>TextOutputFormat</code> 是一个格式化函数，后面我们再来看这个函数，<code>NullWritable</code> 则表示一个占位符，同样是这个字段不需要实际写入 HDFS，<code>Text</code> 表示我们将写入文本类型的数据。</p>
<p>我们看到 <code>TextOutputFormat</code> 这个类中有一个函数是 <code>RecordWriter</code> 用于操作没一条记录的写入，代码如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">public RecordWriter&lt;K, V&gt; getRecordWriter(FileSystem ignored, JobConf job, String name, Progressable progress) throws IOException &#123;</div><div class="line">	boolean isCompressed = getCompressOutput(job);</div><div class="line">	String keyValueSeparator = job.get(&quot;mapreduce.output.textoutputformat.separator&quot;, &quot;\t&quot;);</div><div class="line">	if(!isCompressed) &#123;</div><div class="line">	    Path codecClass1 = FileOutputFormat.getTaskOutputPath(job, name);</div><div class="line">		FileSystem codec1 = codecClass1.getFileSystem(job);</div><div class="line">		FSDataOutputStream file1 = codec1.create(codecClass1, progress);</div><div class="line">		return new TextOutputFormat.LineRecordWriter(file1, keyValueSeparator);</div><div class="line">	&#125; else &#123;</div><div class="line">	    Class codecClass = getOutputCompressorClass(job, GzipCodec.class);</div><div class="line">		CompressionCodec codec = (CompressionCodec)ReflectionUtils.newInstance(codecClass, job);</div><div class="line">		Path file = FileOutputFormat.getTaskOutputPath(job, name + codec.getDefaultExtension());</div><div class="line">		FileSystem fs = file.getFileSystem(job);</div><div class="line">		FSDataOutputStream fileOut = fs.create(file, progress);</div><div class="line">		return new TextOutputFormat.LineRecordWriter(new DataOutputStream(codec.createOutputStream(fileOut)), keyValueSeparator);</div><div class="line">		&#125;</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<p>文件中分为两部分：1）压缩文件，2）非压缩文件。然后剩下的事情就是打开文件，往文件中写数据了。</p>
<p>说到压缩文件，就和写 lzo 格式关联起来了，因为 lzo 格式就是压缩的，那么我们从哪拿到这个压缩的格式的呢？实际上 PariRDDFunctions#saveAsHadoopFile 还可以传入压缩格式类，函数原型如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">def saveAsHadoopFile[F &lt;: OutputFormat[K, V]](</div><div class="line">    path: String,</div><div class="line">    codec: Class[_ &lt;: CompressionCodec])(implicit fm: ClassTag[F]): Unit = self.withScope &#123;</div></pre></td></tr></table></figure>
<p>这里第二个参数表示压缩的类。如果需要我们传入一个压缩类即可，如 <code>classOf[com.hadoop.compression.lzo.LzopCodec]</code> 最终这个参数会传给 <code>TextOutputFormat#RecordWriter</code>.</p>
<p>至此，我们以及可以写 lzo 格式的文件了。但是还没有结束，因为会产生小文件，每个 RDD 的每个 partition 都会在 HDFS 上产生一个文件，而且这些文件大小非常小，就形成了很多小文件，这对 HDFS 的压力会非常大。我们需要解决这个问题</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;需求&quot;&gt;&lt;a href=&quot;#需求&quot; class=&quot;headerlink&quot; title=&quot;需求&quot;&gt;&lt;/a&gt;需求&lt;/h2&gt;&lt;p&gt;将 kafka 上的数据实时同步到 HDFS，不能有太多小文件&lt;/p&gt;
    
    </summary>
    
      <category term="分布式系统" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="实时计算" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F/%E5%AE%9E%E6%97%B6%E8%AE%A1%E7%AE%97/"/>
    
    
      <category term="spark" scheme="http://yoursite.com/tags/spark/"/>
    
      <category term="append" scheme="http://yoursite.com/tags/append/"/>
    
      <category term="hdfs" scheme="http://yoursite.com/tags/hdfs/"/>
    
      <category term="spark-streaming" scheme="http://yoursite.com/tags/spark-streaming/"/>
    
  </entry>
  
</feed>
