<!DOCTYPE HTML>
<html>
<head>
  <meta charset="utf-8">
  
  <title>分布式系统 | Hexo</title>
  <meta name="author" content="John Doe">
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  
  <meta property="og:site_name" content="Hexo"/>

  
    <meta property="og:image" content="undefined"/>
  

  
  
    <link href="/favicon.png" rel="icon">
  

  <!-- CSS -->
  <link rel="stylesheet" href="/css/themes/bootstrap.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/font-awesome.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/style.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/responsive.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/highlight.css" media="screen" type="text/css">
  <link rel="stylesheet" href="/css/google-fonts.css" media="screen" type="text/css">
  <!--[if lt IE 9]><script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

  <script src="/js/jquery-2.0.3.min.js"></script>

  <!-- analytics -->
  



</head>

 <body>  
  <nav id="main-nav" class="navbar navbar-inverse navbar-default navbar-fixed-top" role="navigation">
    <div class="container">
      <button type="button" class="navbar-header navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
	<span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
       <a class="navbar-brand" href="/">Hexo</a>
      <div class="collapse navbar-collapse nav-menu">
		<ul class="nav navbar-nav">
		  
		  <li>
			<a href="/archives" title="All the articles.">
			  <i class="fa fa-archive"></i>Archives
			</a>
		  </li>
		  
		  <li>
			<a href="/categories" title="All the categories.">
			  <i class="fa fa-folder"></i>Categories
			</a>
		  </li>
		  
		  <li>
			<a href="/tags" title="All the tags.">
			  <i class="fa fa-tags"></i>Tags
			</a>
		  </li>
		  
		  <li>
			<a href="/about" title="About me.">
			  <i class="fa fa-user"></i>About
			</a>
		  </li>
		  
		</ul>
      </div>
    </div> <!-- container -->
</nav>
<div class="clearfix"></div>

  <div class="container">
  	<div class="content">
    	 

<!-- title -->
<div class="page-header page-header-inverse ">
  <h1 class="archive-title-category title title-inverse ">分布式系统</h1>
</div>

<div class="row page">
  <!-- cols -->
  
  <div class="col-md-9">
	

	  <div id="top_search"></div>

      

      <div class="mypage">
	  
	    	
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2017-02-16 </div>
			<div class="article-title"><a href="/2017/02/16/spark-streaming-consume-kafka-at-00-second-of-every-minute/" >Spark Streaming 统一在每分钟的 00 秒消费 Kafka 数据的问题解决</a></div>						
		</h3>
	


	    	<div class="entry">
  <div class="row">
	
	
		<h2 id="现象"><a href="#现象" class="headerlink" title="现象"></a>现象</h2><p>一批 Spark Streaming 会统一在每分钟的 00 秒开始消费 kafka 数据</p>
<h2 id="问题排查"><a href="#问题排查" class="headerlink" title="问题排查"></a>问题排查</h2><p>这一批作业的功能就是从 kafka 消费数据，进行转化后存储到外部可靠介质中。所有作业的 <code>batchDuration</code> 都设置为 60s。<br>我们追踪代码可以得到在 <code>JobGenerator</code> 中有一个变量 <code>timer : RecurringTimer</code>，改变量用于定时的启动 task 去消费数据。<br>从 <code>RecurringTimer#getStartTime</code> 我们可以得到作业第一个 batch 的启动时间，后续的 batch 启动时间则是在第一个 batch 的启动时间上加上 <code>batchDuration</code> 的整数倍。<br>第一个 batch 的起动时间实现如下：<br><code>(math.floor(clock.getTimeMillis().toDouble / period) + 1).toLong * period</code><br>其中 <code>clock.getTimeMillis()</code> 是当前时间，period 是<code>batchDuration</code> 的毫秒表示法。通过上述公式，我们可以知道作业的启动时间会对齐到 <code>batchDuration</code>，而我们把这一批作业的 <code>batchDuration</code> 都设置为 60s，因此都会在每分钟的 00 秒开始消费 kafka 数据。</p>
<h2 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h2><p>我们可以通过下面两种方式进行解决</p>
<ol>
<li>设置不同的 <code>batchDuration</code></li>
<li>改写 <code>RecurringTimer#getStartTime</code> 的逻辑，在上述对齐的时间基础上加上一个 [0, period) 范围内的随机数</li>
</ol>
<p>我们知道在上述两种解决方案中，第一种，不同作业还是会在某一时刻重合，而且这个重合的时间点不可控，可能是作业运行一小时后，可能是运行一天后，也可能是运行一周后。而第二种作业则是可控的，在作业启动时就决定了。因此这里我们采用第二种方案。</p>
<p>本文采用了一种新的排版方式，在进行实验，如果效果好的好，后续大部分内容都会以这种形式进行发布</p>
<div class="markdown-here-wrapper" style="font-size: 16px; line-height: 1.8em; letter-spacing: 0.1em;" data-md-url="http://www.klion26.com/wp-admin/post-new.php"></div>

	
	</div>
</div>

	  
	    	
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2016-12-02 </div>
			<div class="article-title"><a href="/2016/12/02/spark-streaming-consume-kafka-message-from-specify-timestamp/" >Spark Streaming 从指定时间戳开始消费 kafka 数据</a></div>						
		</h3>
	


	    	<div class="entry">
  <div class="row">
	
	
		<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>从指定时间戳（比如 2 小时）开始消费 Kafka 数据</p>
<h2 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h2><p>我们知道通过 Kafka 的 API 可以得到指定时间戳对应数据所在的 segment 的起始 offset。那么就可以通过这个功能来粗略的实现需求。</p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>我们知道 <code>KafkaUitls.createDirectStream</code> 这个接口可以指定起始点的 offset，那么我们需要做的就变成如下三步：</p>
<ol>
<li>获取 <code>topic</code> 对应的 <code>TopicAndPartitions</code>，得到当前 topic 有多少 partition</li>
<li>从 Kafka 获取每个 partition 指定时间戳所在 segment 的起始 offset</li>
<li>将步骤 2 中的 offset 作为参数传入 <code>createDirectStream</code> 即可<br>通过查看源码，我们知道步骤 1 和步骤 2 中的功能在 <code>org.apache.spark.streaming.kafka.KafkaCluster</code> 中都已经有现成的函数了：<code>getPartitions</code> 和 <code>getLeaderOffsets</code>，分别表示获取指定 topic 的 partition 以及获取 partition 指定时间戳所在的 segment 的起始 offset，那么我们需要做的就是如何调用这两个函数实现我们的功能了。</li>
</ol>
<p>我们知道 <code>KafkaCluster</code> 的作用域是 <code>private[spark]</code> 所以我们需要在自己的代码中使用 <code>package org.apache.spark(.xxx ... .yyy)</code>(小括号中表示可以省略）来限定自己的代码，因此我们可以将步骤 1 和步骤 2 中的功能实现如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">package org.apache.spark.streaming.kafka</div><div class="line">......      //省略其他不相关的代码</div><div class="line"></div><div class="line">def getPartitions(kafkaParams: Map[String, String], topics: Set[String]): Either[Err, Set[TopicAndPartition]] = &#123;</div><div class="line">        val kc = new KafkaCluster(kafkaParams)</div><div class="line">        kc.getPartitions(topics)    //我们可以在这里处理错误，也可以将错误继续往上传递</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    def getLeaderOffsets(kafkaParams: Map[String, String], topicAndPartitions: Set[TopicAndPartition], before: Long) : Map[TopicAndPartition, Long]  = &#123;</div><div class="line">        val kc = new KafkaCluster(kafkaParams)</div><div class="line">        val leaderOffsets = kc.getLeaderOffsets(topicAndPartitions, before)</div><div class="line">        if (leaderOffsets.isLeft) &#123;  //在本函数内部处理错误，如果有错误抛出异常</div><div class="line">            throw new RuntimeException(s&quot;### Exception when MTKafkaUtils#getLeaderOffsets $&#123;leaderOffsets.left.get&#125; ###&quot;)</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        leaderOffsets.right.get.map &#123; case (k, v) =&gt; (k, v.offset)&#125;  //将 Map[TopicAndPartition, LeaderOffset] 转变为 Map[TopicAndPartition, Long]（Long 为对应 partition 的 offset，从 LeaderOffset 中获取）</div><div class="line">    &#125;</div></pre></td></tr></table></figure>
<p>步骤 3 直接传入参数即可，就可以从指定时间戳开始消费 Kafka 数据了</p>

	
	</div>
</div>

	  
	    	
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2016-11-26 </div>
			<div class="article-title"><a href="/2016/11/26/spark-streaming-e5-be-80-hdfs-e5-86-99-e6-96-87-e4-bb-b6-ef-bc-8c-e8-87-aa-e5-ae-9a-e4-b9-89-e6-96-87-e4-bb-b6-e5-90-8d/" >Spark Streaming 往 HDFS 写文件，自定义文件名</a></div>						
		</h3>
	


	    	<div class="entry">
  <div class="row">
	
	
		<h2 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h2><p>将 kafka 上的数据实时同步到 HDFS，不能有太多小文件</p>
<h2 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h2><p>Spark Streaming 支持 RDD#saveAsTextFile，将数据以 <strong>纯文本</strong> 方式写到 HDFS，我们查看 RDD#saveAsTextFile 可以看到</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">RDD.rddToPairRDDFunctions(r)(nullWritableClassTag, textClassTag, null)</div><div class="line">      .saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path)</div></pre></td></tr></table></figure>
<p>从上面这句话我们可以知道，首先将 RDD 转化为 PariRDD，然后再调用 saveAsHadoopFile 函数进行实际的操作。上面的语句中 <code>r</code> 是原始 RDD，<code>nullWritableClassTag</code> 和 <code>textClassTag</code> 表示所写数据的类型，使用 <code>nullWritableClassTag</code> 是因为 HDFS 不会将这个数据进行实际写入（pariRDD 是 (K,V) 类型， 我们只需要写入 V），从效果上看就只写如后面的一个字段。<code>TextOutputFormat</code> 是一个格式化函数，后面我们再来看这个函数，<code>NullWritable</code> 则表示一个占位符，同样是这个字段不需要实际写入 HDFS，<code>Text</code> 表示我们将写入文本类型的数据。</p>
<p>我们看到 <code>TextOutputFormat</code> 这个类中有一个函数是 <code>RecordWriter</code> 用于操作没一条记录的写入，代码如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">public RecordWriter&lt;K, V&gt; getRecordWriter(FileSystem ignored, JobConf job, String name, Progressable progress) throws IOException &#123;</div><div class="line">	boolean isCompressed = getCompressOutput(job);</div><div class="line">	String keyValueSeparator = job.get(&quot;mapreduce.output.textoutputformat.separator&quot;, &quot;\t&quot;);</div><div class="line">	if(!isCompressed) &#123;</div><div class="line">	    Path codecClass1 = FileOutputFormat.getTaskOutputPath(job, name);</div><div class="line">		FileSystem codec1 = codecClass1.getFileSystem(job);</div><div class="line">		FSDataOutputStream file1 = codec1.create(codecClass1, progress);</div><div class="line">		return new TextOutputFormat.LineRecordWriter(file1, keyValueSeparator);</div><div class="line">	&#125; else &#123;</div><div class="line">	    Class codecClass = getOutputCompressorClass(job, GzipCodec.class);</div><div class="line">		CompressionCodec codec = (CompressionCodec)ReflectionUtils.newInstance(codecClass, job);</div><div class="line">		Path file = FileOutputFormat.getTaskOutputPath(job, name + codec.getDefaultExtension());</div><div class="line">		FileSystem fs = file.getFileSystem(job);</div><div class="line">		FSDataOutputStream fileOut = fs.create(file, progress);</div><div class="line">		return new TextOutputFormat.LineRecordWriter(new DataOutputStream(codec.createOutputStream(fileOut)), keyValueSeparator);</div><div class="line">		&#125;</div><div class="line">	&#125;</div></pre></td></tr></table></figure>
<p>文件中分为两部分：1）压缩文件，2）非压缩文件。然后剩下的事情就是打开文件，往文件中写数据了。</p>
<p>说到压缩文件，就和写 lzo 格式关联起来了，因为 lzo 格式就是压缩的，那么我们从哪拿到这个压缩的格式的呢？实际上 PariRDDFunctions#saveAsHadoopFile 还可以传入压缩格式类，函数原型如下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">def saveAsHadoopFile[F &lt;: OutputFormat[K, V]](</div><div class="line">    path: String,</div><div class="line">    codec: Class[_ &lt;: CompressionCodec])(implicit fm: ClassTag[F]): Unit = self.withScope &#123;</div></pre></td></tr></table></figure>
<p>这里第二个参数表示压缩的类。如果需要我们传入一个压缩类即可，如 <code>classOf[com.hadoop.compression.lzo.LzopCodec]</code> 最终这个参数会传给 <code>TextOutputFormat#RecordWriter</code>.</p>
<p>至此，我们以及可以写 lzo 格式的文件了。但是还没有结束，因为会产生小文件，每个 RDD 的每个 partition 都会在 HDFS 上产生一个文件，而且这些文件大小非常小，就形成了很多小文件，这对 HDFS 的压力会非常大。我们需要解决这个问题</p>

	
	</div>
</div>

	  
	    	
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2016-11-01 </div>
			<div class="article-title"><a href="/2016/11/01/spark-streaming-topic-partition-change-auto-adaptive/" >Spark Streaming 自适应上游 kafka topic partition 数目变化</a></div>						
		</h3>
	


	    	<div class="entry">
  <div class="row">
	
	
		<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Spark Streaming 作业在运行过程中，上游 topic 增加 partition 数目从 A 增加到 B，会造成作业丢失数据，因为该作业只从 topic 中读取了原来的 A 个 partition 的数据，新增的 B-A 个 partition 的数据会被忽略掉。</p>
<h2 id="思考过程"><a href="#思考过程" class="headerlink" title="思考过程"></a>思考过程</h2><p>为了作业能够长时间的运行，一开始遇到这种情况的时候，想到两种方案：</p>
<ol>
<li>感知上游 topic 的 partition 数目变化，然后发送报警，让用户重启</li>
<li>直接在作业内部自适应上游 topic partition 的变化，完全不影响作业<br>方案 1 是简单直接，第一反应的结果，但是效果不好，需要用户人工介入，而且需要删除 checkpoint 文件</li>
</ol>
<p>方案 2 从根本上解决问题，用户不需要关心上游 partition 数目的变化，但是第一眼会觉得较难实现。</p>
<p>方案 1 很快被 pass 掉，因为人工介入的成本太高，而且实现起来很别扭。接下来考虑方案 2.</p>
<p>Spark Streaming 程序中使用 Kafka 的最原始方式为 <code>KafkaUtils.createDirectStream</code> 通过源码，我们找到调用链条大致是这样的</p>
<p><span style="color: #0000ff;"><strong><code>KafkaUtils.createDirectStream</code></strong></span>   –&gt;   <strong><span style="color: #0000ff;"><code>new DirectKafkaInputDStream</code></span></strong> –&gt; 最终由 <code>DirectKafkaInputDStream#compute(validTime : Time)</code> 函数来生成 KafkaRDD。</p>
<p>而 KafkaRDD 的 partition 数和 <strong><span style="color: #0000ff;">作业开始运行时</span></strong> topic 的 partition 数一致，topic 的 partition 数保存在 currentOffsets 变量中，currentOffsets 是一个 Map[TopicAndPartition, Long]类型的变量，保存每个 partition 当前消费的 offset 值，但是作业运行过程中 currentOffsets 不会增加 key，就是说不会增加 KafkaRDD 的 partition，这样导致每次生成 KafkaRDD 的时候都使用 <span style="color: #0000ff;"><strong>作业开始运行时</strong></span> topic 的 partition 数作为 KafkaRDD 的 partition 数，从而会造成数据的丢失。</p>
<h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>我们只需要在每次生成 KafkaRDD 的时候，将 currentOffsets 修正为正常的值（往里面增加对应的 partition 数，总共 B-A 个，以及每个增加的 partition 的当前 offset 从零开始）。</p>
<ul>
<li>第一个问题出现了，我们不能修改 Spark 的源代码，重新进行编译，因为这不是我们自己维护的。想到的一种方案是继承 DirectKafkaInputDStream。我们发现不能继承 DirectKafkaInputDStream 该类，因为这个类是使用 <code>private[streaming]</code> 修饰的。</li>
<li>第二个问题出现了，怎么才能够继承 DirectKafkaInputDStream，这时我们只需要将希望继承 DirectKafkaInputDStream 的类放到一个单独的文件 F 中，文件 F 使用 <code>package org.apache.spark.streaming</code> 进行修饰即可，这样可以绕过不能继承 DirectKafkaInputDStream 的问题。这个问题解决后，我们还需要修改 <code>Object KafkaUtils</code>，让该 Object 内部调用我们修改后的 DirectKafkaInputDStream（我命名为 MTDirectKafkaInputDStream)</li>
<li>第三个问题如何让 Spark 调用 MTDirectKafkaInputDStream，而不是 DirectKafkaInputDStream，这里我们使用简单粗暴的方式，将 KafkaUtils 的代码 copy 一份，然后将其中调用 DirectKafkaInputDStream 的部分都修改为 MTDirectKafkaInputDStream，这样就实现了我们的需要。当然该文件也需要使用 <code>package org.apache.spark.streaming</code> 进行修饰</li>
<li>第二个和第三个问题的解决方案在  中国 Spark 技术峰会 2016  上，广点通的 林立伟 有提及，后续会进行尝试<br>总结下，我们需要做两件事</li>
</ul>
<ol>
<li>修改 DirectKafkaInputDStream#compute 使得能够自适应 topic 的 partition 变更</li>
<li>修改 KafkaUtils，使得我们能够调用修改过后的 DirectKafkaInputDStream</li>
</ol>
<h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div></pre></td><td class="code"><pre><div class="line">package org.apache.spark.streaming.kafka.mt</div><div class="line"></div><div class="line">import com.meituan.data.util.Constants</div><div class="line">import com.meituan.service.inf.kms.client.Kms</div><div class="line">import kafka.common.&#123;ErrorMapping, TopicAndPartition&#125;</div><div class="line">import kafka.javaapi.&#123;TopicMetadata, TopicMetadataRequest&#125;</div><div class="line">import kafka.javaapi.consumer.SimpleConsumer</div><div class="line">import kafka.message.MessageAndMetadata</div><div class="line">import kafka.serializer.Decoder</div><div class="line">import org.apache.spark.streaming.&#123;StreamingContext, Time&#125;</div><div class="line">import org.apache.spark.streaming.kafka.&#123;DirectKafkaInputDStream, KafkaRDD&#125;</div><div class="line"></div><div class="line">import scala.collection.JavaConverters._</div><div class="line">import scala.util.control.Breaks._</div><div class="line">import scala.reflect.ClassTag</div><div class="line"></div><div class="line">/**</div><div class="line">  * Created by qiucongxian on 10/27/16.</div><div class="line">  */</div><div class="line">class MTDirectKafkaInputDStream[</div><div class="line">  K: ClassTag,</div><div class="line">  V: ClassTag,</div><div class="line">  U &lt;: Decoder[K]: ClassTag,</div><div class="line">  T &lt;: Decoder[V]: ClassTag,</div><div class="line">  R: ClassTag](</div><div class="line">    @transient ssc_ : StreamingContext,</div><div class="line">    val MTkafkaParams: Map[String, String],</div><div class="line">    val MTfromOffsets: Map[TopicAndPartition, Long],</div><div class="line">    messageHandler: MessageAndMetadata[K, V] =&gt; R</div><div class="line">) extends DirectKafkaInputDStream[K, V, U, T, R](ssc_, MTkafkaParams , MTfromOffsets, messageHandler) &#123;</div><div class="line">    private val kafkaBrokerList : String = &quot;host1:port1,host2:port2,host3:port3&quot; //根据自己的情况自行修改</div><div class="line"></div><div class="line">    override def compute(validTime: Time) : Option[KafkaRDD[K, V, U, T, R]] = &#123;</div><div class="line">      /**</div><div class="line">        * 在这更新 currentOffsets 从而做到自适应上游 partition 数目变化</div><div class="line">        */</div><div class="line">        updateCurrentOffsetForKafkaPartitionChange()</div><div class="line">        super.compute(validTime)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    private def updateCurrentOffsetForKafkaPartitionChange() : Unit = &#123;</div><div class="line">      val topic = currentOffsets.head._1.topic</div><div class="line">      val nextPartitions : Int = getTopicMeta(topic) match &#123;</div><div class="line">          case Some(x) =&gt; x.partitionsMetadata.size()</div><div class="line">          case _ =&gt; 0</div><div class="line">      &#125;</div><div class="line">      val currPartitions = currentOffsets.keySet.size</div><div class="line"></div><div class="line">      if (nextPartitions &gt; currPartitions) &#123;</div><div class="line">        var i = currPartitions</div><div class="line">        while (i &lt; nextPartitions) &#123;</div><div class="line">           currentOffsets = currentOffsets + (TopicAndPartition(topic, i) -&gt; 0)</div><div class="line">           i = i + 1</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">      logInfo(s&quot;######### $&#123;nextPartitions&#125;  currentParttions $&#123;currentOffsets.keySet.size&#125; ########&quot;)</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    private def getTopicMeta(topic: String) : Option[TopicMetadata] = &#123;</div><div class="line">        var metaData : Option[TopicMetadata] = None</div><div class="line">        var consumer : Option[SimpleConsumer] = None</div><div class="line"></div><div class="line">        val topics = List[String](topic)</div><div class="line">        val brokerList = kafkaBrokerList.split(&quot;,&quot;)</div><div class="line">        brokerList.foreach(</div><div class="line">          item =&gt; &#123;</div><div class="line">            val hostPort = item.split(&quot;:&quot;)</div><div class="line">            try &#123;</div><div class="line">              breakable &#123;</div><div class="line">                  for (i &lt;- 0 to 3) &#123;</div><div class="line">                      consumer = Some(new SimpleConsumer(host = hostPort(0), port = hostPort(1).toInt,</div><div class="line">                                            soTimeout = 10000, bufferSize = 64 * 1024, clientId = &quot;leaderLookup&quot;))</div><div class="line">                      val req : TopicMetadataRequest = new TopicMetadataRequest(topics.asJava)</div><div class="line">                      val resp = consumer.get.send(req)</div><div class="line"></div><div class="line">                      metaData = Some(resp.topicsMetadata.get(0))</div><div class="line">                      if (metaData.get.errorCode == ErrorMapping.NoError) break()</div><div class="line">                  &#125;</div><div class="line">              &#125;</div><div class="line">            &#125; catch &#123;</div><div class="line">              case e =&gt; logInfo(s&quot; ###### Error in MTDirectKafkaInputDStream $&#123;e&#125; ######&quot;)</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        )</div><div class="line">        metaData</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在修改过后的 KafkaUtils 文件中，将所有的 <code>DirectKafkaInputDStream</code> 都替换为 <code>MTDirectKafkaInputDStream</code> 即可</p>

	
	</div>
</div>

	  
	    	
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2016-10-22 </div>
			<div class="article-title"><a href="/2016/10/22/storm-e7-9a-84-e5-8f-af-e9-9d-a0-e6-80-a7-e4-bf-9d-e8-af-81-e6-b5-8b-e8-af-95/" >Storm 的可靠性保证测试</a></div>						
		</h3>
	


	    	<div class="entry">
  <div class="row">
	
	
		<p><span style="color: #ff0000;">文章首发于 <a href="http://tech.meituan.com/test-of-storms-reliability.html" target="_blank" rel="external">美团点评技术博客</a></span></p>
<p><a href="http://storm.apache.org/" target="_blank" rel="external">Storm</a> 是一个分布式的实时计算框架，可以很方便地对流式数据进行实时处理和分析，能运用在实时分析、在线数据挖掘、持续计算以及分布式 RPC 等场景下。Storm 的实时性可以使得数据从收集到处理展示在秒级别内完成，从而为业务方决策提供实时的数据支持。</p>
<p>在美团点评公司内部，实时计算主要应用场景包括实时日志解析、用户行为分析、实时消息推送、消费趋势展示、实时新客判断、实时活跃用户数统计等。这些数据提供给各事业群，并作为他们实时决策的有力依据，弥补了离线计算“T+1”的不足。</p>
<p>在实时计算中，用户不仅仅关心时效性的问题，同时也关心消息处理的成功率。本文将通过实验验证 Storm 的消息可靠性保证机制，文章分为消息保证机制、测试目的、测试环境、测试场景以及总结等五节。</p>
<h2 id="Storm-的消息保证机制"><a href="#Storm-的消息保证机制" class="headerlink" title="Storm 的消息保证机制"></a>Storm 的消息保证机制</h2><p>Storm 提供了三种不同层次的消息保证机制，分别是 At Most Once、At Least Once 以及 Exactly Once。消息保证机制依赖于消息是否被完全处理。</p>
<h3 id="消息完全处理"><a href="#消息完全处理" class="headerlink" title="消息完全处理"></a>消息完全处理</h3><p>每个从 Spout（Storm 中数据源节点）发出的 Tuple（Storm 中的最小消息单元）可能会生成成千上万个新的 Tuple，形成一棵 Tuple 树，当整棵 Tuple 树的节点都被成功处理了，我们就说从 Spout 发出的 Tuple 被完全处理了。 我们可以通过下面的例子来更好地诠释消息被完全处理这个概念：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">TopologyBuilder builder = new TopologyBuilder();</div><div class="line">builder.setSpout(&quot;sentences&quot;, new KafkaSpout(spoutConfig), spoutNum);</div><div class="line">builder.setBolt(&quot;split&quot;, new SplitSentence(), 10)</div><div class="line">    .shuffleGrouping(&quot;sentences&quot;);</div><div class="line">builder.setBolt(&quot;count&quot;, new WordCount(), 20)</div><div class="line">    .fieldsGrouping(&quot;split&quot;, new Fields(&quot;word&quot;));</div></pre></td></tr></table></figure>
<p>这个 Topology 从 Kafka（一个开源的分布式消息队列）读取信息发往下游，下游的 Bolt 将收到的句子分割成单独的单词，并进行计数。每一个从 Spout 发送出来的 Tuple 会衍生出多个新的 Tuple，从 Spout 发送出来的 Tuple 以及后续衍生出来的 Tuple 形成一棵 Tuple 树，下图是一棵 Tuple 树示例：</p>
<p><img src="http://tech.meituan.com/img/test-of-storm&#39;s-reliability/tuple_tree.png" alt="Tuple 树示例图"></p>
<p>上图中所有的 Tuple 都被成功处理了，我们才认为 Spout 发出的 Tuple 被完全处理。如果在一个固定的时间内（这个时间可以配置，默认为 30 秒），有至少一个 Tuple 处理失败或超时，则认为整棵 Tuple 树处理失败，即从 Spout 发出的 Tuple 处理失败。</p>
<h3 id="如何实现不同层次的消息保证机制"><a href="#如何实现不同层次的消息保证机制" class="headerlink" title="如何实现不同层次的消息保证机制"></a>如何实现不同层次的消息保证机制</h3><p><img src="http://tech.meituan.com/img/test-of-storm&#39;s-reliability/spout_bolt_acker.png" alt="spout_bolt_acker"></p>
<p>Tuple 的完全处理需要 Spout、Bolt 以及 Acker（Storm 中用来记录某棵 Tuple 树是否被完全处理的节点）协同完成，如上图所示。从 Spout 发送 Tuple 到下游，并把相应信息通知给 Acker，整棵 Tuple 树中某个 Tuple 被成功处理了都会通知 Acker，待整棵 Tuple 树都被处理完成之后，Acker 将成功处理信息返回给 Spout；如果某个 Tuple 处理失败，或者超时，Acker 将会给 Spout 发送一个处理失败的消息，Spout 根据 Acker 的返回信息以及用户对消息保证机制的选择判断是否需要进行消息重传。</p>
<p>Storm 提供的三种不同消息保证机制中。利用 Spout、Bolt 以及 Acker 的组合我们可以实现 At Most Once 以及 At Least Once 语义，Storm 在 At Least Once 的基础上进行了一次封装（Trident），从而实现 Exactly Once 语义。</p>
<p>Storm 的消息保证机制中，如果需要实现 At Most Once 语义，只需要满足下面任何一条即可：</p>
<ul>
<li>关闭 ACK 机制，即 Acker 数目设置为 0</li>
<li><p>Spout 不实现可靠性传输</p>
</li>
<li><p>Spout 发送消息是使用不带 message ID 的 API</p>
</li>
<li>不实现 fail 函数</li>
<li>Bolt 不把处理成功或失败的消息发送给 Acker</li>
</ul>
<p>如果需要实现 At Least Once 语义，则需要同时保证如下几条：</p>
<ul>
<li>开启 ACK 机制，即 Acker 数目大于 0</li>
<li>Spout 实现可靠性传输保证</li>
<li>Spout 发送消息时附带 message 的 ID</li>
<li>如果收到 Acker 的处理失败反馈，需要进行消息重传，即实现 fail 函数</li>
<li>Bolt 在处理成功或失败后需要调用相应的方法通知 Acker<br>实现 Exactly Once 语义，则需要在 At Least Once 的基础上进行状态的存储，用来防止重复发送的数据被重复处理，在 Storm 中使用 Trident API 实现。</li>
</ul>
<p>下图中，每种消息保证机制中左边的字母表示上游发送的消息，右边的字母表示下游接收到的消息。从图中可以知道，At Most Once 中，消息可能会丢失（上游发送了两个 A，下游只收到一个 A）；At Least Once 中，消息不会丢失，可能重复（上游只发送了一个 B ，下游收到两个 B）；Exactly Once 中，消息不丢失、不重复，因此需要在 At Least Once 的基础上保存相应的状态，表示上游的哪些消息已经成功发送到下游，防止同一条消息发送多次给下游的情况。</p>
<p><img src="http://tech.meituan.com/img/test-of-storm&#39;s-reliability/3_compare.png" alt="三种消息保证机制比较图"></p>
<h2 id="测试目的"><a href="#测试目的" class="headerlink" title="测试目的"></a>测试目的</h2><p>Storm 官方提供 At Most Once、At Least Once 以及 Exactly Once 三种不同层次的消息保证机制，我们希望通过相关测试，达到如下目的：</p>
<ul>
<li>三种消息保证机制的表现，是否与官方的描述相符；</li>
<li>At Most Once 语义下，消息的丢失率和什么有关系、关系如何；</li>
<li>At Least Once 语义下，消息的重复率和什么有关系、关系如何。</li>
</ul>
<h2 id="测试环境"><a href="#测试环境" class="headerlink" title="测试环境"></a>测试环境</h2><p>本文的测试环境如下: 每个 worker（worker 为一个 物理 JVM 进程，用于运行实际的 Storm 作业）分配 1 CPU 以及 1.6G 内存。Spout、Bolt、Acker 分别跑在单独的 worker 上。并通过在程序中控制抛出异常以及人工 Kill Spout/Bolt/Acker 的方式来模拟实际情况中的异常情况。</p>
<p>三种消息保证机制的测试均由 Spout 从 Kafka 读取测试数据，经由相应 Bolt 进行处理，然后发送到 Kafka，并将 Kafka 上的数据同步到 MySQL 方便最终结果的统计，如下图所示：</p>
<p><img src="http://tech.meituan.com/img/test-of-storm&#39;s-reliability/test-flow.png" alt="测试流程示意图"></p>
<p>测试数据为 Kafka 上顺序保存的一系列纯数字，数据量分别有十万、五十万、一百万等，每个数字在每个测试样例中出现且仅出现一次。</p>
<h2 id="测试场景"><a href="#测试场景" class="headerlink" title="测试场景"></a>测试场景</h2><p>对于三种不同的消息保证机制，我们分别设置了不同的测试场景，来进行充分的测试。其中为了保证 Spout/Bolt/Acker 发生异常的情况下不影响其他节点，在下面的测试中，所有的节点单独运行在独立的 Worker 上。</p>
<h3 id="At-Most-Once"><a href="#At-Most-Once" class="headerlink" title="At Most Once"></a>At Most Once</h3><p>从背景中可以得知，如果希望实现 At Most Once 语义，将 Acker 的数目设置为 0 即可，本文的测试过程中通过把设置 Acker 为 0 来进行 At Most Once 的测试。</p>
<h4 id="输入数据"><a href="#输入数据" class="headerlink" title="输入数据"></a>输入数据</h4><p>保存在 Kafka 上的一系列纯数字，数据量从十万到五百万不等，每个测试样例中，同一个数字在 Kafka 中出现且仅出现一次。</p>
<h4 id="测试结果"><a href="#测试结果" class="headerlink" title="测试结果"></a>测试结果</h4><table><br><thead><br><tr><br><th>异常次数</th><br><th>测试数据总量</th><br><th>结果集中不同 Tuple 的总量</th><br><th>丢失的 Tuple 数据量</th><br><th>Tuple 的丢失百分比</th><br><th>Tuple 的重复量</th><br></tr><br></thead><br><tbody><br><tr><br><td>0</td><br><td>500000</td><br><td>500000</td><br><td>0</td><br><td>0%</td><br><td>0</td><br></tr><br><tr><br><td>0</td><br><td>1000000</td><br><td>1000000</td><br><td>0</td><br><td>0%</td><br><td>0</td><br></tr><br><tr><br><td>0</td><br><td>2000000</td><br><td>2000000</td><br><td>0</td><br><td>0%</td><br><td>0</td><br></tr><br><tr><br><td>0</td><br><td>3000000</td><br><td>3000000</td><br><td>0</td><br><td>0%</td><br><td>0</td><br></tr><br></tbody><br></table><br><table><br><thead><br><tr><br><th>异常次数</th><br><th>测试数据总量</th><br><th>结果集中不同 Tuple 的总量</th><br><th>丢失的 Tuple 数据量</th><br><th>Tuple 的丢失百分比</th><br><th>Tuple 的重复量</th><br></tr><br></thead><br><tbody><br><tr><br><td>1</td><br><td>3000000</td><br><td>2774940</td><br><td>225060</td><br><td>7.50%</td><br><td>0</td><br></tr><br><tr><br><td>2</td><br><td>3000000</td><br><td>2307087</td><br><td>692913</td><br><td>23.09%</td><br><td>0</td><br></tr><br><tr><br><td>3</td><br><td>3000000</td><br><td>2082823</td><br><td>917177</td><br><td>30.57%</td><br><td>0</td><br></tr><br><tr><br><td>4</td><br><td>3000000</td><br><td>1420725</td><br><td>1579275</td><br><td>52.64%</td><br><td>0</td><br></tr><br></tbody><br></table>

<h4 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h4><p>不发生异常的情况下，消息能够不丢不重；Bolt 发生异常的情况下，消息会丢失，不会重复，其中消息的<strong>丢失数目</strong>与<strong>异常次数正相关</strong>。与官方文档描述相符，符合预期。</p>
<h3 id="At-Least-Once"><a href="#At-Least-Once" class="headerlink" title="At Least Once"></a>At Least Once</h3><p>为了实现 At Least Once 语义，需要 Spout、Bolt、Acker 进行配合。我们使用 Kafka-Spout 并通过自己管理 offset 的方式来实现可靠的 Spout；Bolt 通过继承 BaseBasicBolt，自动帮我们建立 Tuple 树以及消息处理之后通知 Acker；将 Acker 的数目设置为 1，即打开 ACK 机制，这样整个 Topology 即可提供 At Least Once 的语义。</p>
<h4 id="测试数据"><a href="#测试数据" class="headerlink" title="测试数据"></a>测试数据</h4><p>Kafka 上保存的十万到五十万不等的纯数字，其中每个测试样例中，每个数字在 Kafka 中出现且仅出现一次。</p>
<h4 id="测试结果-1"><a href="#测试结果-1" class="headerlink" title="测试结果"></a>测试结果</h4><p>Acker 发生异常的情况<br>    <table><br>    <thead><br>    <tr><br>    <th>异常的次数</th><br>    <th>测试数据总量</th><br>    <th>结果集中不重复的 Tuple 数</th><br>    <th>数据重复的次数（&gt;1)</th><br>    <th>出现重复的 Tuple 数</th><br>    <th>数据丢失数量</th><br>    <th>最大积压量</th><br>    </tr><br>    </thead><br>    <tbody><br>    <tr><br>    <td>0</td><br>    <td>100000</td><br>    <td>100000</td><br>    <td>-</td><br>    <td>-</td><br>    <td>0</td><br>    <td>2000（默认值）</td><br>    </tr><br>    <tr><br>    <td>0</td><br>    <td>200000</td><br>    <td>200000</td><br>    <td>-</td><br>    <td>-</td><br>    <td>0</td><br>    <td>2000</td><br>    </tr><br>    <tr><br>    <td>0</td><br>    <td>300000</td><br>    <td>300000</td><br>    <td>-</td><br>    <td>-</td><br>    <td>0</td><br>    <td>2000</td><br>    </tr><br>    <tr><br>    <td>0</td><br>    <td>400000</td><br>    <td>400000</td><br>    <td>-</td><br>    <td>-</td><br>    <td>0</td><br>    <td>2000</td><br>    </tr><br>    </tbody><br>    </table><br>    <table><br>    <thead><br>    <tr><br>    <th>异常的次数</th><br>    <th>测试数据总量</th><br>    <th>结果集中不重复的 Tuple 数</th><br>    <th>数据重复的次数（&gt;1)</th><br>    <th>出现重复的 Tuple 数</th><br>    <th>数据丢失数量</th><br>    <th>最大积压量</th><br>    </tr><br>    </thead><br>    <tbody><br>    <tr><br>    <td>1</td><br>    <td>100000</td><br>    <td>100000</td><br>    <td>2</td><br>    <td>2000</td><br>    <td>0</td><br>    <td>2000</td><br>    </tr><br>    <tr><br>    <td>2</td><br>    <td>100000</td><br>    <td>100000</td><br>    <td>2</td><br>    <td>4001</td><br>    <td>0</td><br>    <td>2000</td><br>    </tr><br>    <tr><br>    <td>3</td><br>    <td>100000</td><br>    <td>100000</td><br>    <td>2</td><br>    <td>6000</td><br>    <td>0</td><br>    <td>2000</td><br>    </tr><br>    <tr><br>    <td>4</td><br>    <td>100000</td><br>    <td>100000</td><br>    <td>2</td><br>    <td>8000</td><br>    <td>0</td><br>    <td>2000</td><br>    </tr><br>    </tbody><br>    </table><br>    Spout 发生异常的情况<br>    <table><br>    <thead><br>    <tr><br>    <th>异常的次数</th><br>    <th>测试数据总量</th><br>    <th>结果集中不重复的 Tuple 数</th><br>    <th>数据重复的次数（&gt;1)</th><br>    <th>出现重复的 Tuple 数</th><br>    <th>数据丢失数量</th><br>    </tr><br>    </thead><br>    <tbody><br>    <tr><br>    <td>0</td><br>    <td>100000</td><br>    <td>100000</td><br>    <td>-</td><br>    <td>-</td><br>    <td>0</td><br>    </tr><br>    <tr><br>    <td>0</td><br>    <td>200000</td><br>    <td>200000</td><br>    <td>-</td><br>    <td>-</td><br>    <td>0</td><br>    </tr><br>    <tr><br>    <td>0</td><br>    <td>300000</td><br>    <td>300000</td><br>    <td>-</td><br>    <td>-</td><br>    <td>0</td><br>    </tr><br>    <tr><br>    <td>0</td><br>    <td>400000</td><br>    <td>400000</td><br>    <td>-</td><br>    <td>-</td><br>    <td>0</td><br>    </tr><br>    </tbody><br>    </table><br>    <table><br>    <thead><br>    <tr><br>    <th>异常的次数</th><br>    <th>测试数据总量</th><br>    <th>结果集中不重复的 Tuple 数</th><br>    <th>数据重复的次数（&gt;1)</th><br>    <th>出现重复的 Tuple 数</th><br>    <th>数据丢失数量</th><br>    </tr><br>    </thead><br>    <tbody><br>    <tr><br>    <td>1</td><br>    <td>100000</td><br>    <td>100000</td><br>    <td>2</td><br>    <td>2052</td><br>    <td>0</td><br>    </tr><br>    <tr><br>    <td>2</td><br>    <td>100000</td><br>    <td>100000</td><br>    <td>2</td><br>    <td>4414</td><br>    <td>0</td><br>    </tr><br>    <tr><br>    <td>4</td><br>    <td>100000</td><br>    <td>100000</td><br>    <td>2</td><br>    <td>9008</td><br>    <td>0</td><br>    </tr><br>    <tr><br>    <td>6</td><br>    <td>100000</td><br>    <td>100000</td><br>    <td>2</td><br>    <td>9690</td><br>    <td>0</td><br>    </tr><br>    <tr><br>    <td></td><br>    <td></td><br>    <td></td><br>    <td>3</td><br>    <td>1675</td><br>    <td>0</td><br>    </tr><br>    </tbody><br>    </table><br>Bolt 发生异常的情况</p>
<p>调用 emit 函数之前发生异常<br>    <table><br>    <thead><br>    <tr><br>    <th>异常次数</th><br>    <th>结果集中不重复的 Tuple 数</th><br>    <th>数据重复的次数 (&gt;1)</th><br>    <th>出现重复的 Tuple 数</th><br>    <th>数据丢失量</th><br>    </tr><br>    </thead><br>    <tbody><br>    <tr><br>    <td>0</td><br>    <td>100000</td><br>    <td>-</td><br>    <td>-</td><br>    <td>0</td><br>    </tr><br>    <tr><br>    <td>0</td><br>    <td>200000</td><br>    <td>-</td><br>    <td>-</td><br>    <td>0</td><br>    </tr><br>    <tr><br>    <td>0</td><br>    <td>300000</td><br>    <td>-</td><br>    <td>-</td><br>    <td>0</td><br>    </tr><br>    <tr><br>    <td>0</td><br>    <td>400000</td><br>    <td>-</td><br>    <td>-</td><br>    <td>0</td><br>    </tr><br>    </tbody><br>    </table><br>    <table><br>    <thead><br>    <tr><br>    <th>异常次数</th><br>    <th>结果集中不重复的 Tuple 数</th><br>    <th>数据重复的次数 (&gt;1)</th><br>    <th>出现重复的 Tuple 数</th><br>    <th>数据丢失量</th><br>    </tr><br>    </thead><br>    <tbody><br>    <tr><br>    <td>1</td><br>    <td>100000</td><br>    <td>-</td><br>    <td>-</td><br>    <td>0</td><br>    </tr><br>    <tr><br>    <td>2</td><br>    <td>100000</td><br>    <td>-</td><br>    <td>-</td><br>    <td>0</td><br>    </tr><br>    <tr><br>    <td>4</td><br>    <td>100000</td><br>    <td>-</td><br>    <td>-</td><br>    <td>0</td><br>    </tr><br>    <tr><br>    <td>8</td><br>    <td>100000</td><br>    <td>-</td><br>    <td>-</td><br>    <td>0</td><br>    </tr><br>    <tr><br>    <td>10</td><br>    <td>100000</td><br>    <td>-</td><br>    <td>-</td><br>    <td>0</td><br>    </tr><br>    </tbody><br>    </table><br>    调用 emit 函数之后发生异常<br>    <table><br>    <thead><br>    <tr><br>    <th>异常次数</th><br>    <th>结果集中不重复的 Tuple 数</th><br>    <th>数据重复的次数(&gt;1)</th><br>    <th>出现重复的 Tuple 数</th><br>    <th>数据丢失数量</th><br>    </tr><br>    </thead><br>    <tbody><br>    <tr><br>    <td>0</td><br>    <td>100000</td><br>    <td>-</td><br>    <td>-</td><br>    <td>0</td><br>    </tr><br>    <tr><br>    <td>0</td><br>    <td>200000</td><br>    <td>-</td><br>    <td>-</td><br>    <td>0</td><br>    </tr><br>    <tr><br>    <td>0</td><br>    <td>300000</td><br>    <td>-</td><br>    <td>-</td><br>    <td>0</td><br>    </tr><br>    <tr><br>    <td>0</td><br>    <td>400000</td><br>    <td>-</td><br>    <td>-</td><br>    <td>0</td><br>    </tr><br>    </tbody><br>    </table><br>    <table><br>    <thead><br>    <tr><br>    <th>异常次数</th><br>    <th>结果集中不重复的 Tuple 数</th><br>    <th>数据重复的次数(&gt;1)</th><br>    <th>出现重复的 Tuple 数</th><br>    <th>数据丢失数量</th><br>    </tr><br>    </thead><br>    <tbody><br>    <tr><br>    <td>1</td><br>    <td>100000</td><br>    <td>2</td><br>    <td>2</td><br>    <td>0</td><br>    </tr><br>    <tr><br>    <td>2</td><br>    <td>100000</td><br>    <td>2</td><br>    <td>3</td><br>    <td>0</td><br>    </tr><br>    <tr><br>    <td>4</td><br>    <td>100000</td><br>    <td>2</td><br>    <td>5</td><br>    <td>0</td><br>    </tr><br>    <tr><br>    <td>8</td><br>    <td>100000</td><br>    <td>2</td><br>    <td>9</td><br>    <td>0</td><br>    </tr><br>    <tr><br>    <td>10</td><br>    <td>100000</td><br>    <td>2</td><br>    <td>11</td><br>    <td>0</td><br>    </tr><br>    </tbody><br>    </table></p>
<h4 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h4><p>从上面的表格中可以得到，消息不会丢失，可能发生重复，重复的数目与异常的情况相关。</p>
<ul>
<li>不发生任何异常的情况下，消息不会重复不会丢失。</li>
<li>Spout 发生异常的情况下，消息的重复数目约等于 spout.max.pending(Spout 的配置项，每次可以发送的最多消息条数） * NumberOfException（异常次数）。</li>
<li>Acker 发生异常的情况下，消息重复的数目等于 spout.max.pending * NumberOfException。</li>
<li>Bolt 发生异常的情况：</li>
<li>emit 之前发生异常，消息不会重复。</li>
<li>emit 之后发生异常，消息重复的次数等于异常的次数。<br>结论与官方文档所述相符，每条消息至少发送一次，保证数据不会丢失，但可能重复，符合预期。</li>
</ul>
<h3 id="Exactly-Once"><a href="#Exactly-Once" class="headerlink" title="Exactly Once"></a>Exactly Once</h3><p>对于 Exactly Once 的语义，利用 Storm 中的 Trident 来实现。</p>
<h4 id="测试数据-1"><a href="#测试数据-1" class="headerlink" title="测试数据"></a>测试数据</h4><p>Kafka 上保存的一万到一百万不等的数字，每个数字在每次测试样例中出现且仅出现一次。</p>
<h4 id="测试结果-2"><a href="#测试结果-2" class="headerlink" title="测试结果"></a>测试结果</h4><p>Spout 发生异常情况<br>    <table><br>    <thead><br>    <tr><br>    <th>异常数</th><br>    <th>测试数据量</th><br>    <th>结果集中不重复的 Tuple 数</th><br>    <th>结果集中所有 Tuple 的总和</th><br>    </tr><br>    </thead><br>    <tbody><br>    <tr><br>    <td>1</td><br>    <td>10000</td><br>    <td>10000</td><br>    <td>50005000</td><br>    </tr><br>    <tr><br>    <td>2</td><br>    <td>10000</td><br>    <td>10000</td><br>    <td>50005000</td><br>    </tr><br>    <tr><br>    <td>3</td><br>    <td>10000</td><br>    <td>10000</td><br>    <td>50005000</td><br>    </tr><br>    </tbody><br>    </table><br>    Acker 发生异常的情况<br>    <table><br>    <thead><br>    <tr><br>    <th>异常数</th><br>    <th>测试数据量</th><br>    <th>结果集中不重复的 Tuple 数</th><br>    <th>结果集中所有 Tuple 的总和</th><br>    </tr><br>    </thead><br>    <tbody><br>    <tr><br>    <td>1</td><br>    <td>10000</td><br>    <td>10000</td><br>    <td>50005000</td><br>    </tr><br>    <tr><br>    <td>2</td><br>    <td>10000</td><br>    <td>10000</td><br>    <td>50005000</td><br>    </tr><br>    <tr><br>    <td>3</td><br>    <td>10000</td><br>    <td>10000</td><br>    <td>50005000</td><br>    </tr><br>    </tbody><br>    </table><br>    Bolt 发生异常的情况<br>    <table><br>    <thead><br>    <tr><br>    <th>异常数</th><br>    <th>测试数据量</th><br>    <th>结果集中不重复的 Tuple 数</th><br>    <th>结果集中所有 Tuple 的总和</th><br>    </tr><br>    </thead><br>    <tbody><br>    <tr><br>    <td>1</td><br>    <td>10000</td><br>    <td>10000</td><br>    <td>50005000</td><br>    </tr><br>    <tr><br>    <td>2</td><br>    <td>10000</td><br>    <td>10000</td><br>    <td>50005000</td><br>    </tr><br>    <tr><br>    <td>3</td><br>    <td>10000</td><br>    <td>10000</td><br>    <td>50005000</td><br>    </tr><br>    </tbody><br>    </table></p>
<h4 id="结论-2"><a href="#结论-2" class="headerlink" title="结论"></a>结论</h4><p>在所有情况下，最终结果集中的消息不会丢失，不会重复，与官方文档中的描述相符，符合预期。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>对 Storm 提供的三种不同消息保证机制，用户可以根据自己的需求选择不同的消息保证机制。</p>
<h3 id="不同消息可靠性保证的使用场景"><a href="#不同消息可靠性保证的使用场景" class="headerlink" title="不同消息可靠性保证的使用场景"></a>不同消息可靠性保证的使用场景</h3><p>对于 Storm 提供的三种消息可靠性保证，优缺点以及使用场景如下所示：</p>
<table><br><thead><br><tr><br><th>可靠性保证层次</th><br><th>优点</th><br><th>缺点</th><br><th>使用场景</th><br></tr><br></thead><br>    <tbody><br>    <tr><br>    <td>At most once</td><br>    <td>处理速度快</td><br>    <td>数据可能丢失</td><br>    <td>都处理速度要求高，且对数据丢失容忍度高的场景</td><br>    </tr><br>    <tr><br>    <td>At least once</td><br>    <td>数据不会丢失</td><br>    <td>数据可能重复</td><br>    <td>不能容忍数据丢失，可以容忍数据重复的场景</td><br>    </tr><br>    <tr><br>    <td>Exactly once</td><br>    <td>数据不会丢失，不会重复</td><br>    <td>处理速度慢</td><br>    <td>对数据不丢不重性质要求非常高，且处理速度要求没那么高，比如支付金额</td><br>    </tr><br>    </tbody><br>    </table>

<h3 id="如何实现不同层次的消息可靠性保证"><a href="#如何实现不同层次的消息可靠性保证" class="headerlink" title="如何实现不同层次的消息可靠性保证"></a>如何实现不同层次的消息可靠性保证</h3><p>对于 At Least Once 的保证需要做如下几步：</p>
<ol>
<li>需要开启 ACK 机制，即 Topology 中的 Acker 数量大于零；</li>
<li>Spout 是可靠的。即 Spout 发送消息的时候需要附带 msgId，并且实现失败消息重传功能（fail 函数 ，可以参考下面的 Spout 代码）；</li>
<li>Bolt 在发送消息时，需要调用 emit（inputTuple, outputTuple）进行建立 anchor 树（参考下面建立 anchor 树的代码），并且在成功处理之后调用 ack ，处理失败时调用 fail 函数，通知 Acker。</li>
</ol>
<p>不满足以上三条中任意一条的都只提供 At Most Once 的消息可靠性保证，如果希望得到 Exactly Once 的消息可靠性保证，可以使用 Trident 进行实现。</p>
<h3 id="不同层次的可靠性保证如何实现"><a href="#不同层次的可靠性保证如何实现" class="headerlink" title="不同层次的可靠性保证如何实现"></a>不同层次的可靠性保证如何实现</h3><h4 id="如何实现可靠的-Spout"><a href="#如何实现可靠的-Spout" class="headerlink" title="如何实现可靠的 Spout"></a>如何实现可靠的 Spout</h4><p>实现可靠的 Spout 需要在 nextTuple 函数中发送消息时，调用带 msgID 的 emit 方法，然后实现失败消息的重传（fail 函数），参考如下示例:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">/**</div><div class="line">         * 想实现可靠的 Spout，需要实现如下两点</div><div class="line">         * 1\. 在 nextTuple 函数中调用 emit 函数时需要带一个     msgId，用来表示当前的消息（如果消息发送失败会用 msgId 作为参数回调 fail 函数）</div><div class="line">         * 2\. 自己实现 fail 函数，进行重发（注意，在 storm 中没有 msgId 和消息的对应关系，需要自己进行维护）</div><div class="line">         */</div><div class="line">    public void nextTuple() &#123;</div><div class="line">        //设置 msgId 和 Value 一样，方便 fail 之后重发</div><div class="line">        collector.emit(new Values(curNum + &quot;&quot;, round +     &quot;&quot;), curNum + &quot;:&quot; + round);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    @Override</div><div class="line">    public void fail(Object msgId) &#123;//消息发送失败时的回调函数</div><div class="line">    String tmp = (String)msgId;   //上面我们设置了 msgId 和消息相同，这里通过 msgId 解析出具体的消息</div><div class="line">    String[] args = tmp.split(&quot;:&quot;);</div><div class="line"></div><div class="line">    //消息进行重发</div><div class="line">    collector.emit(new Values(args[0], args[1]), msgId);</div><div class="line">    &#125;</div></pre></td></tr></table></figure></p>
<h4 id="如何实现可靠的-Bolt"><a href="#如何实现可靠的-Bolt" class="headerlink" title="如何实现可靠的 Bolt"></a>如何实现可靠的 Bolt</h4><p>Storm 提供两种不同类型的 Bolt，分别是 BaseRichBolt 和 BaseBasicBolt，都可以实现可靠性消息传递，不过 BaseRichBolt 需要自己做很多周边的事情（建立 anchor 树，以及手动 ACK/FAIL 通知 Acker），使用场景更广泛，而 BaseBasicBolt 则由 Storm 帮忙实现了很多周边的事情，实现起来方便简单，但是使用场景单一。如何用这两个 Bolt 实现（不）可靠的消息传递如下所示：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div></pre></td><td class="code"><pre><div class="line">//BaseRichBolt 实现不可靠消息传递</div><div class="line">    public class SplitSentence extends BaseRichBolt &#123;//不建立 anchor 树的例子</div><div class="line">        OutputCollector _collector;</div><div class="line"></div><div class="line">        public void prepare(Map conf, TopologyContext context, OutputCollector collector) &#123;</div><div class="line">            _collector = collector;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        public void execute(Tuple tuple) &#123;</div><div class="line">            String sentence = tuple.getString(0);</div><div class="line">            for(String word: sentence.split(&quot; &quot;)) &#123;</div><div class="line">                _collector.emit(new Values(word));  // 不建立 anchor 树</div><div class="line">            &#125;</div><div class="line">            _collector.ack(tuple);          //手动 ack，如果不建立 anchor 树，是否 ack 是没有区别的，这句可以进行注释</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        public void declareOutputFields(OutputFieldsDeclarer declarer) &#123;</div><div class="line">            declarer.declare(new Fields(&quot;word&quot;));</div><div class="line">        &#125;      </div><div class="line">    &#125;</div><div class="line"></div><div class="line">    //BaseRichBolt 实现可靠的 Bolt</div><div class="line">    public class SplitSentence extends BaseRichBolt &#123;//建立 anchor 树以及手动 ack 的例子</div><div class="line">        OutputCollector _collector;</div><div class="line"></div><div class="line">        public void prepare(Map conf, TopologyContext context, OutputCollector collector) &#123;</div><div class="line">            _collector = collector;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        public void execute(Tuple tuple) &#123;</div><div class="line">            String sentence = tuple.getString(0);</div><div class="line">            for(String word: sentence.split(&quot; &quot;)) &#123;</div><div class="line">                _collector.emit(tuple, new Values(word));  // 建立 anchor 树</div><div class="line">            &#125;</div><div class="line">            _collector.ack(tuple);          //手动 ack，如果想让 Spout 重发该 Tuple，则调用 _collector.fail(tuple);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        public void declareOutputFields(OutputFieldsDeclarer declarer) &#123;</div><div class="line">            declarer.declare(new Fields(&quot;word&quot;));</div><div class="line">        &#125;      </div><div class="line">    &#125;</div><div class="line"></div><div class="line">    下面的示例会可以建立 Multi-anchoring</div><div class="line">    List&lt;Tuple&gt; anchors = new ArrayList&amp;lt;Tuple&amp;gt;();</div><div class="line">    anchors.add(tuple1);</div><div class="line">    anchors.add(tuple2);</div><div class="line">    _collector.emit(anchors, new Values(1, 2, 3));</div><div class="line"></div><div class="line">    //BaseBasicBolt 是吸纳可靠的消息传递</div><div class="line">    public class SplitSentence extends BaseBasicBolt &#123;//自动建立 anchor，自动 ack</div><div class="line">        public void execute(Tuple tuple, BasicOutputCollector collector) &#123;</div><div class="line">            String sentence = tuple.getString(0);</div><div class="line">            for(String word: sentence.split(&quot; &quot;)) &#123;</div><div class="line">                collector.emit(new Values(word));</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        public void declareOutputFields(OutputFieldsDeclarer declarer) &#123;</div><div class="line">            declarer.declare(new Fields(&quot;word&quot;));</div><div class="line">        &#125;      </div><div class="line">    &#125;</div></pre></td></tr></table></figure></p>
<h4 id="Trident"><a href="#Trident" class="headerlink" title="Trident"></a>Trident</h4><p>在 Trident 中，Spout 和 State 分别有三种状态，如下图所示：</p>
<p><img src="http://tech.meituan.com/img/test-of-storm&#39;s-reliability/spout-vs-state.png" alt="Trident Spout 和 State 的状态图"></p>
<p>其中表格中的 Yes 表示相应的 Spout 和 State 组合可以实现 Exactly Once 语义，No 表示相应的 Spout 和 State 组合不保证 Exactly Once 语义。下面的代码是一个 Trident 示例：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">OpaqueTridentKafkaSpout spout = new OpaqueTridentKafkaSpout(spoutConf);   //Opaque Spout</div><div class="line">//TransactionalTridentKafkaSpout spout = new TransactionalTridentKafkaSpout(spoutConf);   //Transaction Spout</div><div class="line"></div><div class="line">TridentTopology topology = new TridentTopology();</div><div class="line">String spoutTxid = Utils.kafkaSpoutGroupIdBuilder(topologyConfig.kafkaSrcTopic, topologyConfig.topologyName);</div><div class="line">Stream stream = topology.newStream(spoutTxid, spout)</div><div class="line">        .name(&quot;new stream&quot;)</div><div class="line">        .parallelismHint(1);</div><div class="line"></div><div class="line">// kafka config</div><div class="line">KafkaProducerConfig kafkaProducerConfig = new KafkaProducerConfig();      //KafkaProducerConfig 仅对 kafka 相关配置进行了封装，具体可以参考 TridentKafkaStateFactory2(Map&lt;String, String&gt; config)</div><div class="line">Map&lt;String, String&gt; kafkaConfigs = kafkaProducerConfig.loadFromConfig(topologyConfig);</div><div class="line">TridentToKafkaMapper tridentToKafkaMapper = new TridentToKafkaMapper();  //TridentToKafkaMapper 继承自 TridentTupleToKafkaMapper&lt;String, String&gt;，实现 getMessageFromTuple 接口，该接口中返回 tridentTuple.getString(0);</div><div class="line"></div><div class="line">String  dstTopic = &quot;test__topic_for_all&quot;;</div><div class="line"></div><div class="line">TridentKafkaStateFactory2 stateFactory = new TridentKafkaStateFactory2(kafkaConfigs);</div><div class="line">stateFactory.withTridentTupleToKafkaMapper(tridentToKafkaMapper);</div><div class="line">stateFactory.withKafkaTopicSelector(new DefaultTopicSelector(dstTopic));</div><div class="line"></div><div class="line">stream.each(new Fields(&quot;bytes&quot;), new AddMarkFunction(), new Fields(&quot;word&quot;)) //从spout 出来数据是一个 bytes 类型的数据，第二个是参数是自己的处理函数，第三个参数是处理函数的输出字段</div><div class="line">        .name(&quot;write2kafka&quot;)</div><div class="line">        .partitionPersist(stateFactory         //将数据写入到 Kafka 中，可以保证写入到 Kafka 的数据是 exactly once 的</div><div class="line">                , new Fields(&quot;word&quot;)</div><div class="line">                , new TridentKafkaUpdater())</div><div class="line">        .parallelismHint(1);</div></pre></td></tr></table></figure></p>
<p><strong>关注我们的官方微信公众号“美团点评技术团队”。现在就拿出手机，扫一扫：</strong></p>
<p><img src="http://tech.meituan.com/img/qrcode_for_gh.jpg" alt="公众号二维码"></p>

	
	</div>
</div>

	  
	    	
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2016-08-27 </div>
			<div class="article-title"><a href="/2016/08/27/spark-streaming-kafka-read-binlog-to-json/" >Spark Streaming 从 Kafka 读取 binlog 转换成 Json</a></div>						
		</h3>
	


	    	<div class="entry">
  <div class="row">
	
	
		<p>在开发 Spark Streaming 的公共组件过程中，需要将 binlog 的数据(Array[Byte])转换为 Json 格式，供用户使用，本文提供一种转换的思路。另外我们会用到几个辅助类，为了行文流畅，我们将辅助类的定义放在文章的最后面。如果</p>
<p>如果本文有讲述不详细，或者错误指出，肯请指出，谢谢</p>
<p>对于 binlog 数据，每一次操作(INSERT/UPDATE/DELETE 等）都会作为一条记录写入 binlog 文件，但是同一条记录可能包含数据库中的几行数据（这里比较绕，可以看一个具体的例子）</p>
<p>在数据库中，有 id, name，age 三个字段，其中 id 为主键，name 随意, age 随意。有两行数据如下</p>
<table>
<thead>
<tr>
<th>id</th>
<th>name</th>
<th>age</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>john</td>
<td>30</td>
</tr>
<tr>
<td>2</td>
<td>john</td>
<td>40</td>
</tr>
</tbody>
</table>
<p>那么你进行操作</p>
<p><pre class="lang:tsql decode:true  ">update table set age = 50 where name = “john”</pre><br>的时候，就会将两行的数据都进行更改，这两行更改的数据会在同一个 binlog 记录中，这一点会在后面的实现中有体现。</p>
<p>下面，我们给出具体的代码，然后对代码进行分析</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">def desirializeByte(b: (String, Array[Byte])) : (String, String) = &#123; </div><div class="line">  val binlogEntry = BinlogEntryUtil.serializeToBean(b._2)   //将 Array[Byte] 数据转换成 com.meituan.data.binlog.BinlogEntry 类，相关类定义参考附录</div><div class="line"></div><div class="line">  val pkeys = binlogEntry.getPrimaryKeys.asScala   //获取主键，这里的 asScala 将 Java 的 List 转换为 Scala 的 List</div><div class="line">  val rowDatas : List[BinlogRow] = binlogEntry.getRowDatas.asScala.toList  //获取具体的信息</div><div class="line">  val strRowDatas = rowDatas.map(a =&gt; &#123;            //将获取到的具体信息进行转换，这里主要是将没一条信息的内容，转换 [(K1:V1,K2:V2...Kn:Vn)] 的形式，方面后面进行 Json 化</div><div class="line">    val b = a.getBeforeColumns.asScala    //获取 beforColumns</div><div class="line">    val c = a.getAfterColumns.asScala     //获取 afterColumns</div><div class="line">    val mb = b.map(d =&gt; (d._1, d._2.getValue))  //去掉所有不需要的信息，只保留每个字段的值</div><div class="line">    val mc = c.map(c =&gt; (c._1, c._2.getValue))  //去掉所有不需要的信息，只保留每个字段的值</div><div class="line">    (mb, mc) //返回转换后的 beforeColumns 和 afterColumns</div><div class="line">  &#125;)</div><div class="line">  //下面利用 json4s 进行 Json 化</div><div class="line">  (binlogEntry.getEventType, compact(&quot;rowdata&quot; -&gt; strRowDatas.map&#123;</div><div class="line">    w =&gt; List(&quot;row_data&quot; -&amp;gt; (&quot;before&quot; -&amp;gt; w._1.toMap) ~ (&quot;after&quot; -&amp;gt; w._2.toMap))  //这里的两个 toMap 是必要的，不然里层会变成 List，这个地方比较疑惑的是，</div><div class="line">                                                                                 //w._1 按理是 Map类型，为什么还需要强制转换成 Map</div><div class="line">                                                                              //而且用 strRowDatas.foreach(x =&gt; println(s&quot;$&#123;x._1&#125;  $&#123;x._2&#125;&quot;)打印的结果表名是 Map</div><div class="line">  &#125;))&lt;/pre&gt;</div><div class="line">desirializeByte 函数传入 topic 中的一条记录，返回参数自己确定，我这里为了测试，返回一个 (String, String) 的 Tuple，第一个字段表示该条记录的 EventType（Insert/Update/Delete 等），第二个字段为 Json 化后的数据。</div><div class="line"></div><div class="line">BinlogEntryUtil.serilizeToBean 是一个辅助类，将 binlog 数据转化为一个 Java bean 类。</div><div class="line"></div><div class="line">第 4 行，我们得到表对应的主键，第 5 行获得具体的数据</div><div class="line"></div><div class="line">第 6 行到第 12 行是 Json 化之前的辅助工作，将所有不需要的东西给剔除掉，只留下字段，以及字段对应的值。</div><div class="line"></div><div class="line">第 14， 15 行就是具体的 Json 工作了（使用了 json4s 包进行 Json 化）</div><div class="line"></div><div class="line">这个过程中有一点需要注意的是，在 Json 化的时候，记得为 w._1 和 w._2 加 toMap 操作，不然会变成 List（很奇怪，我将 w._1 和 w._2 打印出来看，都是 Map 类型）或者你可以在第 7，8 行的末尾加上 .toMap 操作。这个我查了 API，进行了实验，暂时怀疑是在和 json4s 组合的时候，出现了问题，有待验证。</div><div class="line"></div><div class="line">利用上述代码，我们可以得到下面这样 Json 化之后的字符串(我进行了排版，程序返回的 Json 串是不换行的）</div><div class="line">&lt;pre class=&quot;font-size:8 lang:default decode:true&quot;&gt;&#123;&quot;rowdata&quot;:</div><div class="line">   [&#123;&quot;row_data&quot;:</div><div class="line">       &#123;&quot;before&quot;:&#123;&quot;param_name&quot;:&quot;creator&quot;,&quot;param_value&quot;:&quot;chenqiang05&quot;,&quot;horigindb_etl_id&quot;:&quot;2532&quot;,&quot;utime&quot;:&quot;2016-07-26 15:07:16&quot;,&quot;id&quot;:&quot;15122&quot;,&quot;status&quot;:&quot;0&quot;,&quot;ctime&quot;:&quot;2016-07-25 17:06:01&quot;&#125;,</div><div class="line">        &quot;after&quot;:&#123;&quot;param_name&quot;:&quot;creator&quot;,&quot;param_value&quot;:&quot;chendayao&quot;,&quot;horigindb_etl_id&quot;:&quot;2532&quot;,&quot;utime&quot;:&quot;2016-08-01 10:32:01&quot;,&quot;id&quot;:&quot;15122&quot;,&quot;status&quot;:&quot;0&quot;,&quot;ctime&quot;:&quot;2016-07-25 17:06:01&quot;&#125;</div><div class="line">       &#125;</div><div class="line">    &#125;]</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>到这里，基本就完成了一种将 binlog 数据 Json 化的代码。</p>
<p>附录代码，由于这些代码是从其他工程里面抠出来的，可能读起来会不顺畅，还请见谅。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div></pre></td><td class="code"><pre><div class="line">public static BinlogEntry serializeToBean(byte[] input) &#123;</div><div class="line">      BinlogEntry binlogEntry = null;</div><div class="line">      Entry entry = deserializeFromProtoBuf(input);//从 protobuf 反序列化</div><div class="line">      if(entry != null) &#123;</div><div class="line">         binlogEntry = serializeToBean(entry);</div><div class="line">      &#125;</div><div class="line">      return binlogEntry;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">public static Entry deserializeFromProtoBuf(byte[] input) &#123;</div><div class="line">        Entry entry = null;</div><div class="line"></div><div class="line">        try &#123;</div><div class="line">            entry = Entry.parseFrom(input);</div><div class="line">//com.alibaba.otter.canal.protocol.CanalEntry#Entry 类的方法，由 protobuf 生成</div><div class="line">        &#125; catch (InvalidProtocolBufferException var3) &#123;</div><div class="line">            logger.error(&quot;Exception:&quot; + var3);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        return entry;</div><div class="line">    &#125;</div><div class="line">//将 Entry 解析为一个 bean 类</div><div class="line">public static BinlogEntry serializeToBean(Entry entry) &#123;</div><div class="line">        RowChange rowChange = null;</div><div class="line"></div><div class="line">        try &#123;</div><div class="line">            rowChange = RowChange.parseFrom(entry.getStoreValue());</div><div class="line">        &#125; catch (Exception var8) &#123;</div><div class="line">            throw new RuntimeException(&quot;parse event has an error , data:&quot; + entry.toString(), var8);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        BinlogEntry binlogEntry = new BinlogEntry();</div><div class="line">        String[] logFileNames = entry.getHeader().getLogfileName().split(&quot;\\.&quot;);</div><div class="line">        String logFileNo = &quot;000000&quot;;</div><div class="line">        if(logFileNames.length &gt; 1) &#123;</div><div class="line">            logFileNo = logFileNames[1];</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        binlogEntry.setBinlogFileName(logFileNo);</div><div class="line">        binlogEntry.setBinlogOffset(entry.getHeader().getLogfileOffset());</div><div class="line">        binlogEntry.setExecuteTime(entry.getHeader().getExecuteTime());</div><div class="line">        binlogEntry.setTableName(entry.getHeader().getTableName());</div><div class="line">        binlogEntry.setEventType(entry.getHeader().getEventType().toString());</div><div class="line">        Iterator primaryKeysList = rowChange.getRowDatasList().iterator();</div><div class="line"></div><div class="line">        while(primaryKeysList.hasNext()) &#123;</div><div class="line">            RowData rowData = (RowData)primaryKeysList.next();</div><div class="line">            BinlogRow row = new BinlogRow(binlogEntry.getEventType());</div><div class="line">            row.setBeforeColumns(getColumnInfo(rowData.getBeforeColumnsList()));</div><div class="line">            row.setAfterColumns(getColumnInfo(rowData.getAfterColumnsList()));</div><div class="line">            binlogEntry.addRowData(row);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        if(binlogEntry.getRowDatas().size() &gt;= 1) &#123;</div><div class="line">            BinlogRow primaryKeysList1 = (BinlogRow)binlogEntry.getRowDatas().get(0);</div><div class="line">            binlogEntry.setPrimaryKeys(getPrimaryKeys(primaryKeysList1));</div><div class="line">        &#125; else &#123;</div><div class="line">            ArrayList primaryKeysList2 = new ArrayList();</div><div class="line">            binlogEntry.setPrimaryKeys(primaryKeysList2);</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        return binlogEntry;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">public class BinlogEntry implements Serializable &#123;</div><div class="line">    private String binlogFileName;</div><div class="line">    private long binlogOffset;</div><div class="line">    private long executeTime;</div><div class="line">    private String tableName;</div><div class="line">    private String eventType;</div><div class="line">    private List&lt;String&gt; primaryKeys;</div><div class="line">    private List&lt;BinlogRow&gt; rowDatas = new ArrayList();</div><div class="line">&#125;</div><div class="line">public class BinlogRow implements Serializable &#123;</div><div class="line">    public static final String EVENT_TYPE_INSERT = &quot;INSERT&quot;;</div><div class="line">    public static final String EVENT_TYPE_UPDATE = &quot;UPDATE&quot;;</div><div class="line">    public static final String EVENT_TYPE_DELETE = &quot;DELETE&quot;;</div><div class="line">    private String eventType;</div><div class="line">    private Map&lt;String, BinlogColumn&gt; beforeColumns;</div><div class="line">    private Map&lt;String, BinlogColumn&gt; afterColumns;</div><div class="line">&#125;</div><div class="line">public class BinlogColumn implements Serializable &#123;</div><div class="line">    private int index;</div><div class="line">    private String mysqlType;</div><div class="line">    private String name;</div><div class="line">    private boolean isKey;</div><div class="line">    private boolean updated;</div><div class="line">    private boolean isNull;</div><div class="line">    private String value;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>&nbsp;</p>

	
	</div>
</div>

	  
	    	
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2016-03-30 </div>
			<div class="article-title"><a href="/2016/03/30/mit-6-824-lab-2-part-a/" >MIT 6.824 Lab 2 Part A</a></div>						
		</h3>
	


	    	<div class="entry">
  <div class="row">
	
	
		<p><strong><span style="color: #ff0000;">做这个实验，最能学习的地方就是调试和思考的过程了，如果你直接参考了别人的思路或者代码，那么对于你来说，这个实验能学习到的东西则会大大减少</span></strong></p>
<p>记录 MIT 6.824 Lab 2 中 Part A的一些想法以及思路，如果错误，还请指出，谢谢</p>
<p>Lab 2 的链接如下<a href="http://nil.csail.mit.edu/6.824/2015/labs/lab-2.html" target="_blank" rel="external">http://nil.csail.mit.edu/6.824/2015/labs/lab-2.html</a>，其中 Part A 要求实现一个 ViewService，根据 Server的状态，进行相应的 View 切换（这里 View 表示当前能提供服务的 Server 以及相应的状态组合，ViewService 提供 View 的增删改查功能），这里将该 Lab 的两个部分分开来写。</p>
<p>Part A 实现 ViewService 的整个功能，ViewService 需要保证如下几点：</p>
<ol>
<li><p>在以下几种情况中的任何一种发生的时候才进行 View 的切换</p>
<ol>
<li>primary 和 backup 都没有 ack</li>
<li>primary 或者 backup 重启</li>
<li>backup 为空，且有闲置的 Server（会发送 Ping 命令给 ViewService）</li>
</ol>
</li>
<li><p>只有在 primary ack 过了当前的 View 之后，才能进行 View 的切换，换句话说，如果 primary 收到一个新的 View，然后挂了，那么 ViewService 就不应该切换 View（<span style="color: #0000ff;">根据页面的介绍，必须 ack 当前 View，那么这里有一个问题，如果 primary 收到一个新的 View，然后重启了，这种情况做何处理？</span>），这个限制简化了架构以及实现，但是可能导致一直不能更换 View</p>
</li>
<li>如果 primary 或 backup 在约定好的时间内没有发送 Ping 命令，则认为该 Server 挂了，需要做相应的操作</li>
<li>View 的 primary 只能是当前 View 的 primary 或者前一个 View 的 backup（在 ViewService 初始化的时候，primary 是第一个连接进来的 Server）</li>
<li>View 的 backup 可以是除 primary 之外的任何 Server，可为空<br>Part A 的要求实现如下三个函数：<br><pre class="lang:go decode:true ">func (vs <em>ViewServer) Ping(args </em>PingArgs, reply <em>PingReply) error {}<br>func (vs </em>ViewServer) Get(args <em>GetArgs, reply </em>GetReply) error {}<br>func (vs *ViewServer) tick() {}</pre><br>其中 Ping 接受 Server 发送过来的信息，并更新 View 的相应情况，Get 获取当前的 View，tick 则是一个回调函数，在固定时间内调用一次，检查 primary 和 backup 是否已经宕机，这里我实现的 Get 很简单，直接返回当前 View（在 ViewServer 里面定义一个字段 curView 用来表示当前 View），其他两个才是重点</li>
</ol>
<p>先把我定义的 ViewServer 贴一下（这个应该不算贴代码吧），下面能够更好的进行描述<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">type ViewServer struct &#123;</div><div class="line">	mu       sync.Mutex</div><div class="line">	l        net.Listener</div><div class="line">	dead     int32 // for testing</div><div class="line">	rpccount int32 // for testing</div><div class="line">	me       string</div><div class="line"></div><div class="line">	// Your declarations here.</div><div class="line">	lastPing map[string]time.Time  //记录 server 上次请求的时间，用来判断是否宕机</div><div class="line">	curView  View                  //当前 View</div><div class="line">	hasView bool                   //当前是否有 View 存在</div><div class="line">	hasAcked bool                  //Primary 是否已经 ack 了当前 View</div><div class="line">	secondBackup string            //将要被提升为 backup 的server</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>先说 tick，在 tick 中，首先我们需要知道 primary 是否已经 ack 了当前 View，如果没有 ack，那么就直接返回即可，<span style="color: #0000ff;">如果 ack 过了当前 View</span>，那么就继续进行下面的操作（<span style="color: #0000ff;">下面的操作必须在 primary ack 过了当前 View 之后才能进行</span>）</p>
<ol>
<li><p>判断 Primary 是否超时</p>
<ol>
<li><p>如果超时，则判断当前 View 是否存在 backup</p>
<ol>
<li>存在 backup，则将 backup 提升为 primary，然后将 primary 从 lastPing 中删除，并且将 hasAcked 置为 false</li>
<li>不存在，将 hasView 置为 false</li>
</ol>
</li>
<li><p>不超时，不做操作</p>
</li>
</ol>
</li>
<li><p>判断 backup 是否超时</p>
<ol>
<li>超时，则将 curView 中的 backup 置为 “”，然后 hasAcked 置为 false</li>
<li>不超时，不做操作<br>然后接下来是 Ping 函数</li>
</ol>
</li>
<li><p>判断当前是否有 View（通过 hasView)</p>
<ol>
<li>没有，就将当前发送 Ping 的 Server 当成 Primary，然后返回</li>
<li><p>有当前 View</p>
<ol>
<li><p>发送 Ping 的 Server 是 什么角色?</p>
<ol>
<li><p>是 primary，考虑 primary 是否重启</p>
<ol>
<li><p>重启（通过 ping 命令 请求参数是否为 0 判断），则判断当前 View 是否有 backup</p>
<ol>
<li>有，将 backup 提升为 primary，然后将 primary 设置成 secondBackup（会在下次请求的时候加入到 View 中），<span style="color: #0000ff;">这样的实现，是否合理，是否需要直接将 primary 设置为 backup?</span></li>
<li>没有，则将当前 View 的 Viewnum 加 1 即可</li>
</ol>
</li>
<li><p>不是重启，则 ack 当前 View（<span style="color: #0000ff;">请求参数可能是 当前 View 的 Viewnum 和 0 之外的第三个值吗</span>？）</p>
<ol>
<li><p>backup，考虑是否重启</p>
<ol>
<li>重启，将 backup 的角色切换到 secondBackup</li>
<li>不是重启，则不做操作</li>
</ol>
</li>
<li><p>闲置的 Server，考虑当前 View 是否有 backup</p>
<ol>
<li>有，不做操作（<span style="color: #0000ff;">这里是否需要将当前 Server 加入到 secondBackup？</span>）</li>
<li>没有，则将当前 Server 加入到 secondBackup，等待下一次 primary 发送 ping 的时候，提升为 backup</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<p><span style="color: #0000ff;"><del>当然，Ping 和 tick 函数 需要考虑加锁的问题，如果只为了通过测试，可以不加锁，测试都是串行的请求（有 goroutine）,如果不加锁，可能会遇到很诡异的问题</del></span></p>
<p>思路整理之后发现也不是太难，不过过程中还是有不少细节需要注意，如果可以，最好是自己进行思考，然后不断的调试，通过打印日志，思考是否符合自己的理解，然后进行代码的调整</p>
<p>&nbsp;</p>

	
	</div>
</div>

	  
	    	
	
	<!-- display as entry -->	
		<h3 class="title">
			<div class="date"> 2016-03-23 </div>
			<div class="article-title"><a href="/2016/03/23/mit-6-824-2015-lab-1/" >MIT 6.824 2015 Lab 1 记录</a></div>						
		</h3>
	


	    	<div class="entry">
  <div class="row">
	
	
		<p>===========<span style="color: #0000ff;">本文需要有 Go 的基础，并且知道 6.824 Lab 的相关内容作为预备知识</span>===========</p>
<p>最开始做这个 Lab 是去年，所以使用的是 2015 年的（现在已经有 2016 年的了），地址<a href="http://nil.csail.mit.edu/6.824/2015/" target="_blank" rel="external">Distributed System</a></p>
<p>第一个 Lab 是阅读 MapReduce 的论文，然后在提供的框架下实现一个简单版的 MapReduce 程序，论文地址：<a href="http://static.googleusercontent.com/media/research.google.com/en//archive/mapreduce-osdi04.pdf" target="_blank" rel="external">MapReduce</a></p>
<p>Part I</p>
<p>在提供的框架下，自己实现 Map 和 Reduce 函数，从而实现单机版的 MapReduce 程序，用来统计单词的数据，类似分布式程序的 Hello World。</p>
<p>根据提供的代码，以及我们在 Part I 执行的语句可知，在 wc.go 中我们找到如下的语句</p>
<p><pre class="theme:github lang:go decode:true ">mapreduce.RunSingle(5, 3, os.Args[2], Map, Reduce)</pre><br>我们最终执行的是 mapreduce.RunSingle 这个函数，在 RunSingle 函数中，可以分为如下几步</p>
<ol>
<li>InitMapReduce</li>
<li>Split</li>
<li>DoMap</li>
<li>DoReduce<br>其中 InitMapReduce，初始化一个 mapreduce 结构体，在后面使用，Split 则将输入的文件进行，然后顺序调用 DoMap，这里面会调用我们写的 map 函数，DoMap 都做完之后，再继续执行 DoReduce，这个函数会调用我们写的 reduce 函数。然后根据论文中的伪代码，差不多就可以完成这两个函数了<br><pre class="theme:github lang:go decode:true">map(String key, String value):<br>// key: document name<br>// value: document contents<br>for each word w in value:<br>EmitIntermediate(w, “1”);<br>reduce(String key, Iterator values):<br>// key: a word<br>// values: a list of counts<br>int result = 0;<br>for each v in values:<br>result += ParseInt(v);<br>Emit(AsString(result));</pre><br>Part IIIII</li>
</ol>
<p>首先查看 test_test.go 中的所有的 test 函数，看是如何实现测试的，大致顺序会形成一张如下的图，从上到下形成调用间的层次，同一层次间的函数执行顺序是从左往右顺序执行，其中绿色的表示是通过 go func()(另起一个线程)来执行的，我们只需要完成 mapreduce.go#RunMaster 函数即可。</p>
<p><a href="http://www.klion26.com/wp-content/uploads/2016/03/part2.png" target="_blank" rel="external"><img src="http://www.klion26.com/wp-content/uploads/2016/03/part2.png" alt="part2"></a></p>
<p>从 test 中的代码以及 Part I 中相关代码可以得知，我们需要写的代码（RunMaster 函数），实际上就是把所有的任务（map 或者 reduce）分配给具体的 worker 来执行。</p>
<p>首先，如果我们不考虑 worker 这个概念，那么怎么实现 RunMaster 函数呢，我们只需要把  Part I 中 RunSingle 中的两个 for 循环改成 goroutine 的，也就是在函数 DoMap 和 DoReduce 之前加关键字 go 即可，当然到这里我们还需要考虑，如何做到所有的 map 都完成之后才处理 reduce？reduce 都处理完成才算 RunMaster 函数处理完成？这就变成了 goroutine 的的同步问题了，可以参考 <a href="https://gobyexample.com/channel-buffering" target="_blank" rel="external">channel buffering。</a></p>
<p>到这里，如果我们不考虑 worker 的话，所有的 test case 都可以通过了，但是发现 TestBasic 函数的起的 worker 我们根本没有用到（后面几个 test case 还有 worker fail 的情况），那么就变成了，如何将我们上面的代码改写为，使用 worker 来执行，而不是直接通过 go DoMap() 以及 go DoReduce 来执行，通过阅读 worker.go 发现有一个 RPC 接口 DoJob，刚好满足我们的需要，阅读整个项目的其他代码（mapreduce.go#CleanupRegistration())，发现通过调用 common.go#call() 来统一进行 RPC 调用.</p>
<p>这里我们需要知道，从哪知道一个 worker 准备就绪，以及如何知道一个 woker 从忙状态（处理任务）—&gt; 闲状态（任务处理完成），我们可以看到在 worker.go#RunWoker 里面有一句</p>
<p><pre class="theme:github lang:go decode:true">Register(MasterAddress, me)</pre><br>我们发现 Register 函数如下</p>
<p><pre class="theme:github lang:go mark:6 decode:true">// Tell the master we exist and ready to work<br>func Register(master string, me string) {<br>    args := RegisterArgs{}<br>    args.Worker = me<br>    var reply RegisterReply<br>    ok := call(master, “MapReduce.Register”, args, reply)<br>    if ok == false {<br>        fmt.Printf(“Register: RPC %s register error\n”, master)<br>    }<br>}</pre><br>其中第6行调用 MapReduce.Register 这个 RPC 接口，继续看，发现 mapreduce.go#Register 这个函数中有下面一句话</p>
<p><pre class="theme:github lang:go decode:true">mr.registerChannel &lt;- args.Worker</pre><br>发现 registerChannel 是 mapreduce 这个结构体中的一个 channel，也就是在 RunWorker 的时候，我们能从 mr.registerChannel 得到一个标识 worker 的字符串（可以理解为这个 worker 的名字），而这个字符串，后续我们传给 common.go#call 函数，调用相关的 RPC 接口。</p>
<p>好，至少我们知道什么时候会得到通知有 worker 注册了，那么如何知道 worker 从忙变成闲呢，通过上面的流程，我们可以复用 registerChannel，也就是如果一个 worker 处理完任务的时候，我们也往这个 channel 发送 args.Worker 这个字段，这里就需要更改 registerChannel 的定义，因为我们不知道注册 worker 和分配任务给 worker 谁先谁后，在这里我们只需要把 registerChannel 变成带 buffer 的就行了。最终需要处理 worker 中途挂掉的情况，只需要在外层起一个死循环，直到 call 这个 函数返回 true 的时候才退出即可。</p>
<p>总结：</p>
<p>梳理一下：我们在 RunMaster 中需要并行的执行 Map，所有 Map 操作执行完成之后，并行的执行 Reduce 操作，这些操作需要通过分配给 worker 来执行，通过 channel 可以知道什么时候有空闲的 worker（注册或者由忙变闲），然后在调用 Worker.DoJob 的外层用死循环包装一层，知道 RPC 返回成功才退出即可</p>
<p>&nbsp;</p>

	
	</div>
</div>

	  
      </div>
	  <div>
	    <center>
	    <div class="pagination">
<ul class="pagination">
	 
</ul>
</div>

        </center>
	    </div>	
      

</div> <!-- col-md-9/col-md-12 -->


<div class="col-md-3">
	<div id="sidebar">
	
			
  <div id="site_search">
   <div class="form-group">
    <input type="text" id="local-search-input" name="q" results="0" placeholder="Search" class="st-search-input st-default-search-input form-control"/>
   </div>  
  <div id="local-search-result"></div>
  </div>


		
			
	<div class="widget">
		<h4>Categories</h4>
		<ul class="tag_box inline list-unstyled">
		
			<li><a href="/categories/ACM/">ACM<span>16</span></a></li>
		
			<li><a href="/categories/Algorithm/">Algorithm<span>14</span></a></li>
		
			<li><a href="/categories/ACM/HDU/">HDU<span>1</span></a></li>
		
			<li><a href="/categories/HDU/">HDU<span>9</span></a></li>
		
			<li><a href="/categories/Linux/">Linux<span>32</span></a></li>
		
			<li><a href="/categories/POJ/">POJ<span>6</span></a></li>
		
			<li><a href="/categories/Linux/TeX/">TeX<span>1</span></a></li>
		
			<li><a href="/categories/USACO/">USACO<span>21</span></a></li>
		
			<li><a href="/categories/Uncategorized/">Uncategorized<span>3</span></a></li>
		
			<li><a href="/categories/Visual-C/">Visual C++<span>1</span></a></li>
		
			<li><a href="/categories/wordpress/">wordpress<span>8</span></a></li>
		
			<li><a href="/categories/具体数学/">具体数学<span>2</span></a></li>
		
			<li><a href="/categories/分布式系统/">分布式系统<span>8</span></a></li>
		
			<li><a href="/categories/实时计算/">实时计算<span>6</span></a></li>
		
			<li><a href="/categories/分布式系统/实时计算/">实时计算<span>4</span></a></li>
		
			<li><a href="/categories/想清楚/">想清楚<span>1</span></a></li>
		
			<li><a href="/categories/成长/">成长<span>3</span></a></li>
		
			<li><a href="/categories/我的生活/">我的生活<span>8</span></a></li>
		
			<li><a href="/categories/所谓开源/">所谓开源<span>3</span></a></li>
		
			<li><a href="/categories/数学/">数学<span>2</span></a></li>
		
			<li><a href="/categories/POJ/数学/">数学<span>2</span></a></li>
		
			<li><a href="/categories/Algorithm/数学/">数学<span>1</span></a></li>
		
			<li><a href="/categories/源码阅读/">源码阅读<span>1</span></a></li>
		
			<li><a href="/categories/Linux/计算机图形学图像处理/">计算机图形学图像处理<span>1</span></a></li>
		
			<li><a href="/categories/计算机图形学图像处理/">计算机图形学图像处理<span>1</span></a></li>
		
			<li><a href="/categories/计算机基础/">计算机基础<span>22</span></a></li>
		
			<li><a href="/categories/Algorithm/计算机基础/">计算机基础<span>1</span></a></li>
		
			<li><a href="/categories/Linux/计算机基础/">计算机基础<span>2</span></a></li>
		
			<li><a href="/categories/所谓开源/计算机基础/">计算机基础<span>1</span></a></li>
		
			<li><a href="/categories/计算机安全/">计算机安全<span>3</span></a></li>
		
			<li><a href="/categories/计算机基础/语言学习/">语言学习<span>1</span></a></li>
		
			<li><a href="/categories/语言学习/">语言学习<span>1</span></a></li>
		
		</ul>
	</div>

		
			
	<div class="widget">
		<h4>Tag Cloud</h4>
		<ul class="tag_box inline list-unstyled">		
		
			<li><a href="/tags/syslog/">syslog<span>1</span></a></li>
		
			<li><a href="/tags/chain-matrix-multiplication/">chain matrix multiplication<span>1</span></a></li>
		
			<li><a href="/tags/trident/">trident<span>1</span></a></li>
		
			<li><a href="/tags/dag/">dag<span>1</span></a></li>
		
			<li><a href="/tags/单调队列/">单调队列<span>1</span></a></li>
		
			<li><a href="/tags/critical/">critical<span>1</span></a></li>
		
			<li><a href="/tags/vi/">vi<span>2</span></a></li>
		
			<li><a href="/tags/program/">program<span>1</span></a></li>
		
			<li><a href="/tags/tutorial/">tutorial<span>1</span></a></li>
		
			<li><a href="/tags/乱码/">乱码<span>1</span></a></li>
		
			<li><a href="/tags/longest-increasing-subsequences/">longest increasing subsequences<span>1</span></a></li>
		
			<li><a href="/tags/filter/">filter<span>1</span></a></li>
		
			<li><a href="/tags/ndbm/">ndbm<span>1</span></a></li>
		
			<li><a href="/tags/fork/">fork<span>2</span></a></li>
		
			<li><a href="/tags/rpm/">rpm<span>1</span></a></li>
		
			<li><a href="/tags/at-most-once/">at-most-once<span>1</span></a></li>
		
			<li><a href="/tags/problem-solve/">problem_solve<span>1</span></a></li>
		
			<li><a href="/tags/protobuf/">protobuf<span>1</span></a></li>
		
			<li><a href="/tags/knapsack/">knapsack<span>1</span></a></li>
		
			<li><a href="/tags/horn-formulas/">horn formulas<span>1</span></a></li>
		
		
		   <li><a href="/tags">...<span>260</span></a></li>
		 
		</ul>
	</div>


		
			
<div class="widget">
  <h4>Recent Posts</h4>
  <ul class="entry list-unstyled">
    
      <li>
        <a href="/2017/02/16/spark-streaming-consume-kafka-at-00-second-of-every-minute/" ><i class="fa fa-file-o"></i>Spark Streaming 统一在每分钟的 00 ...</a>
      </li>
    
      <li>
        <a href="/2017/01/15/spark-streaming-e5-be-80-hdfs-e8-bf-bd-e5-8a-a0-lzo-e6-96-87-e4-bb-b6/" ><i class="fa fa-file-o"></i>Spark Streaming 往 HDFS 追加 L...</a>
      </li>
    
      <li>
        <a href="/2016/12/16/spark-streaming-ran-out-of-messages-before-reaching-ending-offset/" ><i class="fa fa-file-o"></i>Spark Streaming Ran out of ...</a>
      </li>
    
      <li>
        <a href="/2016/12/02/spark-streaming-consume-kafka-message-from-specify-timestamp/" ><i class="fa fa-file-o"></i>Spark Streaming 从指定时间戳开始消费 ...</a>
      </li>
    
      <li>
        <a href="/2016/11/26/spark-streaming-e5-be-80-hdfs-e5-86-99-e6-96-87-e4-bb-b6-ef-bc-8c-e8-87-aa-e5-ae-9a-e4-b9-89-e6-96-87-e4-bb-b6-e5-90-8d/" ><i class="fa fa-file-o"></i>Spark Streaming 往 HDFS 写文件，...</a>
      </li>
    
  </ul>
</div>

		
			
<div class="widget">
	<h4>Links</h4>
	<ul class="blogroll list-unstyled">
	
		<li><i class="fa fa-github"></i><a href="https://github.com/wzpan/freemind/" title="Freemind's Github repository." target="_blank"]);">Freemind</a></li>
	
		<li><i class="fa fa-github"></i><a href="http://www.github.com/wzpan" title="My Github account." target="_blank"]);">My Github</a></li>
	
		<li><i class="fa fa-linkedin"></i><a href="http://www.linkedin.com/in/hahack" title="My Linkin account." target="_blank"]);">My LinkedIn</a></li>
	
	</ul>
</div>


		
	</div> <!-- sidebar -->
</div> <!-- col-md-3 -->




	</div>
  </div>
  <div class="container-narrow">
  <footer> <p>
  &copy; 2017 John Doe
  
      with help from <a href="http://hexo.io/" target="_blank">Hexo</a> and <a href="http://getbootstrap.com/" target="_blank">Twitter Bootstrap</a>. Theme by <a href="http://github.com/wzpan/hexo-theme-freemind/">Freemind</a>.    
</p> </footer>
</div> <!-- container-narrow -->
  


  
<a id="gotop" href="#">   
  <span>▲</span> 
</a>

<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/bootstrap.min.js"></script>
<script src="/js/main.js"></script>
<script src="/js/search.js"></script> 


<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
(function($){
  $('.fancybox').fancybox();
})(jQuery);
</script>



   <script type="text/javascript">      
     var search_path = "search.xml";
	 if (search_path.length == 0) {
	 	search_path = "search.xml";
	 }
	 var path = "/" + search_path;
     searchFunc(path, 'local-search-input', 'local-search-result');
   </script>

</body>
   </html>
